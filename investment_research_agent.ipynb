{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8f6fdf",
   "metadata": {},
   "source": [
    "# Investment Research Agent - Multi-Agent System\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an autonomous Investment Research Agent that demonstrates:\n",
    "\n",
    "## Architecture\n",
    "- **Multi-Agent System**: Coordinator, Specialist Agents (News, Technical, Fundamental)\n",
    "- **Memory System**: FAISS vector database for persistent learning\n",
    "- **Data Sources**: Yahoo Finance, NewsAPI, FRED, Alpha Vantage\n",
    "- **Interface**: Gradio web interface for user interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f83c34",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20daea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.3.33)\n",
      "Requirement already satisfied: langchain-community in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.3.30)\n",
      "Requirement already satisfied: yfinance in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.2.66)\n",
      "Requirement already satisfied: pandas in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (6.3.0)\n",
      "Requirement already satisfied: gradio in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (5.47.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: fredapi in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: newsapi-python in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: chromadb in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (0.4.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-openai) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (4.14.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (6.32.1)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from plotly) (2.6.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.118.0)\n",
      "Requirement already satisfied: ffmpy in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.35.3)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: pydub in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.48.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.19.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio) (0.37.0)\n",
      "Requirement already satisfied: fsspec in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from gradio-client==1.13.3->gradio) (2025.9.0)\n",
      "Requirement already satisfied: filelock in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (1.75.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.41.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: sympy in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\source\\aiml\\aai520_8proj\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-community yfinance pandas numpy matplotlib seaborn plotly gradio faiss-cpu python-dotenv requests fredapi newsapi-python chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d41f5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional\n",
    "import gradio as gr\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.tools import BaseTool, tool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Data source imports\n",
    "import yfinance as yf\n",
    "from newsapi import NewsApiClient\n",
    "from fredapi import Fred\n",
    "import requests\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a415bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration from environment variables\n",
    "AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_GPT_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_GPT_API_VERSION = os.getenv('AZURE_OPENAI_GPT_API_VERSION', '2024-02-15-preview')\n",
    "AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv('AZURE_OPENAI_EMBEDDING_API_VERSION', '2024-02-15-preview')\n",
    "\n",
    "ALPHA_VANTAGE_API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
    "NEWS_API_KEY = os.getenv('NEWSAPI_KEY')\n",
    "FRED_API_KEY = os.getenv('FRED_API_KEY')\n",
    "\n",
    "# Initialize Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_OPENAI_GPT_DEPLOYMENT_NAME,\n",
    "    openai_api_version=AZURE_OPENAI_GPT_API_VERSION,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize embeddings for vector database\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    openai_api_version=AZURE_OPENAI_EMBEDDING_API_VERSION,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "print(\"Environment setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ff17160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt configuration class created!\n"
     ]
    }
   ],
   "source": [
    "class PromptConfiguration:\n",
    "    \"\"\"Central configuration class for all prompts used in the investment research system\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_planning_prompt(role: str, task: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As an {role}, create a detailed research plan for: {task}\n",
    "        \n",
    "        Consider these aspects:\n",
    "        1. Data gathering (price data, news, economic indicators, fundamentals)\n",
    "        2. Analysis techniques (technical, fundamental, sentiment)\n",
    "        3. Risk assessment\n",
    "        4. Market context evaluation\n",
    "        \n",
    "        Return a numbered list of specific, actionable research steps.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_reflection_prompt(analysis_result: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Please evaluate this investment analysis for quality and completeness:\n",
    "        \n",
    "        Analysis: {analysis_result}\n",
    "        \n",
    "        Provide a structured evaluation covering:\n",
    "        1. Completeness (1-10): Are all key aspects covered?\n",
    "        2. Data Quality (1-10): Is the data comprehensive and current?\n",
    "        3. Logic (1-10): Is the reasoning sound and well-structured?\n",
    "        4. Actionability (1-10): Are the conclusions practical and specific?\n",
    "        5. Risk Assessment (1-10): Are risks properly identified and evaluated?\n",
    "        \n",
    "        Also provide:\n",
    "        - Overall Score (1-10)\n",
    "        - Key Strengths (2-3 points)\n",
    "        - Areas for Improvement (2-3 points)\n",
    "        - Specific Recommendations for enhancement\n",
    "        \n",
    "        Format as JSON with these exact keys: completeness, data_quality, logic, actionability, risk_assessment, overall_score, strengths, improvements, recommendations\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_news_classification_prompt(title: str, description: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Classify this news article:\n",
    "        Title: {title}\n",
    "        Description: {description}\n",
    "        \n",
    "        Provide classification in JSON format:\n",
    "        {{\n",
    "            \"category\": \"earnings|product|market|regulation|management|merger|other\",\n",
    "            \"sentiment\": \"positive|negative|neutral\",\n",
    "            \"importance\": \"high|medium|low\",\n",
    "            \"reasoning\": \"brief explanation\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_insights_extraction_prompt(classified_articles: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Extract key insights from these classified news articles:\n",
    "        \n",
    "        {classified_articles}\n",
    "        \n",
    "        Provide insights in JSON format:\n",
    "        {{\n",
    "            \"key_themes\": [\"list of main themes\"],\n",
    "            \"sentiment_distribution\": {{\"positive\": 0, \"negative\": 0, \"neutral\": 0}},\n",
    "            \"high_importance_items\": [\"list of high importance findings\"],\n",
    "            \"potential_catalysts\": [\"events that could drive stock price\"],\n",
    "            \"risk_factors\": [\"identified risks or concerns\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_news_summarization_prompt(insights: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Create a comprehensive news analysis summary for {symbol} based on these insights:\n",
    "        \n",
    "        {insights}\n",
    "        \n",
    "        Provide a professional investment-focused summary covering:\n",
    "        1. Executive Summary (2-3 sentences)\n",
    "        2. Key Developments and Themes\n",
    "        3. Sentiment Analysis\n",
    "        4. Potential Stock Price Catalysts\n",
    "        5. Risk Factors to Monitor\n",
    "        6. Investment Implications\n",
    "        \n",
    "        Keep the summary concise but comprehensive, suitable for investment decision-making.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_technical_analysis_prompt(symbol: str, stock_data: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As a Technical Analysis Specialist, analyze the following stock data for {symbol}:\n",
    "        \n",
    "        Stock Data: {stock_data}\n",
    "        \n",
    "        Provide a comprehensive technical analysis covering:\n",
    "        1. Price Trend Analysis (short-term and medium-term trends)\n",
    "        2. Support and Resistance Levels\n",
    "        3. Volume Analysis\n",
    "        4. Key Technical Indicators (if calculable from available data)\n",
    "        5. Chart Patterns (if identifiable)\n",
    "        6. Technical Price Targets\n",
    "        7. Risk Levels and Stop-Loss Recommendations\n",
    "        \n",
    "        Conclude with:\n",
    "        - Technical Rating: Buy/Hold/Sell\n",
    "        - Confidence Level: High/Medium/Low\n",
    "        - Key Technical Risks\n",
    "        - Next Important Price Levels to Watch\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_fundamental_analysis_prompt(symbol: str, stock_data: str, alpha_overview: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As a Fundamental Analysis Specialist, analyze {symbol} using this data:\n",
    "        \n",
    "        Basic Stock Data: {stock_data}\n",
    "        Company Overview: {alpha_overview}\n",
    "        \n",
    "        Provide comprehensive fundamental analysis covering:\n",
    "        1. Company Business Model and Competitive Position\n",
    "        2. Financial Health Assessment\n",
    "        3. Valuation Analysis (P/E, PEG, other relevant ratios)\n",
    "        4. Growth Prospects and Market Opportunities\n",
    "        5. Management Quality and Corporate Governance\n",
    "        6. Industry and Sector Analysis\n",
    "        7. Competitive Advantages and Moats\n",
    "        \n",
    "        Conclude with:\n",
    "        - Fundamental Rating: Strong Buy/Buy/Hold/Sell/Strong Sell\n",
    "        - Fair Value Estimate (if possible)\n",
    "        - Key Fundamental Risks\n",
    "        - Catalysts for Value Realization\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod  \n",
    "    def get_sentiment_analysis_prompt(symbol: str, news_analysis: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As a News and Sentiment Analysis Specialist, provide additional insights on this news analysis for {symbol}:\n",
    "        \n",
    "        {news_analysis}\n",
    "        \n",
    "        Focus on:\n",
    "        1. Market Sentiment Implications\n",
    "        2. News Flow Impact on Stock Price\n",
    "        3. Institutional vs Retail Sentiment\n",
    "        4. Social Media and Public Perception Trends\n",
    "        5. News-Based Trading Opportunities\n",
    "        6. Event-Driven Catalysts\n",
    "        \n",
    "        Conclude with:\n",
    "        - Sentiment Rating: Very Positive/Positive/Neutral/Negative/Very Negative\n",
    "        - News Impact Assessment: High/Medium/Low\n",
    "        - Recommended Action Based on News Flow\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_routing_prompt(request: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Analyze this investment research request and determine which specialists should handle it:\n",
    "        \n",
    "        Request: {request}\n",
    "        Symbol: {symbol}\n",
    "        \n",
    "        Available specialists:\n",
    "        - technical: Technical analysis, chart patterns, price trends\n",
    "        - fundamental: Company financials, valuation, business analysis\n",
    "        - news: News analysis, sentiment, market events\n",
    "        \n",
    "        Return JSON format:\n",
    "        {{\n",
    "            \"specialists_needed\": [\"list of specialist types needed\"],\n",
    "            \"priority_order\": [\"ordered list by importance\"],\n",
    "            \"reasoning\": \"why these specialists were chosen\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_analysis_generation_prompt(symbol: str, specialist_analyses: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As a Senior Investment Analyst, create a comprehensive investment analysis for {symbol} \n",
    "        using these specialist reports:\n",
    "        \n",
    "        {specialist_analyses}\n",
    "        \n",
    "        Create a structured investment analysis with:\n",
    "        1. Executive Summary\n",
    "        2. Investment Thesis\n",
    "        3. Key Strengths and Opportunities\n",
    "        4. Risks and Concerns\n",
    "        5. Financial Analysis Summary\n",
    "        6. Technical Analysis Summary\n",
    "        7. Market Sentiment Analysis\n",
    "        8. Price Target and Recommendation\n",
    "        9. Risk-Adjusted Return Expectations\n",
    "        10. Conclusion and Action Items\n",
    "        \n",
    "        Make it comprehensive but concise, suitable for investment decision-making.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_evaluation_prompt(analysis: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As an Investment Analysis Quality Evaluator, assess this investment analysis for {symbol}:\n",
    "        \n",
    "        Analysis:\n",
    "        {analysis}\n",
    "        \n",
    "        Evaluate on these criteria (1-10 scale):\n",
    "        1. Completeness: Are all key investment aspects covered?\n",
    "        2. Data Integration: How well are different data sources synthesized?\n",
    "        3. Risk Assessment: Is risk analysis comprehensive and realistic?\n",
    "        4. Actionability: Are recommendations specific and implementable?\n",
    "        5. Logic and Reasoning: Is the analysis logical and well-structured?\n",
    "        6. Market Context: Is broader market context considered?\n",
    "        7. Clarity: Is the analysis clear and professional?\n",
    "        \n",
    "        Provide feedback in JSON format:\n",
    "        {{\n",
    "            \"scores\": {{\n",
    "                \"completeness\": X,\n",
    "                \"data_integration\": X,\n",
    "                \"risk_assessment\": X,\n",
    "                \"actionability\": X,\n",
    "                \"logic_reasoning\": X,\n",
    "                \"market_context\": X,\n",
    "                \"clarity\": X\n",
    "            }},\n",
    "            \"overall_score\": X,\n",
    "            \"grade\": \"A|B|C|D|F\",\n",
    "            \"strengths\": [\"list of key strengths\"],\n",
    "            \"weaknesses\": [\"list of areas needing improvement\"],\n",
    "            \"specific_improvements\": [\"detailed suggestions for enhancement\"],\n",
    "            \"missing_elements\": [\"what's missing from the analysis\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_refinement_prompt(original_analysis: str, evaluation: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Improve this investment analysis for {symbol} based on the evaluation feedback:\n",
    "        \n",
    "        Original Analysis:\n",
    "        {original_analysis}\n",
    "        \n",
    "        Evaluation Feedback:\n",
    "        {evaluation}\n",
    "        \n",
    "        Create an improved version that addresses these issues:\n",
    "        1. Fix identified weaknesses\n",
    "        2. Add missing elements\n",
    "        3. Implement specific improvements\n",
    "        4. Enhance overall quality and completeness\n",
    "        5. Maintain professional investment analysis standards\n",
    "        \n",
    "        Focus particularly on areas that scored below 7/10 in the evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "print(\"Prompt configuration class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dcca6",
   "metadata": {},
   "source": [
    "## 2. Data Source Tools and Integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1585e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source tools created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Yahoo Finance Tool\n",
    "@tool\n",
    "def get_stock_data(symbol: str, period: str = \"1y\") -> str:\n",
    "    \"\"\"Get stock price data and basic financial information from Yahoo Finance.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., 'AAPL', 'MSFT')\n",
    "        period: Time period ('1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        hist = stock.history(period=period)\n",
    "        info = stock.info\n",
    "        \n",
    "        current_price = hist['Close'].iloc[-1]\n",
    "        price_change = hist['Close'].iloc[-1] - hist['Close'].iloc[-2]\n",
    "        price_change_pct = (price_change / hist['Close'].iloc[-2]) * 100\n",
    "        \n",
    "        result = {\n",
    "            'symbol': symbol,\n",
    "            'current_price': round(current_price, 2),\n",
    "            'price_change': round(price_change, 2),\n",
    "            'price_change_pct': round(price_change_pct, 2),\n",
    "            'volume': hist['Volume'].iloc[-1],\n",
    "            'market_cap': info.get('marketCap', 'N/A'),\n",
    "            'pe_ratio': info.get('trailingPE', 'N/A'),\n",
    "            'company_name': info.get('longName', 'N/A'),\n",
    "            'sector': info.get('sector', 'N/A'),\n",
    "            'industry': info.get('industry', 'N/A')\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock data for {symbol}: {str(e)}\"\n",
    "\n",
    "# News API Tool\n",
    "@tool\n",
    "def get_stock_news(symbol: str, days: int = 7) -> str:\n",
    "    \"\"\"Get recent news articles related to a stock symbol.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol to search news for\n",
    "        days: Number of days to look back for news\n",
    "    \"\"\"\n",
    "    try:\n",
    "        newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "        \n",
    "        from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        articles = newsapi.get_everything(\n",
    "            q=symbol,\n",
    "            language='en',\n",
    "            sort_by='relevancy',\n",
    "            from_param=from_date,\n",
    "            page_size=10\n",
    "        )\n",
    "        \n",
    "        news_items = []\n",
    "        for article in articles['articles'][:5]:  # Top 5 articles\n",
    "            news_items.append({\n",
    "                'title': article['title'],\n",
    "                'description': article['description'],\n",
    "                'source': article['source']['name'],\n",
    "                'published_at': article['publishedAt'],\n",
    "                'url': article['url']\n",
    "            })\n",
    "        \n",
    "        return json.dumps(news_items, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching news for {symbol}: {str(e)}\"\n",
    "\n",
    "# FRED Economic Data Tool\n",
    "@tool\n",
    "def get_economic_data(series_id: str = \"GDP\") -> str:\n",
    "    \"\"\"Get economic data from FRED (Federal Reserve Economic Data).\n",
    "    \n",
    "    Args:\n",
    "        series_id: FRED series ID (e.g., 'GDP', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fred = Fred(api_key=FRED_API_KEY)\n",
    "        data = fred.get_series(series_id, limit=12)  # Last 12 observations\n",
    "        \n",
    "        result = {\n",
    "            'series_id': series_id,\n",
    "            'latest_value': float(data.iloc[-1]),\n",
    "            'latest_date': data.index[-1].strftime('%Y-%m-%d'),\n",
    "            'previous_value': float(data.iloc[-2]),\n",
    "            'change': float(data.iloc[-1] - data.iloc[-2]),\n",
    "            'change_pct': float((data.iloc[-1] - data.iloc[-2]) / data.iloc[-2] * 100)\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching economic data for {series_id}: {str(e)}\"\n",
    "\n",
    "# Alpha Vantage Tool\n",
    "@tool\n",
    "def get_alpha_vantage_data(symbol: str, function: str = \"TIME_SERIES_DAILY\") -> str:\n",
    "    \"\"\"Get financial data from Alpha Vantage API.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol\n",
    "        function: Alpha Vantage function (e.g., 'TIME_SERIES_DAILY', 'OVERVIEW', 'INCOME_STATEMENT')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_url = \"https://www.alphavantage.co/query\"\n",
    "        params = {\n",
    "            'function': function,\n",
    "            'symbol': symbol,\n",
    "            'apikey': ALPHA_VANTAGE_API_KEY\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Return a simplified version to avoid token limits\n",
    "        if function == \"OVERVIEW\":\n",
    "            overview = {\n",
    "                'symbol': data.get('Symbol', 'N/A'),\n",
    "                'market_cap': data.get('MarketCapitalization', 'N/A'),\n",
    "                'pe_ratio': data.get('PERatio', 'N/A'),\n",
    "                'peg_ratio': data.get('PEGRatio', 'N/A'),\n",
    "                'dividend_yield': data.get('DividendYield', 'N/A'),\n",
    "                'eps': data.get('EPS', 'N/A'),\n",
    "                '52_week_high': data.get('52WeekHigh', 'N/A'),\n",
    "                '52_week_low': data.get('52WeekLow', 'N/A')\n",
    "            }\n",
    "            return json.dumps(overview, indent=2)\n",
    "        \n",
    "        return json.dumps(data, indent=2)[:1000]  # Truncate to avoid token limits\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching Alpha Vantage data for {symbol}: {str(e)}\"\n",
    "\n",
    "print(\"Data source tools created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15736764",
   "metadata": {},
   "source": [
    "## 3. Vector Database and Memory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f10a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://sasw-mgfv7ds1-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new agent memory\n",
      "Agent memory system initialized!\n"
     ]
    }
   ],
   "source": [
    "class AgentMemory:\n",
    "    \"\"\"Persistent memory system using FAISS vector database.\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_path: str = \"./database/agent_memory\"):\n",
    "        self.memory_path = memory_path\n",
    "        self.vector_store = None\n",
    "        self.initialize_memory()\n",
    "    \n",
    "    def initialize_memory(self):\n",
    "        \"\"\"Initialize or load existing memory.\"\"\"\n",
    "        try:\n",
    "            # Try to load existing memory\n",
    "            if os.path.exists(f\"{self.memory_path}.faiss\"):\n",
    "                self.vector_store = FAISS.load_local(self.memory_path, embeddings)\n",
    "                print(\"Loaded existing agent memory\")\n",
    "            else:\n",
    "                # Create new memory with initial documents\n",
    "                initial_docs = [\n",
    "                    Document(page_content=\"Investment analysis requires considering multiple factors: technical indicators, fundamental analysis, market sentiment, and economic conditions.\", \n",
    "                            metadata={\"type\": \"general_knowledge\", \"timestamp\": datetime.now().isoformat()})\n",
    "                ]\n",
    "                self.vector_store = FAISS.from_documents(initial_docs, embeddings)\n",
    "                self.save_memory()\n",
    "                print(\"Created new agent memory\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing memory: {e}\")\n",
    "            # Fallback: create minimal memory\n",
    "            initial_docs = [\n",
    "                Document(page_content=\"Fallback memory initialized\", \n",
    "                        metadata={\"type\": \"system\", \"timestamp\": datetime.now().isoformat()})\n",
    "            ]\n",
    "            self.vector_store = FAISS.from_documents(initial_docs, embeddings)\n",
    "    \n",
    "    def add_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        \"\"\"Add new memory to the vector store.\"\"\"\n",
    "        try:\n",
    "            doc = Document(page_content=content, metadata=metadata)\n",
    "            self.vector_store.add_documents([doc])\n",
    "            self.save_memory()\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding memory: {e}\")\n",
    "    \n",
    "    def search_memory(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Search for relevant memories.\"\"\"\n",
    "        try:\n",
    "            return self.vector_store.similarity_search(query, k=k)\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching memory: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_memory(self):\n",
    "        \"\"\"Save memory to disk.\"\"\"\n",
    "        try:\n",
    "            self.vector_store.save_local(self.memory_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memory: {e}\")\n",
    "\n",
    "# Initialize global memory\n",
    "agent_memory = AgentMemory()\n",
    "print(\"Agent memory system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43182421",
   "metadata": {},
   "source": [
    "## 4. Base Agent Class with Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eca9fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Investment Research Agent class created!\n"
     ]
    }
   ],
   "source": [
    "class InvestmentResearchAgent:\n",
    "    \"\"\"Base Investment Research Agent with planning, tool usage, self-reflection, and learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, role: str, memory: AgentMemory):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.memory = memory\n",
    "        self.llm = llm\n",
    "        self.session_memory = ConversationBufferWindowMemory(\n",
    "            k=10, \n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        )\n",
    "        self.tools = [get_stock_data, get_stock_news, get_economic_data, get_alpha_vantage_data]\n",
    "        self.execution_log = []\n",
    "        \n",
    "    def plan_research(self, task: str) -> List[str]:\n",
    "        \"\"\"Plan research steps for a given task.\"\"\"\n",
    "        planning_prompt = PromptConfiguration.get_planning_prompt(self.role, task)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=planning_prompt)])\n",
    "            plan_text = response.content\n",
    "            \n",
    "            # Extract numbered steps\n",
    "            steps = []\n",
    "            for line in plan_text.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and (line[0].isdigit() or line.startswith('-') or line.startswith('*')):\n",
    "                    steps.append(line)\n",
    "            \n",
    "            # Log the plan\n",
    "            self.execution_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'action': 'plan_created',\n",
    "                'task': task,\n",
    "                'plan': steps\n",
    "            })\n",
    "            \n",
    "            return steps\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in planning: {e}\")\n",
    "            return [\"1. Gather basic stock data\", \"2. Analyze recent news\", \"3. Review economic context\", \"4. Synthesize findings\"]\n",
    "    \n",
    "    def use_tool_dynamically(self, tool_name: str, **kwargs) -> str:\n",
    "        \"\"\"Use tools dynamically based on context.\"\"\"\n",
    "        tool_map = {\n",
    "            'stock_data': get_stock_data,\n",
    "            'news': get_stock_news,\n",
    "            'economic': get_economic_data,\n",
    "            'alpha_vantage': get_alpha_vantage_data\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if tool_name in tool_map:\n",
    "                result = tool_map[tool_name].invoke(kwargs)\n",
    "                \n",
    "                # Log tool usage\n",
    "                self.execution_log.append({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'action': 'tool_used',\n",
    "                    'tool': tool_name,\n",
    "                    'parameters': kwargs,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "                return result\n",
    "            else:\n",
    "                return f\"Tool '{tool_name}' not available\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.execution_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'action': 'tool_used',\n",
    "                'tool': tool_name,\n",
    "                'parameters': kwargs,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            return f\"Error using tool '{tool_name}': {str(e)}\"\n",
    "    \n",
    "    def self_reflect(self, analysis_result: str) -> Dict[str, Any]:\n",
    "        \"\"\"Self-reflect on the quality of analysis output.\"\"\"\n",
    "        reflection_prompt = PromptConfiguration.get_reflection_prompt(analysis_result)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=reflection_prompt)])\n",
    "            reflection_text = response.content\n",
    "            \n",
    "            # Try to parse as JSON, fallback to structured text parsing\n",
    "            try:\n",
    "                reflection_data = json.loads(reflection_text)\n",
    "            except:\n",
    "                # Fallback parsing\n",
    "                reflection_data = {\n",
    "                    'overall_score': 7,\n",
    "                    'strengths': [\"Analysis provided\"],\n",
    "                    'improvements': [\"Could be more detailed\"],\n",
    "                    'recommendations': [\"Gather more data points\"]\n",
    "                }\n",
    "            \n",
    "            # Log reflection\n",
    "            self.execution_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'action': 'self_reflection',\n",
    "                'reflection': reflection_data\n",
    "            })\n",
    "            \n",
    "            return reflection_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in self-reflection: {e}\")\n",
    "            return {\n",
    "                'overall_score': 5,\n",
    "                'strengths': [\"Attempt made\"],\n",
    "                'improvements': [\"Technical issues encountered\"],\n",
    "                'recommendations': [\"Retry analysis\"]\n",
    "            }\n",
    "    \n",
    "    def learn_from_experience(self, task: str, result: str, reflection: Dict[str, Any]):\n",
    "        \"\"\"Learn from the current analysis and store insights for future use.\"\"\"\n",
    "        try:\n",
    "            # Create learning content\n",
    "            learning_content = f\"\"\"\n",
    "            Task: {task}\n",
    "            Analysis Quality Score: {reflection.get('overall_score', 'N/A')}\n",
    "            Key Insights: {', '.join(reflection.get('strengths', []))}\n",
    "            Improvement Areas: {', '.join(reflection.get('improvements', []))}\n",
    "            Recommendations: {', '.join(reflection.get('recommendations', []))}\n",
    "            Execution Log: {len(self.execution_log)} actions taken\n",
    "            \"\"\"\n",
    "            \n",
    "            # Store in memory\n",
    "            metadata = {\n",
    "                'type': 'learning_experience',\n",
    "                'agent': self.name,\n",
    "                'task': task,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'quality_score': reflection.get('overall_score', 0)\n",
    "            }\n",
    "            \n",
    "            self.memory.add_memory(learning_content, metadata)\n",
    "            \n",
    "            print(f\"{self.name} learned from experience (Score: {reflection.get('overall_score', 'N/A')})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in learning: {e}\")\n",
    "    \n",
    "    def retrieve_relevant_experience(self, task: str) -> List[str]:\n",
    "        \"\"\"Retrieve relevant past experiences for the current task.\"\"\"\n",
    "        try:\n",
    "            memories = self.memory.search_memory(task, k=3)\n",
    "            experiences = []\n",
    "            \n",
    "            for memory in memories:\n",
    "                if memory.metadata.get('type') == 'learning_experience':\n",
    "                    experiences.append(memory.page_content)\n",
    "            \n",
    "            return experiences\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving experience: {e}\")\n",
    "            return []\n",
    "\n",
    "print(\"Base Investment Research Agent class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584927e",
   "metadata": {},
   "source": [
    "## 5. Workflow Pattern 1: Prompt Chaining (News Processing Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "709ceefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Processing Chain (Prompt Chaining) created!\n"
     ]
    }
   ],
   "source": [
    "class NewsProcessingChain:\n",
    "    \"\"\"Implements Prompt Chaining: Ingest  Preprocess  Classify  Extract  Summarize\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def ingest_news(self, symbol: str) -> List[Dict]:\n",
    "        \"\"\"Step 1: Ingest news data\"\"\"\n",
    "        try:\n",
    "            news_data = get_stock_news.invoke({\"symbol\": symbol, \"days\": 7})\n",
    "            \n",
    "            # Handle empty or None response\n",
    "            if not news_data or news_data.strip() == \"\":\n",
    "                print(f\"No news data returned for {symbol}\")\n",
    "                return []\n",
    "            \n",
    "            # Check if response is an error message (string starting with \"Error\")\n",
    "            if isinstance(news_data, str) and news_data.startswith(\"Error\"):\n",
    "                print(f\"News API error: {news_data}\")\n",
    "                return []\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                parsed_data = json.loads(news_data)\n",
    "                \n",
    "                # Handle different response formats\n",
    "                if isinstance(parsed_data, list):\n",
    "                    return parsed_data\n",
    "                elif isinstance(parsed_data, dict):\n",
    "                    # If it's a dict with news items, extract them\n",
    "                    if 'news' in parsed_data:\n",
    "                        return parsed_data['news']\n",
    "                    elif 'articles' in parsed_data:\n",
    "                        return parsed_data['articles']\n",
    "                    else:\n",
    "                        # Return as single item list\n",
    "                        return [parsed_data]\n",
    "                else:\n",
    "                    print(f\"Unexpected news data format: {type(parsed_data)}\")\n",
    "                    return []\n",
    "                    \n",
    "            except json.JSONDecodeError as je:\n",
    "                print(f\"JSON parsing error: {je}\")\n",
    "                print(f\"Raw response: {news_data[:200]}...\")  # Show first 200 chars\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error ingesting news: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def preprocess_news(self, news_articles: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Step 2: Preprocess and clean news articles\"\"\"\n",
    "        processed_articles = []\n",
    "        \n",
    "        for article in news_articles:\n",
    "            # Clean and structure the article\n",
    "            processed_article = {\n",
    "                'title': article.get('title', '').strip(),\n",
    "                'description': article.get('description', '').strip(),\n",
    "                'source': article.get('source', 'Unknown'),\n",
    "                'published_at': article.get('published_at', ''),\n",
    "                'url': article.get('url', ''),\n",
    "                'combined_text': f\"{article.get('title', '')} {article.get('description', '')}\".strip()\n",
    "            }\n",
    "            \n",
    "            # Only include articles with meaningful content\n",
    "            if processed_article['combined_text'] and len(processed_article['combined_text']) > 20:\n",
    "                processed_articles.append(processed_article)\n",
    "        \n",
    "        return processed_articles\n",
    "    \n",
    "    def classify_news(self, processed_articles: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Step 3: Classify news articles by type and sentiment\"\"\"\n",
    "        classified_articles = []\n",
    "        \n",
    "        for article in processed_articles:\n",
    "            classify_prompt = PromptConfiguration.get_news_classification_prompt(\n",
    "                article['title'], \n",
    "                article['description']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                response = self.llm.invoke([HumanMessage(content=classify_prompt)])\n",
    "                classification_text = response.content\n",
    "                \n",
    "                # Try to parse JSON, fallback to default values\n",
    "                try:\n",
    "                    classification = json.loads(classification_text)\n",
    "                except:\n",
    "                    classification = {\n",
    "                        \"category\": \"other\",\n",
    "                        \"sentiment\": \"neutral\",\n",
    "                        \"importance\": \"medium\",\n",
    "                        \"reasoning\": \"Classification parsing failed\"\n",
    "                    }\n",
    "                \n",
    "                article.update(classification)\n",
    "                classified_articles.append(article)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error classifying article: {e}\")\n",
    "                article.update({\n",
    "                    \"category\": \"other\",\n",
    "                    \"sentiment\": \"neutral\",\n",
    "                    \"importance\": \"medium\",\n",
    "                    \"reasoning\": \"Error in classification\"\n",
    "                })\n",
    "                classified_articles.append(article)\n",
    "        \n",
    "        return classified_articles\n",
    "    \n",
    "    def extract_insights(self, classified_articles: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Step 4: Extract key insights from classified articles\"\"\"\n",
    "        classified_articles_str = json.dumps(classified_articles, indent=2)[:2000]  # Truncate to avoid token limits\n",
    "        insights_prompt = PromptConfiguration.get_insights_extraction_prompt(classified_articles_str)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=insights_prompt)])\n",
    "            insights_text = response.content\n",
    "            \n",
    "            try:\n",
    "                insights = json.loads(insights_text)\n",
    "            except:\n",
    "                # Fallback insights\n",
    "                sentiment_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
    "                for article in classified_articles:\n",
    "                    sentiment = article.get('sentiment', 'neutral')\n",
    "                    sentiment_counts[sentiment] += 1\n",
    "                \n",
    "                insights = {\n",
    "                    \"key_themes\": [\"General market activity\"],\n",
    "                    \"sentiment_distribution\": sentiment_counts,\n",
    "                    \"high_importance_items\": [item['title'] for item in classified_articles if item.get('importance') == 'high'],\n",
    "                    \"potential_catalysts\": [\"Market developments\"],\n",
    "                    \"risk_factors\": [\"Market volatility\"]\n",
    "                }\n",
    "            \n",
    "            return insights\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting insights: {e}\")\n",
    "            return {\n",
    "                \"key_themes\": [\"Analysis error\"],\n",
    "                \"sentiment_distribution\": {\"positive\": 0, \"negative\": 0, \"neutral\": len(classified_articles)},\n",
    "                \"high_importance_items\": [],\n",
    "                \"potential_catalysts\": [],\n",
    "                \"risk_factors\": [\"Analysis uncertainty\"]\n",
    "            }\n",
    "    \n",
    "    def summarize_analysis(self, insights: Dict[str, Any], symbol: str) -> str:\n",
    "        \"\"\"Step 5: Summarize the complete news analysis\"\"\"\n",
    "        insights_str = json.dumps(insights, indent=2)\n",
    "        summarize_prompt = PromptConfiguration.get_news_summarization_prompt(insights_str, symbol)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=summarize_prompt)])\n",
    "            return response.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization: {e}\")\n",
    "            return f\"News analysis for {symbol} completed with {len(insights.get('key_themes', []))} key themes identified. Sentiment distribution shows mixed signals. Further analysis recommended.\"\n",
    "    \n",
    "    def process_news_chain(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the complete news processing chain\"\"\"\n",
    "        print(f\"Starting news processing chain for {symbol}...\")\n",
    "        \n",
    "        # Step 1: Ingest\n",
    "        print(\"Step 1: Ingesting news...\")\n",
    "        raw_news = self.ingest_news(symbol)\n",
    "        \n",
    "        # Step 2: Preprocess\n",
    "        print(\"Step 2: Preprocessing news...\")\n",
    "        processed_news = self.preprocess_news(raw_news)\n",
    "        \n",
    "        # Step 3: Classify\n",
    "        print(\"Step 3: Classifying news...\")\n",
    "        classified_news = self.classify_news(processed_news)\n",
    "        \n",
    "        # Step 4: Extract\n",
    "        print(\"Step 4: Extracting insights...\")\n",
    "        insights = self.extract_insights(classified_news)\n",
    "        \n",
    "        # Step 5: Summarize\n",
    "        print(\"Step 5: Creating summary...\")\n",
    "        summary = self.summarize_analysis(insights, symbol)\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'raw_articles_count': len(raw_news),\n",
    "            'processed_articles_count': len(processed_news),\n",
    "            'classified_articles': classified_news,\n",
    "            'insights': insights,\n",
    "            'summary': summary,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize news processing chain\n",
    "news_chain = NewsProcessingChain(llm)\n",
    "print(\"News Processing Chain (Prompt Chaining) created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff796d",
   "metadata": {},
   "source": [
    "## 6. Workflow Pattern 2: Routing (Specialist Agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d469dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing System (Specialist Agents) created!\n"
     ]
    }
   ],
   "source": [
    "class SpecialistAgent(InvestmentResearchAgent):\n",
    "    \"\"\"Base class for specialist agents\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, role: str, specialization: str, memory: AgentMemory):\n",
    "        super().__init__(name, role, memory)\n",
    "        self.specialization = specialization\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform specialized analysis - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TechnicalAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for technical analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"TechnicalAnalyst\", \"Technical Analysis Specialist\", \"technical_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform technical analysis\"\"\"\n",
    "        try:\n",
    "            # Get stock data for technical analysis\n",
    "            stock_data = self.use_tool_dynamically('stock_data', symbol=symbol, period='6mo')\n",
    "            \n",
    "            technical_prompt = PromptConfiguration.get_technical_analysis_prompt(symbol, stock_data)\n",
    "            \n",
    "            response = self.llm.invoke([HumanMessage(content=technical_prompt)])\n",
    "            analysis = response.content\n",
    "            \n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'technical',\n",
    "                'symbol': symbol,\n",
    "                'analysis': analysis,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['stock_price_data', 'volume_data']\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'technical',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"Technical analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True\n",
    "            }\n",
    "\n",
    "class FundamentalAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for fundamental analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"FundamentalAnalyst\", \"Fundamental Analysis Specialist\", \"fundamental_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform fundamental analysis\"\"\"\n",
    "        try:\n",
    "            # Get fundamental data\n",
    "            stock_data = self.use_tool_dynamically('stock_data', symbol=symbol)\n",
    "            alpha_overview = self.use_tool_dynamically('alpha_vantage', symbol=symbol, function='OVERVIEW')\n",
    "            \n",
    "            fundamental_prompt = PromptConfiguration.get_fundamental_analysis_prompt(symbol, stock_data, alpha_overview)\n",
    "            \n",
    "            response = self.llm.invoke([HumanMessage(content=fundamental_prompt)])\n",
    "            analysis = response.content\n",
    "            \n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'fundamental',\n",
    "                'symbol': symbol,\n",
    "                'analysis': analysis,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['company_financials', 'market_data', 'ratios']\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'fundamental',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"Fundamental analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True\n",
    "            }\n",
    "\n",
    "class NewsAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for news and sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"NewsAnalyst\", \"News and Sentiment Analysis Specialist\", \"news_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform news and sentiment analysis\"\"\"\n",
    "        try:\n",
    "            # Use the news processing chain\n",
    "            news_analysis = news_chain.process_news_chain(symbol)\n",
    "            \n",
    "            news_analysis_str = json.dumps(news_analysis, indent=2)[:1500]\n",
    "            sentiment_prompt = PromptConfiguration.get_sentiment_analysis_prompt(symbol, news_analysis_str)\n",
    "            \n",
    "            response = self.llm.invoke([HumanMessage(content=sentiment_prompt)])\n",
    "            enhanced_analysis = response.content\n",
    "            \n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'news_sentiment',\n",
    "                'symbol': symbol,\n",
    "                'analysis': enhanced_analysis,\n",
    "                'raw_news_analysis': news_analysis,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['news_articles', 'sentiment_data']\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'news_sentiment',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"News analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True\n",
    "            }\n",
    "\n",
    "class RoutingCoordinator:\n",
    "    \"\"\"Coordinates routing of analysis tasks to appropriate specialists\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        self.memory = memory\n",
    "        self.specialists = {\n",
    "            'technical': TechnicalAnalyst(memory),\n",
    "            'fundamental': FundamentalAnalyst(memory),\n",
    "            'news': NewsAnalyst(memory)\n",
    "        }\n",
    "        self.llm = llm\n",
    "    \n",
    "    def route_analysis(self, request: str, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route analysis request to appropriate specialists\"\"\"\n",
    "        routing_prompt = PromptConfiguration.get_routing_prompt(request, symbol)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=routing_prompt)])\n",
    "            routing_text = response.content\n",
    "            \n",
    "            try:\n",
    "                routing_decision = json.loads(routing_text)\n",
    "            except:\n",
    "                # Default routing - use all specialists\n",
    "                routing_decision = {\n",
    "                    \"specialists_needed\": [\"technical\", \"fundamental\", \"news\"],\n",
    "                    \"priority_order\": [\"fundamental\", \"technical\", \"news\"],\n",
    "                    \"reasoning\": \"Default comprehensive analysis\"\n",
    "                }\n",
    "            \n",
    "            # Execute analysis with selected specialists\n",
    "            results = {}\n",
    "            for specialist_type in routing_decision.get('specialists_needed', []):\n",
    "                if specialist_type in self.specialists:\n",
    "                    print(f\"Routing to {specialist_type} specialist...\")\n",
    "                    specialist_result = self.specialists[specialist_type].analyze({}, symbol)\n",
    "                    results[specialist_type] = specialist_result\n",
    "            \n",
    "            return {\n",
    "                'routing_decision': routing_decision,\n",
    "                'specialist_analyses': results,\n",
    "                'symbol': symbol,\n",
    "                'request': request,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in routing: {e}\")\n",
    "            return {\n",
    "                'error': f\"Routing error: {str(e)}\",\n",
    "                'symbol': symbol,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize routing system\n",
    "routing_coordinator = RoutingCoordinator(agent_memory)\n",
    "print(\"Routing System (Specialist Agents) created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2e8ef",
   "metadata": {},
   "source": [
    "## 7. Workflow Pattern 3: Evaluator-Optimizer (Analysis Refinement Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f860643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator-Optimizer system created!\n"
     ]
    }
   ],
   "source": [
    "class EvaluatorOptimizer:\n",
    "    \"\"\"Implements Evaluator-Optimizer pattern: Generate  Evaluate  Refine\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, memory: AgentMemory):\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        self.max_iterations = 3\n",
    "    \n",
    "    def generate_initial_analysis(self, symbol: str, specialist_analyses: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate initial comprehensive analysis\"\"\"\n",
    "        specialist_analyses_str = json.dumps(specialist_analyses, indent=2)[:3000]\n",
    "        generate_prompt = PromptConfiguration.get_analysis_generation_prompt(symbol, specialist_analyses_str)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=generate_prompt)])\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating initial analysis: {str(e)}\"\n",
    "    \n",
    "    def evaluate_analysis_quality(self, analysis: str, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the quality of the analysis using Azure OpenAI\"\"\"\n",
    "        evaluation_prompt = PromptConfiguration.get_evaluation_prompt(analysis, symbol)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=evaluation_prompt)])\n",
    "            evaluation_text = response.content\n",
    "            \n",
    "            try:\n",
    "                evaluation = json.loads(evaluation_text)\n",
    "            except:\n",
    "                # Fallback evaluation\n",
    "                evaluation = {\n",
    "                    \"scores\": {\n",
    "                        \"completeness\": 7,\n",
    "                        \"data_integration\": 6,\n",
    "                        \"risk_assessment\": 6,\n",
    "                        \"actionability\": 7,\n",
    "                        \"logic_reasoning\": 7,\n",
    "                        \"market_context\": 6,\n",
    "                        \"clarity\": 7\n",
    "                    },\n",
    "                    \"overall_score\": 6.5,\n",
    "                    \"grade\": \"B\",\n",
    "                    \"strengths\": [\"Basic analysis provided\"],\n",
    "                    \"weaknesses\": [\"Could be more detailed\"],\n",
    "                    \"specific_improvements\": [\"Add more quantitative analysis\"],\n",
    "                    \"missing_elements\": [\"Market comparisons\"]\n",
    "                }\n",
    "            \n",
    "            return evaluation\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\")\n",
    "            return {\n",
    "                \"overall_score\": 5,\n",
    "                \"grade\": \"C\",\n",
    "                \"strengths\": [\"Attempt made\"],\n",
    "                \"weaknesses\": [\"Evaluation error\"],\n",
    "                \"specific_improvements\": [\"Retry analysis\"],\n",
    "                \"missing_elements\": [\"Complete analysis\"]\n",
    "            }\n",
    "    \n",
    "    def refine_analysis(self, original_analysis: str, evaluation: Dict[str, Any], symbol: str) -> str:\n",
    "        \"\"\"Refine analysis based on evaluation feedback\"\"\"\n",
    "        evaluation_str = json.dumps(evaluation, indent=2)\n",
    "        refinement_prompt = PromptConfiguration.get_refinement_prompt(original_analysis, evaluation_str, symbol)\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=refinement_prompt)])\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error in refinement: {str(e)}. Original analysis maintained.\"\n",
    "    \n",
    "    def optimize_analysis(self, symbol: str, specialist_analyses: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the complete evaluator-optimizer loop\"\"\"\n",
    "        print(f\"Starting analysis optimization for {symbol}...\")\n",
    "        \n",
    "        iterations = []\n",
    "        current_analysis = self.generate_initial_analysis(symbol, specialist_analyses)\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"Optimization iteration {iteration + 1}/{self.max_iterations}\")\n",
    "            \n",
    "            # Evaluate current analysis\n",
    "            evaluation = self.evaluate_analysis_quality(current_analysis, symbol)\n",
    "            \n",
    "            iteration_data = {\n",
    "                'iteration': iteration + 1,\n",
    "                'analysis': current_analysis,\n",
    "                'evaluation': evaluation,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Check if quality is acceptable (grade A or B, or score > 8)\n",
    "            overall_score = evaluation.get('overall_score', 0)\n",
    "            grade = evaluation.get('grade', 'F')\n",
    "            \n",
    "            if grade in ['A', 'B'] or overall_score >= 8.0:\n",
    "                print(f\"Analysis quality acceptable (Grade: {grade}, Score: {overall_score})\")\n",
    "                iteration_data['optimization_complete'] = True\n",
    "                iterations.append(iteration_data)\n",
    "                break\n",
    "            \n",
    "            # Refine analysis if quality is not acceptable\n",
    "            if iteration < self.max_iterations - 1:  # Don't refine on last iteration\n",
    "                print(f\"Refining analysis (Grade: {grade}, Score: {overall_score})\")\n",
    "                current_analysis = self.refine_analysis(current_analysis, evaluation, symbol)\n",
    "                iteration_data['refinement_applied'] = True\n",
    "            else:\n",
    "                print(f\"Maximum iterations reached. Final grade: {grade}, Score: {overall_score}\")\n",
    "                iteration_data['max_iterations_reached'] = True\n",
    "            \n",
    "            iterations.append(iteration_data)\n",
    "        \n",
    "        # Store learning in memory\n",
    "        final_evaluation = iterations[-1]['evaluation']\n",
    "        learning_content = f\"\"\"\n",
    "        Analysis Optimization for {symbol}:\n",
    "        Iterations: {len(iterations)}\n",
    "        Final Score: {final_evaluation.get('overall_score', 'N/A')}\n",
    "        Final Grade: {final_evaluation.get('grade', 'N/A')}\n",
    "        Key Learnings: {', '.join(final_evaluation.get('strengths', []))}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.add_memory(learning_content, {\n",
    "            'type': 'optimization_learning',\n",
    "            'symbol': symbol,\n",
    "            'final_score': final_evaluation.get('overall_score', 0),\n",
    "            'iterations': len(iterations),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'optimization_iterations': iterations,\n",
    "            'final_analysis': current_analysis,\n",
    "            'final_evaluation': final_evaluation,\n",
    "            'improvement_achieved': len(iterations) > 1,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize evaluator-optimizer\n",
    "evaluator_optimizer = EvaluatorOptimizer(llm, agent_memory)\n",
    "print(\"Evaluator-Optimizer system created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eeca8",
   "metadata": {},
   "source": [
    "## 8. Main Investment Research Agent Coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9b40e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Investment Research Agent Coordinator initialized!\n"
     ]
    }
   ],
   "source": [
    "class MainInvestmentResearchAgent(InvestmentResearchAgent):\n",
    "    \"\"\"Main coordinator agent that orchestrates all workflows and patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"MainCoordinator\", \"Investment Research Coordinator\", memory)\n",
    "        self.routing_coordinator = routing_coordinator\n",
    "        self.evaluator_optimizer = evaluator_optimizer\n",
    "        self.news_chain = news_chain\n",
    "        \n",
    "    def conduct_comprehensive_research(self, symbol: str, request: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Conduct comprehensive investment research using all three workflow patterns:\n",
    "        1. Prompt Chaining (News Processing)\n",
    "        2. Routing (Specialist Analysis) \n",
    "        3. Evaluator-Optimizer (Analysis Refinement)\n",
    "        \"\"\"\n",
    "        print(f\"\\\\nStarting comprehensive investment research for {symbol}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Planning\n",
    "        if not request:\n",
    "            request = f\"Conduct comprehensive investment analysis for {symbol} including technical, fundamental, and sentiment analysis\"\n",
    "        \n",
    "        research_plan = self.plan_research(request)\n",
    "        print(f\"\\\\nResearch Plan Created ({len(research_plan)} steps)\")\n",
    "        \n",
    "        # Step 2: Retrieve past experience\n",
    "        past_experiences = self.retrieve_relevant_experience(request)\n",
    "        if past_experiences:\n",
    "            print(f\"Retrieved {len(past_experiences)} relevant past experiences\")\n",
    "        \n",
    "        # Step 3: Execute Routing Workflow (Specialist Analysis)\n",
    "        print(f\"\\\\nExecuting Routing Workflow...\")\n",
    "        routing_results = self.routing_coordinator.route_analysis(request, symbol)\n",
    "        \n",
    "        # Step 4: Execute Prompt Chaining (News Processing) - included in news specialist\n",
    "        print(f\"\\\\nNews Processing Chain completed within specialist analysis\")\n",
    "        \n",
    "        # Step 5: Execute Evaluator-Optimizer Workflow\n",
    "        print(f\"\\\\nExecuting Evaluator-Optimizer Workflow...\")\n",
    "        specialist_analyses = routing_results.get('specialist_analyses', {})\n",
    "        optimization_results = self.evaluator_optimizer.optimize_analysis(symbol, specialist_analyses)\n",
    "        \n",
    "        # Step 6: Self-reflect on overall process\n",
    "        print(f\"\\\\nConducting self-reflection...\")\n",
    "        final_analysis = optimization_results.get('final_analysis', '')\n",
    "        reflection = self.self_reflect(final_analysis)\n",
    "        \n",
    "        # Step 7: Learn from this experience\n",
    "        print(f\"\\\\nLearning from this research experience...\")\n",
    "        self.learn_from_experience(request, final_analysis, reflection)\n",
    "        \n",
    "        # Compile comprehensive results\n",
    "        comprehensive_results = {\n",
    "            'symbol': symbol,\n",
    "            'request': request,\n",
    "            'research_plan': research_plan,\n",
    "            'past_experiences': past_experiences,\n",
    "            'routing_results': routing_results,\n",
    "            'optimization_results': optimization_results,\n",
    "            'self_reflection': reflection,\n",
    "            'execution_log': self.execution_log,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'agent_name': self.name\n",
    "        }\n",
    "        \n",
    "        print(f\"\\\\nComprehensive research completed!\")\n",
    "        print(f\"Final Analysis Quality Score: {reflection.get('overall_score', 'N/A')}/10\")\n",
    "        \n",
    "        return comprehensive_results\n",
    "    \n",
    "    def generate_investment_report(self, research_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a formatted investment report from research results\"\"\"\n",
    "        symbol = research_results.get('symbol', 'N/A')\n",
    "        final_analysis = research_results.get('optimization_results', {}).get('final_analysis', '')\n",
    "        reflection = research_results.get('self_reflection', {})\n",
    "        \n",
    "        report_template = f\"\"\"\n",
    "        # Investment Research Report: {symbol}\n",
    "        **Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        **Research Quality Score:** {reflection.get('overall_score', 'N/A')}/10\n",
    "\n",
    "        ## Executive Summary\n",
    "        {final_analysis[:500]}...\n",
    "\n",
    "        ## Key Research Insights\n",
    "        - **Research Plan Execution:** {len(research_results.get('research_plan', []))} planned steps completed\n",
    "        - **Specialist Analyses:** {len(research_results.get('routing_results', {}).get('specialist_analyses', {}))} specialist reports generated\n",
    "        - **Analysis Iterations:** {len(research_results.get('optimization_results', {}).get('optimization_iterations', []))} optimization cycles\n",
    "        - **Quality Improvements:** {'Yes' if research_results.get('optimization_results', {}).get('improvement_achieved', False) else 'No'}\n",
    "\n",
    "        ## Analysis Quality Assessment\n",
    "        - **Strengths:** {', '.join(reflection.get('strengths', ['Analysis completed']))}\n",
    "        - **Recommendations:** {', '.join(reflection.get('recommendations', ['Continue monitoring']))}\n",
    "\n",
    "        ## Full Analysis\n",
    "        {final_analysis}\n",
    "\n",
    "        ---\n",
    "        *This report was generated by an AI Investment Research Agent.*\n",
    "        \"\"\"\n",
    "        \n",
    "        return report_template\n",
    "\n",
    "# Initialize main research agent\n",
    "main_research_agent = MainInvestmentResearchAgent(agent_memory)\n",
    "print(\"Main Investment Research Agent Coordinator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa07c98",
   "metadata": {},
   "source": [
    "## 9. Visualization and Reporting Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dcd920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Visualizer created!\n"
     ]
    }
   ],
   "source": [
    "class InvestmentVisualizer:\n",
    "    \"\"\"Create visualizations for investment research results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        plt.style.use('default')\n",
    "        sns.set_theme()\n",
    "    \n",
    "    def create_stock_price_chart(self, symbol: str, period: str = \"6mo\"):\n",
    "        \"\"\"Create stock price chart with volume\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(symbol)\n",
    "            hist = stock.history(period=period)\n",
    "            \n",
    "            if hist.empty:\n",
    "                return None\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), \n",
    "                                          gridspec_kw={'height_ratios': [3, 1]})\n",
    "            \n",
    "            # Price chart\n",
    "            ax1.plot(hist.index, hist['Close'], linewidth=2, color='blue', label='Close Price')\n",
    "            ax1.fill_between(hist.index, hist['Low'], hist['High'], alpha=0.3, color='lightblue')\n",
    "            ax1.set_title(f'{symbol} Stock Price - {period.upper()}', fontsize=16, fontweight='bold')\n",
    "            ax1.set_ylabel('Price ($)', fontsize=12)\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Volume chart\n",
    "            ax2.bar(hist.index, hist['Volume'], alpha=0.7, color='orange')\n",
    "            ax2.set_title('Trading Volume', fontsize=12)\n",
    "            ax2.set_ylabel('Volume', fontsize=10)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating price chart: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_sentiment_analysis_chart(self, news_analysis: Dict[str, Any]):\n",
    "        \"\"\"Create sentiment analysis visualization\"\"\"\n",
    "        try:\n",
    "            insights = news_analysis.get('insights', {})\n",
    "            sentiment_dist = insights.get('sentiment_distribution', {})\n",
    "            \n",
    "            if not sentiment_dist:\n",
    "                return None\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Sentiment pie chart\n",
    "            labels = list(sentiment_dist.keys())\n",
    "            sizes = list(sentiment_dist.values())\n",
    "            colors = ['green', 'red', 'gray']\n",
    "            \n",
    "            ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            ax1.set_title('News Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Key themes bar chart\n",
    "            themes = insights.get('key_themes', [])[:5]  # Top 5 themes\n",
    "            if themes:\n",
    "                ax2.barh(range(len(themes)), [1] * len(themes), color='steelblue')\n",
    "                ax2.set_yticks(range(len(themes)))\n",
    "                ax2.set_yticklabels(themes)\n",
    "                ax2.set_title('Key News Themes', fontsize=14, fontweight='bold')\n",
    "                ax2.set_xlabel('Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating sentiment chart: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_analysis_quality_dashboard(self, optimization_results: Dict[str, Any]):\n",
    "        \"\"\"Create analysis quality improvement dashboard\"\"\"\n",
    "        try:\n",
    "            iterations = optimization_results.get('optimization_iterations', [])\n",
    "            if not iterations:\n",
    "                return None\n",
    "            \n",
    "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "            # Quality score progression\n",
    "            scores = [iter_data['evaluation'].get('overall_score', 0) for iter_data in iterations]\n",
    "            ax1.plot(range(1, len(scores) + 1), scores, marker='o', linewidth=2, markersize=8)\n",
    "            ax1.set_title('Analysis Quality Score Progression', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Iteration')\n",
    "            ax1.set_ylabel('Quality Score (1-10)')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.set_ylim(0, 10)\n",
    "            \n",
    "            # Final evaluation scores\n",
    "            if iterations:\n",
    "                final_eval = iterations[-1]['evaluation']\n",
    "                categories = list(final_eval.get('scores', {}).keys())\n",
    "                values = list(final_eval.get('scores', {}).values())\n",
    "                \n",
    "                bars = ax2.bar(range(len(categories)), values, color='steelblue')\n",
    "                ax2.set_title('Final Analysis Quality Breakdown', fontsize=14, fontweight='bold')\n",
    "                ax2.set_ylabel('Score (1-10)')\n",
    "                ax2.set_xticks(range(len(categories)))\n",
    "                ax2.set_xticklabels(categories, rotation=45, ha='right')\n",
    "                ax2.set_ylim(0, 10)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, value in zip(bars, values):\n",
    "                    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                            f'{value:.1f}', ha='center', va='bottom')\n",
    "            \n",
    "            # Grade distribution (if multiple analyses)\n",
    "            ax3.text(0.5, 0.5, f\"Final Grade: {iterations[-1]['evaluation'].get('grade', 'N/A')}\", \n",
    "                    ha='center', va='center', fontsize=24, fontweight='bold',\n",
    "                    transform=ax3.transAxes)\n",
    "            ax3.set_title('Analysis Grade', fontsize=14, fontweight='bold')\n",
    "            ax3.axis('off')\n",
    "            \n",
    "            # Improvement areas\n",
    "            if iterations:\n",
    "                improvements = iterations[-1]['evaluation'].get('specific_improvements', [])[:5]\n",
    "                if improvements:\n",
    "                    ax4.barh(range(len(improvements)), [1] * len(improvements), color='orange')\n",
    "                    ax4.set_yticks(range(len(improvements)))\n",
    "                    ax4.set_yticklabels([imp[:30] + '...' if len(imp) > 30 else imp for imp in improvements])\n",
    "                    ax4.set_title('Key Improvement Areas', fontsize=14, fontweight='bold')\n",
    "                    ax4.set_xlabel('Priority')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating quality dashboard: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_comprehensive_dashboard(self, research_results: Dict[str, Any]):\n",
    "        \"\"\"Create comprehensive research dashboard\"\"\"\n",
    "        symbol = research_results.get('symbol', 'N/A')\n",
    "        \n",
    "        # Create individual charts\n",
    "        price_chart = self.create_stock_price_chart(symbol)\n",
    "        \n",
    "        # Get news analysis from routing results\n",
    "        routing_results = research_results.get('routing_results', {})\n",
    "        specialist_analyses = routing_results.get('specialist_analyses', {})\n",
    "        news_analysis = specialist_analyses.get('news', {}).get('raw_news_analysis', {})\n",
    "        sentiment_chart = self.create_sentiment_analysis_chart(news_analysis)\n",
    "        \n",
    "        optimization_results = research_results.get('optimization_results', {})\n",
    "        quality_dashboard = self.create_analysis_quality_dashboard(optimization_results)\n",
    "        \n",
    "        return {\n",
    "            'price_chart': price_chart,\n",
    "            'sentiment_chart': sentiment_chart,\n",
    "            'quality_dashboard': quality_dashboard\n",
    "        }\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = InvestmentVisualizer()\n",
    "print(\"Investment Visualizer created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2321f",
   "metadata": {},
   "source": [
    "## 10. Gradio Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279d139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio interface created! Ready to launch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def analyze_stock(symbol, analysis_type=\"Comprehensive Analysis\", include_visualizations=True):\n",
    "    \"\"\"Main function for Gradio interface\"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not symbol or len(symbol.strip()) == 0:\n",
    "            return \"ERROR: Please enter a valid stock symbol\", None, None, None, None\n",
    "        \n",
    "        symbol = symbol.upper().strip()\n",
    "        \n",
    "        # Create analysis request\n",
    "        request_mapping = {\n",
    "            \"Comprehensive Analysis\": f\"Conduct comprehensive investment analysis for {symbol} including technical, fundamental, and sentiment analysis\",\n",
    "            \"Technical Analysis Only\": f\"Perform detailed technical analysis for {symbol} focusing on price trends, indicators, and chart patterns\", \n",
    "            \"Fundamental Analysis Only\": f\"Conduct fundamental analysis for {symbol} focusing on financials, valuation, and business prospects\",\n",
    "            \"News & Sentiment Only\": f\"Analyze recent news and market sentiment for {symbol}\",\n",
    "            \"Quick Overview\": f\"Provide a quick investment overview for {symbol}\"\n",
    "        }\n",
    "        \n",
    "        request = request_mapping.get(analysis_type, request_mapping[\"Comprehensive Analysis\"])\n",
    "        \n",
    "        # Perform analysis\n",
    "        print(f\"Starting {analysis_type} for {symbol}...\")\n",
    "        research_results = main_research_agent.conduct_comprehensive_research(symbol, request)\n",
    "        \n",
    "        # Generate report\n",
    "        report = main_research_agent.generate_investment_report(research_results)\n",
    "        \n",
    "        # Create visualizations if requested\n",
    "        price_chart = None\n",
    "        sentiment_chart = None\n",
    "        quality_chart = None\n",
    "        \n",
    "        if include_visualizations:\n",
    "            try:\n",
    "                dashboard = visualizer.create_comprehensive_dashboard(research_results)\n",
    "                price_chart = dashboard.get('price_chart')\n",
    "                sentiment_chart = dashboard.get('sentiment_chart') \n",
    "                quality_chart = dashboard.get('quality_dashboard')\n",
    "            except Exception as e:\n",
    "                print(f\"Visualization error: {e}\")\n",
    "        \n",
    "        # Prepare summary stats\n",
    "        reflection = research_results.get('self_reflection', {})\n",
    "        optimization_results = research_results.get('optimization_results', {})\n",
    "        \n",
    "        summary_stats = f\"\"\"\n",
    "        **Analysis Summary:**\n",
    "        - Quality Score: {reflection.get('overall_score', 'N/A')}/10\n",
    "        - Analysis Grade: {optimization_results.get('final_evaluation', {}).get('grade', 'N/A')}\n",
    "        - Optimization Iterations: {len(optimization_results.get('optimization_iterations', []))}\n",
    "        - Specialists Consulted: {len(research_results.get('routing_results', {}).get('specialist_analyses', {}))}\n",
    "        - Research Plan Steps: {len(research_results.get('research_plan', []))}\n",
    "\n",
    "        **Workflow Patterns Executed:**\n",
    "        - Prompt Chaining (News Processing Pipeline)\n",
    "        - Routing (Specialist Agent Coordination) \n",
    "        - Evaluator-Optimizer (Quality Refinement Loop)\n",
    "\n",
    "        **Agent Capabilities Demonstrated:**\n",
    "        - Dynamic Planning & Tool Usage\n",
    "        - Self-Reflection & Quality Assessment  \n",
    "        - Learning & Memory Across Sessions\n",
    "        - Multi-Source Data Integration\n",
    "        \"\"\"\n",
    "        \n",
    "        return report, summary_stats, price_chart, sentiment_chart, quality_chart\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Analysis Error: {str(e)}\\n\\nPlease check the stock symbol and try again.\"\n",
    "        return error_msg, None, None, None, None\n",
    "\n",
    "# Create Gradio interface\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create and configure the Gradio web interface\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"AI Investment Research Agent\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # AI Investment Research Agent - Multi-Agent System\n",
    "        \n",
    "        **Advanced Investment Analysis using Three Workflow Patterns:**\n",
    "        \n",
    "        **Prompt Chaining**: News  Preprocess  Classify  Extract  Summarize  \n",
    "        **Routing**: Specialist Agents (Technical, Fundamental, News Analysis)  \n",
    "        **Evaluator-Optimizer**: Generate  Evaluate  Refine Analysis\n",
    "        \n",
    "        **Powered by:** Azure OpenAI, LangChain, Multi-Source Data Integration\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"## Analysis Configuration\")\n",
    "                \n",
    "                symbol_input = gr.Textbox(\n",
    "                    label=\"Stock Symbol\",\n",
    "                    placeholder=\"Enter stock symbol (e.g., AAPL, MSFT, TSLA)\",\n",
    "                    value=\"AAPL\"\n",
    "                )\n",
    "                \n",
    "                analysis_type = gr.Dropdown(\n",
    "                    choices=[\n",
    "                        \"Comprehensive Analysis\",\n",
    "                        \"Technical Analysis Only\", \n",
    "                        \"Fundamental Analysis Only\",\n",
    "                        \"News & Sentiment Only\",\n",
    "                        \"Quick Overview\"\n",
    "                    ],\n",
    "                    value=\"Comprehensive Analysis\",\n",
    "                    label=\"Analysis Type\"\n",
    "                )\n",
    "                \n",
    "                include_viz = gr.Checkbox(\n",
    "                    label=\"Include Visualizations\",\n",
    "                    value=True\n",
    "                )\n",
    "                \n",
    "                analyze_btn = gr.Button(\n",
    "                    \"Start Analysis\", \n",
    "                    variant=\"primary\",\n",
    "                    size=\"lg\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"\"\"\n",
    "                ### System Features:\n",
    "                - **Agent Functions**: Planning, Tool Usage, Self-Reflection, Learning\n",
    "                - **Data Sources**: Yahoo Finance, NewsAPI, FRED, Alpha Vantage\n",
    "                - **Memory System**: FAISS Vector Database\n",
    "                - **Quality Assurance**: Automated evaluation and refinement\n",
    "                \"\"\")\n",
    "            \n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"## Analysis Results\")\n",
    "                \n",
    "                summary_output = gr.Markdown(label=\"Analysis Summary\")\n",
    "                \n",
    "                with gr.Tabs():\n",
    "                    with gr.TabItem(\"Investment Report\"):\n",
    "                        report_output = gr.Markdown(\n",
    "                            label=\"Investment Analysis Report\",\n",
    "                            value=\"Run analysis to see results...\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.TabItem(\"Price Chart\"):\n",
    "                        price_plot = gr.Plot(label=\"Stock Price Analysis\")\n",
    "                    \n",
    "                    with gr.TabItem(\"Sentiment Analysis\"):\n",
    "                        sentiment_plot = gr.Plot(label=\"News Sentiment Analysis\")\n",
    "                    \n",
    "                    with gr.TabItem(\"Quality Dashboard\"):\n",
    "                        quality_plot = gr.Plot(label=\"Analysis Quality Metrics\")\n",
    "        \n",
    "        # Event handlers\n",
    "        analyze_btn.click(\n",
    "            fn=analyze_stock,\n",
    "            inputs=[symbol_input, analysis_type, include_viz],\n",
    "            outputs=[report_output, summary_output, price_plot, sentiment_plot, quality_plot]\n",
    "        )\n",
    "        \n",
    "        # Example inputs\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### Try These Examples:\n",
    "        - **AAPL**: Apple Inc. - Tech giant with strong fundamentals\n",
    "        - **TSLA**: Tesla Inc. - High volatility growth stock  \n",
    "        - **MSFT**: Microsoft Corp. - Stable large-cap technology\n",
    "        - **NVDA**: NVIDIA Corp. - AI and semiconductor leader\n",
    "        \"\"\")\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Create the interface\n",
    "gradio_demo = create_gradio_interface()\n",
    "print(\"Gradio interface created! Ready to launch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b28b4",
   "metadata": {},
   "source": [
    "## 11. Testing and Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5c91e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Investment Research Agent System\n",
      "==================================================\n",
      "\\n1. Testing data source tools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://sasw-mgfv7ds1-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://sasw-mgfv7ds1-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://sasw-mgfv7ds1-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock data tool working: 81 characters retrieved\n",
      "\\n2. Testing memory system...\n",
      "Memory system working: 1 memories retrieved\n",
      "\\n3. Testing news processing chain...\n",
      "News API error: Error fetching news for AAPL: HTTPSConnectionPool(host='newsapi.org', port=443): Max retries exceeded with url: /v2/everything?q=AAPL&from=2025-10-05&language=en&sortBy=relevancy&pageSize=10 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)')))\n",
      "News processing working: 0 articles processed\n",
      "\\n4. Testing main research agent...\n",
      "News API error: Error fetching news for AAPL: HTTPSConnectionPool(host='newsapi.org', port=443): Max retries exceeded with url: /v2/everything?q=AAPL&from=2025-10-05&language=en&sortBy=relevancy&pageSize=10 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)')))\n",
      "News processing working: 0 articles processed\n",
      "\\n4. Testing main research agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://sasw-mgfv7ds1-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-mini/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research planning working: 37 steps planned\n",
      "\\nSystem tests completed successfully!\n",
      "Ready for full analysis demonstration\n"
     ]
    }
   ],
   "source": [
    "# Test the system with a sample analysis\n",
    "def test_system():\n",
    "    \"\"\"Test the investment research agent system\"\"\"\n",
    "    print(\"Testing Investment Research Agent System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Basic tool functionality\n",
    "        print(\"\\\\n1. Testing data source tools...\")\n",
    "        stock_data = get_stock_data.invoke({\"symbol\": \"AAPL\", \"period\": \"1mo\"})\n",
    "        print(f\"Stock data tool working: {len(stock_data)} characters retrieved\")\n",
    "        \n",
    "        # Test 2: Memory system\n",
    "        print(\"\\\\n2. Testing memory system...\")\n",
    "        test_memory_content = \"Test memory entry for system validation\"\n",
    "        agent_memory.add_memory(test_memory_content, {\"type\": \"test\", \"timestamp\": datetime.now().isoformat()})\n",
    "        memories = agent_memory.search_memory(\"test\", k=1)\n",
    "        print(f\"Memory system working: {len(memories)} memories retrieved\")\n",
    "        \n",
    "        # Test 3: News processing chain\n",
    "        print(\"\\\\n3. Testing news processing chain...\")\n",
    "        try:\n",
    "            news_result = news_chain.ingest_news(\"AAPL\")\n",
    "            print(f\"News processing working: {len(news_result)} articles processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"News processing issue: {e}\")\n",
    "        \n",
    "        # Test 4: Agent coordination\n",
    "        print(\"\\\\n4. Testing main research agent...\")\n",
    "        # This is a lightweight test - full test would take longer\n",
    "        plan = main_research_agent.plan_research(\"Quick test analysis for AAPL\")\n",
    "        print(f\"Research planning working: {len(plan)} steps planned\")\n",
    "        \n",
    "        print(\"\\\\nSystem tests completed successfully!\")\n",
    "        print(\"Ready for full analysis demonstration\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"System test error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run system tests\n",
    "test_success = test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f530f90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "if test_success:\n",
    "    \n",
    "    # Launch the interface\n",
    "    # gradio_demo.launch(share=True, debug=True)\n",
    "    \n",
    "    # For notebook environment, use queue\n",
    "    gradio_demo.queue().launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        share=False,\n",
    "        debug=True,\n",
    "        show_error=True\n",
    "    )\n",
    "else:\n",
    "    print(\"System tests failed. Please check the configuration and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
