{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8f6fdf",
   "metadata": {},
   "source": [
    "# Investment Research Agent - Multi-Agent System\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements an autonomous Investment Research Agent that demonstrates:\n",
    "\n",
    "## Architecture\n",
    "- **Multi-Agent System**: Coordinator, Specialist Agents (News, Technical, Fundamental)\n",
    "- **Memory System**: FAISS vector database for persistent learning\n",
    "- **Data Sources**: Yahoo Finance, NewsAPI, FRED, Alpha Vantage\n",
    "- **Interface**: Gradio web interface for user interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f83c34",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies - TODO: Swapnil - Add Architecture diagram and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20daea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-community yfinance pandas numpy matplotlib seaborn plotly gradio faiss-cpu python-dotenv requests fredapi newsapi-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41f5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Source\\AIML\\aai520_8proj\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core Python Libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import ssl\n",
    "import urllib3\n",
    "import contextlib\n",
    "import traceback\n",
    "import io\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Analysis and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Environment and Configuration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.tools import BaseTool, tool\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# LangChain Azure OpenAI\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "# LangChain Community (Vector Store)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# External Data Sources\n",
    "import yfinance as yf\n",
    "from newsapi import NewsApiClient\n",
    "from fredapi import Fred\n",
    "\n",
    "# Web Interface\n",
    "import gradio as gr\n",
    "\n",
    "# Jupyter/IPython Display\n",
    "from IPython.display import display, Markdown, HTML, Image\n",
    "\n",
    "# HTTP Clients\n",
    "import requests\n",
    "import requests.sessions\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Matplotlib for Proper Display\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configure logging for clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Suppress verbose logging from external libraries\n",
    "external_loggers = [\n",
    "    \"httpx\",\n",
    "    \"azure.core.pipeline.policies.http_logging_policy\", \n",
    "    \"azure.identity\",\n",
    "    \"urllib3\"\n",
    "]\n",
    "\n",
    "for logger_name in external_loggers:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_llm_logs():\n",
    "    \"\"\"Context manager to suppress verbose LLM logging during execution\"\"\"\n",
    "    original_levels = {}\n",
    "    \n",
    "    # Store original log levels\n",
    "    for logger_name in external_loggers:\n",
    "        logger = logging.getLogger(logger_name)\n",
    "        original_levels[logger_name] = logger.level\n",
    "        logger.setLevel(logging.WARNING)\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore original levels\n",
    "        for logger_name, level in original_levels.items():\n",
    "            logging.getLogger(logger_name).setLevel(level)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a415bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI LLM initialized successfully using model: model-router\n",
      "Azure OpenAI Embeddings initialized successfully using model: text-embedding-3-small\n",
      "Environment setup completed successfully!\n",
      "Azure OpenAI Embeddings initialized successfully using model: text-embedding-3-small\n",
      "Environment setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration from environment variables\n",
    "AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_GPT_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_GPT_API_VERSION = os.getenv('AZURE_OPENAI_GPT_API_VERSION', '2024-02-15-preview')\n",
    "AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv('AZURE_OPENAI_EMBEDDING_API_VERSION', '2024-02-15-preview')\n",
    "\n",
    "ALPHA_VANTAGE_API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
    "NEWS_API_KEY = os.getenv('NEWSAPI_KEY')\n",
    "FRED_API_KEY = os.getenv('FRED_API_KEY')\n",
    "SEC_API_KEY = os.getenv('SEC_API_KEY')\n",
    "\n",
    "# Initialize Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_OPENAI_GPT_DEPLOYMENT_NAME,\n",
    "    openai_api_version=AZURE_OPENAI_GPT_API_VERSION,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY\n",
    ")\n",
    "print(f\"Azure OpenAI LLM initialized successfully using model: {AZURE_OPENAI_GPT_DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Initialize embeddings for vector database\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    openai_api_version=AZURE_OPENAI_EMBEDDING_API_VERSION,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY\n",
    ")\n",
    "print(f\"Azure OpenAI Embeddings initialized successfully using model: {AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME}\")\n",
    "\n",
    "print(\"Environment setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98b8ba",
   "metadata": {},
   "source": [
    "## 2. Vector Database and Memory System - TODO: Nelson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5134e0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new agent memory with caching enabled\n",
      "Agent memory system initialized!\n"
     ]
    }
   ],
   "source": [
    "class AgentMemory:\n",
    "    \"\"\"persistent memory system using FAISS vector database with intelligent caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_path: str = \"./database/agent_memory\"):\n",
    "        self.memory_path = memory_path\n",
    "        self.vector_store = None\n",
    "        self.data_cache = {}  # In-memory cache for quick lookups\n",
    "        self.cache_expiry = 3600  # 1 hour cache expiry for data\n",
    "        self.analysis_cache_expiry = 7200  # 2 hours for analysis results\n",
    "        self.initialize_memory()\n",
    "    \n",
    "    def initialize_memory(self):\n",
    "        \"\"\"Initialize or load existing memory.\"\"\"\n",
    "        try:\n",
    "            # Try to load existing memory\n",
    "            if os.path.exists(f\"{self.memory_path}.faiss\"):\n",
    "                self.vector_store = FAISS.load_local(self.memory_path, embeddings)\n",
    "                print(\"Loaded existing agent memory with cached data\")\n",
    "            else:\n",
    "                # Create new memory with initial documents\n",
    "                initial_docs = [\n",
    "                    Document(page_content=\"Investment analysis requires considering multiple factors: technical indicators, fundamental analysis, market sentiment, and economic conditions.\", \n",
    "                            metadata={\"type\": \"general_knowledge\", \"timestamp\": datetime.now().isoformat()})\n",
    "                ]\n",
    "                self.vector_store = FAISS.from_documents(initial_docs, embeddings)\n",
    "                self.save_memory()\n",
    "                print(\"Created new agent memory with caching enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing memory: {e}\")\n",
    "            # Fallback: create minimal memory\n",
    "            initial_docs = [\n",
    "                Document(page_content=\"Fallback memory initialized\", \n",
    "                        metadata={\"type\": \"system\", \"timestamp\": datetime.now().isoformat()})\n",
    "            ]\n",
    "            self.vector_store = FAISS.from_documents(initial_docs, embeddings)\n",
    "    \n",
    "    def _generate_cache_key(self, data_type: str, symbol: str, **params) -> str:\n",
    "        \"\"\"Generate a unique cache key for data requests.\"\"\"\n",
    "        # Create a consistent key from parameters\n",
    "        param_str = \"_\".join([f\"{k}_{v}\" for k, v in sorted(params.items())])\n",
    "        return f\"{data_type}_{symbol}_{param_str}\".lower()\n",
    "    \n",
    "    def _is_cache_valid(self, timestamp_str: str, cache_type: str = \"data\") -> bool:\n",
    "        \"\"\"Check if cached data is still valid.\"\"\"\n",
    "        try:\n",
    "            cache_time = datetime.fromisoformat(timestamp_str)\n",
    "            current_time = datetime.now()\n",
    "            expiry_time = self.cache_expiry if cache_type == \"data\" else self.analysis_cache_expiry\n",
    "            return (current_time - cache_time).total_seconds() < expiry_time\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_cached_data(self, data_type: str, symbol: str, **params) -> Optional[str]:\n",
    "        \"\"\"Retrieve cached data if available and valid.\"\"\"\n",
    "        try:\n",
    "            cache_key = self._generate_cache_key(data_type, symbol, **params)\n",
    "            \n",
    "            # First check in-memory cache\n",
    "            if cache_key in self.data_cache:\n",
    "                cache_entry = self.data_cache[cache_key]\n",
    "                if self._is_cache_valid(cache_entry['timestamp']):\n",
    "                    print(f\"Using cached {data_type} data for {symbol}\")\n",
    "                    return cache_entry['data']\n",
    "                else:\n",
    "                    # Remove expired cache\n",
    "                    del self.data_cache[cache_key]\n",
    "            \n",
    "            # Search vector database for cached results\n",
    "            search_query = f\"{data_type} data for {symbol} {' '.join(str(v) for v in params.values())}\"\n",
    "            memories = self.vector_store.similarity_search(search_query, k=3)\n",
    "            \n",
    "            for memory in memories:\n",
    "                metadata = memory.metadata\n",
    "                if (metadata.get('type') == 'cached_data' and \n",
    "                    metadata.get('data_type') == data_type and \n",
    "                    metadata.get('symbol', '').lower() == symbol.lower() and\n",
    "                    metadata.get('cache_key') == cache_key):\n",
    "                    \n",
    "                    if self._is_cache_valid(metadata.get('timestamp', ''), 'data'):\n",
    "                        # Cache in memory for faster future access\n",
    "                        self.data_cache[cache_key] = {\n",
    "                            'data': memory.page_content,\n",
    "                            'timestamp': metadata['timestamp']\n",
    "                        }\n",
    "                        print(f\"Retrieved cached {data_type} data for {symbol} from vector DB\")\n",
    "                        return memory.page_content\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving cached data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def cache_data(self, data_type: str, symbol: str, data: str, **params):\n",
    "        \"\"\"Cache data in both memory and vector database.\"\"\"\n",
    "        try:\n",
    "            cache_key = self._generate_cache_key(data_type, symbol, **params)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "            \n",
    "            # Store in memory cache\n",
    "            self.data_cache[cache_key] = {\n",
    "                'data': data,\n",
    "                'timestamp': timestamp\n",
    "            }\n",
    "            \n",
    "            # Store in vector database\n",
    "            metadata = {\n",
    "                'type': 'cached_data',\n",
    "                'data_type': data_type,\n",
    "                'symbol': symbol,\n",
    "                'cache_key': cache_key,\n",
    "                'timestamp': timestamp,\n",
    "                **{f\"param_{k}\": str(v) for k, v in params.items()}\n",
    "            }\n",
    "            \n",
    "            doc = Document(page_content=data, metadata=metadata)\n",
    "            self.vector_store.add_documents([doc])\n",
    "            self.save_memory()\n",
    "            \n",
    "            print(f\"Cached {data_type} data for {symbol}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error caching data: {e}\")\n",
    "    \n",
    "    def get_cached_analysis(self, analysis_type: str, symbol: str, request_hash: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve cached analysis results.\"\"\"\n",
    "        try:\n",
    "            search_query = f\"{analysis_type} analysis for {symbol}\"\n",
    "            memories = self.vector_store.similarity_search(search_query, k=5)\n",
    "            \n",
    "            for memory in memories:\n",
    "                metadata = memory.metadata\n",
    "                if (metadata.get('type') == 'cached_analysis' and \n",
    "                    metadata.get('analysis_type') == analysis_type and \n",
    "                    metadata.get('symbol', '').lower() == symbol.lower() and\n",
    "                    metadata.get('request_hash') == request_hash):\n",
    "                    \n",
    "                    if self._is_cache_valid(metadata.get('timestamp', ''), 'analysis'):\n",
    "                        try:\n",
    "                            cached_result = json.loads(memory.page_content)\n",
    "                            print(f\"Using cached {analysis_type} analysis for {symbol}\")\n",
    "                            return cached_result\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving cached analysis: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def cache_analysis(self, analysis_type: str, symbol: str, analysis_result: Dict[str, Any], request_hash: str):\n",
    "        \"\"\"Cache analysis results.\"\"\"\n",
    "        try:\n",
    "            metadata = {\n",
    "                'type': 'cached_analysis',\n",
    "                'analysis_type': analysis_type,\n",
    "                'symbol': symbol,\n",
    "                'request_hash': request_hash,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'execution_time': analysis_result.get('execution_time', 0)\n",
    "            }\n",
    "            \n",
    "            doc = Document(page_content=json.dumps(analysis_result), metadata=metadata)\n",
    "            self.vector_store.add_documents([doc])\n",
    "            self.save_memory()\n",
    "\n",
    "            print(f\"Cached {analysis_type} analysis for {symbol}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error caching analysis: {e}\")\n",
    "    \n",
    "    def add_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        \"\"\"Add new memory to the vector store.\"\"\"\n",
    "        try:\n",
    "            doc = Document(page_content=content, metadata=metadata)\n",
    "            self.vector_store.add_documents([doc])\n",
    "            self.save_memory()\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding memory: {e}\")\n",
    "    \n",
    "    def search_memory(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Search for relevant memories.\"\"\"\n",
    "        try:\n",
    "            return self.vector_store.similarity_search(query, k=k)\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching memory: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def cleanup_expired_cache(self):\n",
    "        \"\"\"Clean up expired cache entries from memory.\"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now()\n",
    "            expired_keys = []\n",
    "            \n",
    "            for key, entry in self.data_cache.items():\n",
    "                if not self._is_cache_valid(entry['timestamp']):\n",
    "                    expired_keys.append(key)\n",
    "            \n",
    "            for key in expired_keys:\n",
    "                del self.data_cache[key]\n",
    "            \n",
    "            if expired_keys:\n",
    "                print(f\"Cleaned up {len(expired_keys)} expired cache entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning cache: {e}\")\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        try:\n",
    "            memories = self.vector_store.similarity_search(\"cached\", k=100)\n",
    "            cached_data_count = sum(1 for m in memories if m.metadata.get('type') == 'cached_data')\n",
    "            cached_analysis_count = sum(1 for m in memories if m.metadata.get('type') == 'cached_analysis')\n",
    "            \n",
    "            return {\n",
    "                'in_memory_cache_size': len(self.data_cache),\n",
    "                'vector_db_cached_data': cached_data_count,\n",
    "                'vector_db_cached_analyses': cached_analysis_count,\n",
    "                'total_memories': len(memories)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting cache stats: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_memory(self):\n",
    "        \"\"\"Save memory to disk.\"\"\"\n",
    "        try:\n",
    "            self.vector_store.save_local(self.memory_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memory: {e}\")\n",
    "\n",
    "# Initialize global memory\n",
    "agent_memory = AgentMemory()\n",
    "print(\"Agent memory system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dcca6",
   "metadata": {},
   "source": [
    "## 3. Data Source Tools and Integrations - TODO: Nelson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1585e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source tools created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Yahoo Finance Tool with Caching\n",
    "@tool\n",
    "def get_stock_data(symbol: str, period: str = \"1y\") -> str:\n",
    "    \"\"\"Get stock price data and basic financial information from Yahoo Finance.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., 'AAPL', 'MSFT')\n",
    "        period: Time period ('1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first\n",
    "        cached_data = agent_memory.get_cached_data('stock_data', symbol, period=period)\n",
    "        if cached_data:\n",
    "            return cached_data\n",
    "        \n",
    "        # Fetch fresh data if not cached\n",
    "        stock = yf.Ticker(symbol)\n",
    "        hist = stock.history(period=period)\n",
    "        info = stock.info\n",
    "        \n",
    "        # Check if we have sufficient data\n",
    "        if hist.empty or len(hist) < 2:\n",
    "            return f\"Error: Insufficient price data for {symbol}\"\n",
    "        \n",
    "        current_price = hist['Close'].iloc[-1]\n",
    "        price_change = hist['Close'].iloc[-1] - hist['Close'].iloc[-2]\n",
    "        price_change_pct = (price_change / hist['Close'].iloc[-2]) * 100\n",
    "\n",
    "        print(f\"get_stock_data - Fetched fresh stock data for {symbol} from Yahoo Finance\")\n",
    "\n",
    "        # Convert pandas/numpy types to native Python types for JSON serialization\n",
    "        result = {\n",
    "            'symbol': symbol,\n",
    "            'current_price': float(round(current_price, 2)),\n",
    "            'price_change': float(round(price_change, 2)),\n",
    "            'price_change_pct': float(round(price_change_pct, 2)),\n",
    "            'volume': int(hist['Volume'].iloc[-1]) if pd.notna(hist['Volume'].iloc[-1]) else 0,\n",
    "            'market_cap': info.get('marketCap', 'N/A'),\n",
    "            'pe_ratio': info.get('trailingPE', 'N/A'),\n",
    "            'company_name': info.get('longName', 'N/A'),\n",
    "            'sector': info.get('sector', 'N/A'),\n",
    "            'industry': info.get('industry', 'N/A'),\n",
    "            # Add historical price data for technical analysis (last 50 days)\n",
    "            'price_history': [float(price) for price in hist['Close'].tail(50).tolist()],\n",
    "            'volume_history': [int(vol) if pd.notna(vol) else 0 for vol in hist['Volume'].tail(50).tolist()],\n",
    "            'high_52_week': float(hist['High'].max()),\n",
    "            'low_52_week': float(hist['Low'].min()),\n",
    "            'avg_volume': int(hist['Volume'].mean()) if not hist['Volume'].isna().all() else 0\n",
    "        }\n",
    "        \n",
    "        result_json = json.dumps(result, indent=2)\n",
    "\n",
    "        agent_memory.cache_data('stock_data', symbol, result_json, period=period)\n",
    "        \n",
    "        return result_json\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock data for {symbol}: {str(e)}\"\n",
    "\n",
    "# News API Tool with Caching\n",
    "@tool\n",
    "def get_stock_news(symbol: str, days: int = 7) -> str:\n",
    "    \"\"\"Get recent news articles related to a stock symbol.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol to search news for\n",
    "        days: Number of days to look back for news\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first (with shorter cache time for news)\n",
    "        cached_data = agent_memory.get_cached_data('stock_news', symbol, days=days)\n",
    "        if cached_data:\n",
    "            return cached_data\n",
    "        \n",
    "        # Configure SSL handling for NewsAPI\n",
    "        # Disable SSL warnings and verification for NewsAPI (development/testing only)\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        \n",
    "        # Create custom session with SSL configuration\n",
    "        session = requests.sessions.Session()\n",
    "        session.verify = False  # Disable SSL verification for NewsAPI\n",
    "        \n",
    "        # Alternative direct requests approach to avoid NewsApiClient SSL issues\n",
    "        from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        url = \"https://newsapi.org/v2/everything\"\n",
    "        params = {\n",
    "            'q': symbol,\n",
    "            'language': 'en',\n",
    "            'sortBy': 'relevancy',\n",
    "            'from': from_date,\n",
    "            'pageSize': 10,\n",
    "            'apiKey': NEWS_API_KEY\n",
    "        }\n",
    "        \n",
    "        response = session.get(url, params=params, verify=False, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') != 'ok':\n",
    "            return f\"Error: NewsAPI returned status: {data.get('status')} - {data.get('message', 'Unknown error')}\"\n",
    "        \n",
    "        news_items = []\n",
    "        articles = data.get('articles', [])\n",
    "        for article in articles[:5]:  # Top 5 articles\n",
    "            if article.get('title') and article.get('description'):\n",
    "                news_items.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['description'],\n",
    "                    'source': article.get('source', {}).get('name', 'Unknown'),\n",
    "                    'published_at': article.get('publishedAt', ''),\n",
    "                    'url': article.get('url', '')\n",
    "                })\n",
    "        \n",
    "        result_json = json.dumps(news_items, indent=2)\n",
    "        \n",
    "        print(f\"get_stock_news - Fetched fresh news for {symbol} from NewsAPI\")\n",
    "\n",
    "        # Cache the result (news has shorter cache time due to freshness requirements)\n",
    "        agent_memory.cache_data('stock_news', symbol, result_json, days=days)\n",
    "        \n",
    "        return result_json\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching news for {symbol}: {str(e)}\"\n",
    "\n",
    "# FRED Economic Data Tool with Caching\n",
    "@tool\n",
    "def get_economic_data(series_id: str = \"GDP\") -> str:\n",
    "    \"\"\"Get economic data from FRED (Federal Reserve Economic Data).\n",
    "    \n",
    "    Args:\n",
    "        series_id: FRED series ID (e.g., 'GDP', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first\n",
    "        cached_data = agent_memory.get_cached_data('economic_data', series_id)\n",
    "        if cached_data:\n",
    "            return cached_data\n",
    "        \n",
    "        fred = Fred(api_key=FRED_API_KEY)\n",
    "        data = fred.get_series(series_id, limit=12)  # Last 12 observations\n",
    "        \n",
    "        # Check if we have sufficient data\n",
    "        if data.empty or len(data) < 2:\n",
    "            return f\"Error: Insufficient economic data for series {series_id}\"\n",
    "        \n",
    "        # Convert pandas/numpy types to native Python types for JSON serialization\n",
    "        result = {\n",
    "            'series_id': series_id,\n",
    "            'latest_value': float(data.iloc[-1]),\n",
    "            'latest_date': data.index[-1].strftime('%Y-%m-%d'),\n",
    "            'previous_value': float(data.iloc[-2]),\n",
    "            'change': float(data.iloc[-1] - data.iloc[-2]),\n",
    "            'change_pct': float((data.iloc[-1] - data.iloc[-2]) / data.iloc[-2] * 100),\n",
    "            'historical_data': [float(val) for val in data.tolist()]\n",
    "        }\n",
    "        \n",
    "        result_json = json.dumps(result, indent=2)\n",
    "\n",
    "        print(f\"get_economic_data - Caching economic data for {series_id}\")\n",
    "\n",
    "        # Cache the result\n",
    "        agent_memory.cache_data('economic_data', series_id, result_json)\n",
    "        \n",
    "        return result_json\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching economic data for {series_id}: {str(e)}\"\n",
    "\n",
    "# Alpha Vantage Tool with Caching\n",
    "@tool\n",
    "def get_alpha_vantage_data(symbol: str, function: str = \"TIME_SERIES_DAILY\") -> str:\n",
    "    \"\"\"Get financial data from Alpha Vantage API.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol\n",
    "        function: Alpha Vantage function (e.g., 'TIME_SERIES_DAILY', 'OVERVIEW', 'INCOME_STATEMENT')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first\n",
    "        cached_data = agent_memory.get_cached_data('alpha_vantage', symbol, function=function)\n",
    "        if cached_data:\n",
    "            return cached_data\n",
    "        \n",
    "        base_url = \"https://www.alphavantage.co/query\"\n",
    "        params = {\n",
    "            'function': function,\n",
    "            'symbol': symbol,\n",
    "            'apikey': ALPHA_VANTAGE_API_KEY\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Return a simplified version to avoid token limits\n",
    "        if function == \"OVERVIEW\":\n",
    "            overview = {\n",
    "                'symbol': data.get('Symbol', 'N/A'),\n",
    "                'market_cap': data.get('MarketCapitalization', 'N/A'),\n",
    "                'pe_ratio': data.get('PERatio', 'N/A'),\n",
    "                'peg_ratio': data.get('PEGRatio', 'N/A'),\n",
    "                'dividend_yield': data.get('DividendYield', 'N/A'),\n",
    "                'eps': data.get('EPS', 'N/A'),\n",
    "                '52_week_high': data.get('52WeekHigh', 'N/A'),\n",
    "                '52_week_low': data.get('52WeekLow', 'N/A')\n",
    "            }\n",
    "            result_json = json.dumps(overview, indent=2)\n",
    "        else:\n",
    "            result_json = json.dumps(data, indent=2)[:1000]  # Truncate to avoid token limits\n",
    "        \n",
    "        print(f\"get_alpha_vantage_data - Fetched fresh Alpha Vantage data for {symbol} function {function}\")\n",
    "\n",
    "        # Cache the result\n",
    "        agent_memory.cache_data('alpha_vantage', symbol, result_json, function=function)\n",
    "        \n",
    "        return result_json\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching Alpha Vantage data for {symbol}: {str(e)}\"\n",
    "\n",
    "# SEC Filings Tool with Caching\n",
    "@tool\n",
    "def get_sec_filings(symbol: str, form_type: str = \"10-K\") -> str:\n",
    "    \"\"\"Get SEC filings data including business description and risk factors.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., 'AAPL', 'MSFT')\n",
    "        form_type: SEC form type (e.g., '10-K', '10-Q')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first\n",
    "        cached_data = agent_memory.get_cached_data('sec_filings', symbol, form_type=form_type)\n",
    "        if cached_data:\n",
    "            return cached_data\n",
    "        # Dynamic CIK lookup using SEC's company tickers API\n",
    "        def lookup_cik_online(ticker_symbol):\n",
    "            \"\"\"\n",
    "            Look up CIK (Central Index Key) for a given ticker symbol using SEC's API\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # SEC company tickers API endpoint\n",
    "                tickers_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "                headers = {\"User-Agent\": \"Investment-Research-Agent research@example.com\"}\n",
    "                \n",
    "                # Add SEC API key to headers if available (for enhanced rate limits or premium services)\n",
    "                if SEC_API_KEY:\n",
    "                    headers[\"Authorization\"] = f\"Bearer {SEC_API_KEY}\"\n",
    "                    headers[\"X-API-Key\"] = SEC_API_KEY\n",
    "                \n",
    "                response = requests.get(tickers_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                tickers_data = response.json()\n",
    "                \n",
    "                # Search for the ticker symbol\n",
    "                ticker_upper = ticker_symbol.upper()\n",
    "                for entry in tickers_data.values():\n",
    "                    if entry.get('ticker', '').upper() == ticker_upper:\n",
    "                        # Return CIK with proper zero-padding\n",
    "                        cik_str = str(entry['cik_str']).zfill(10)\n",
    "                        return cik_str\n",
    "                \n",
    "                return None\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Online CIK lookup failed for {ticker_symbol}: {e}\")\n",
    "                # Fallback to hardcoded lookup for major companies\n",
    "                fallback_lookup = {\n",
    "                    \"AAPL\": \"0000320193\", \"MSFT\": \"0000789019\", \"TSLA\": \"0001318605\",\n",
    "                    \"AMZN\": \"0001018724\", \"GOOG\": \"0001652044\", \"GOOGL\": \"0001652044\",\n",
    "                    \"META\": \"0001326801\", \"NVDA\": \"0001045810\", \"NFLX\": \"0001065280\",\n",
    "                    \"ADBE\": \"0000796343\", \"JPM\": \"0000019617\", \"JNJ\": \"0000200406\",\n",
    "                    \"PG\": \"0000080424\", \"UNH\": \"0000731766\", \"HD\": \"0000354950\"\n",
    "                }\n",
    "                return fallback_lookup.get(ticker_symbol.upper())\n",
    "        \n",
    "        # Look up CIK dynamically\n",
    "        print(f\"Looking up CIK for {symbol}...\")\n",
    "        cik = lookup_cik_online(symbol)\n",
    "        if not cik:\n",
    "            return f\"CIK not found for symbol: {symbol}. Please verify the ticker symbol is valid and traded on US exchanges.\"\n",
    "        \n",
    "        # Get filings from SEC EDGAR API\n",
    "        headers = {\"User-Agent\": \"Investment-Research-Agent research@example.com\"}\n",
    "        \n",
    "        # Add SEC API key to headers if available (for enhanced rate limits or premium services)\n",
    "        if SEC_API_KEY:\n",
    "            headers[\"Authorization\"] = f\"Bearer {SEC_API_KEY}\"\n",
    "            headers[\"X-API-Key\"] = SEC_API_KEY\n",
    "        \n",
    "        url = f\"https://data.sec.gov/submissions/CIK{cik.zfill(10)}.json\"\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Find most recent filing of requested type\n",
    "        filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        filing_info = None\n",
    "        \n",
    "        for i in range(len(filings.get(\"accessionNumber\", []))):\n",
    "            if form_type.upper() in filings[\"form\"][i].upper():\n",
    "                filing_info = {\n",
    "                    \"form_type\": filings[\"form\"][i],\n",
    "                    \"filed_date\": filings[\"filingDate\"][i],\n",
    "                    \"report_date\": filings[\"reportDate\"][i],\n",
    "                    \"accession_number\": filings[\"accessionNumber\"][i],\n",
    "                    \"primary_document\": filings[\"primaryDocument\"][i]\n",
    "                }\n",
    "                break\n",
    "        \n",
    "        if not filing_info:\n",
    "            return f\"No {form_type} filing found for {symbol}\"\n",
    "        \n",
    "        # Construct filing URL\n",
    "        accession_clean = filing_info[\"accession_number\"].replace(\"-\", \"\")\n",
    "        filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_clean}/{filing_info['primary_document']}\"\n",
    "        \n",
    "        # For now, return filing metadata and URL\n",
    "        # In a full implementation, you could use SEC-API or similar service to extract specific sections\n",
    "        result = {\n",
    "            \"symbol\": symbol,\n",
    "            \"company_name\": data.get(\"name\", \"N/A\"),\n",
    "            \"cik\": cik,\n",
    "            \"latest_filing\": {\n",
    "                \"form_type\": filing_info[\"form_type\"],\n",
    "                \"filed_date\": filing_info[\"filed_date\"],\n",
    "                \"report_date\": filing_info[\"report_date\"], \n",
    "                \"filing_url\": filing_url,\n",
    "                \"business_summary\": f\"Latest {filing_info['form_type']} filing for {symbol}. Contains comprehensive business description, risk factors, and financial statements.\",\n",
    "                \"key_sections\": [\n",
    "                    \"Item 1: Business Description\",\n",
    "                    \"Item 1A: Risk Factors\", \n",
    "                    \"Item 2: Properties\",\n",
    "                    \"Item 3: Legal Proceedings\",\n",
    "                    \"Financial Statements and Supplementary Data\"\n",
    "                ],\n",
    "                \"investment_relevance\": f\"This {filing_info['form_type']} filing provides detailed insights into {symbol}'s business model, competitive position, regulatory environment, and material risks that could impact investment returns.\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        result_json = json.dumps(result, indent=2)\n",
    "        \n",
    "        print(f\"get_sec_filings - Fetched fresh SEC filings for {symbol} from SEC EDGAR\")\n",
    "\n",
    "        # Cache the result\n",
    "        agent_memory.cache_data('sec_filings', symbol, result_json, form_type=form_type)\n",
    "        \n",
    "        return result_json\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching SEC filings for {symbol}: {str(e)}\"\n",
    "\n",
    "# Investment Recommendation Tool with Caching\n",
    "@tool\n",
    "def get_investment_recommendation(symbol: str, analysis_context: str = \"\") -> str:\n",
    "    \"\"\"Generate investment recommendation (Strong Buy, Buy, Hold, Sell, Strong Sell) based on comprehensive analysis.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., 'AAPL', 'MSFT')\n",
    "        analysis_context: Optional context from previous analyses to inform recommendation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first (recommendations have shorter cache time due to market volatility)\n",
    "        cached_data = agent_memory.get_cached_data('investment_recommendation', symbol, context_hash=hash(analysis_context))\n",
    "        if cached_data:\n",
    "            return cached_data\n",
    "        \n",
    "        # Gather comprehensive data for recommendation\n",
    "        print(f\"Gathering data for investment recommendation on {symbol}...\")\n",
    "        \n",
    "        # Get current stock data\n",
    "        stock_data = get_stock_data.invoke({\"symbol\": symbol, \"period\": \"6mo\"})\n",
    "        stock_info = json.loads(stock_data) if stock_data.startswith('{') else {}\n",
    "        \n",
    "        # Get fundamental data from Alpha Vantage\n",
    "        alpha_data = get_alpha_vantage_data.invoke({\"symbol\": symbol, \"function\": \"OVERVIEW\"})\n",
    "        alpha_info = json.loads(alpha_data) if alpha_data.startswith('{') else {}\n",
    "        \n",
    "        # Get recent economic context\n",
    "        economic_data = get_economic_data.invoke({\"series_id\": \"FEDFUNDS\"})  # Federal Funds Rate\n",
    "        econ_info = json.loads(economic_data) if economic_data.startswith('{') else {}\n",
    "        \n",
    "        # Calculate recommendation metrics\n",
    "        recommendation_metrics = {\n",
    "            'symbol': symbol,\n",
    "            'current_price': stock_info.get('current_price', 0),\n",
    "            'price_change_pct': stock_info.get('price_change_pct', 0),\n",
    "            'pe_ratio': alpha_info.get('pe_ratio', 'N/A'),\n",
    "            'peg_ratio': alpha_info.get('peg_ratio', 'N/A'),\n",
    "            'dividend_yield': alpha_info.get('dividend_yield', 'N/A'),\n",
    "            'market_cap': alpha_info.get('market_cap', 'N/A'),\n",
    "            '52_week_high': alpha_info.get('52_week_high', stock_info.get('high_52_week', 0)),\n",
    "            '52_week_low': alpha_info.get('52_week_low', stock_info.get('low_52_week', 0)),\n",
    "            'volume': stock_info.get('volume', 0),\n",
    "            'avg_volume': stock_info.get('avg_volume', 0),\n",
    "            'sector': stock_info.get('sector', 'N/A'),\n",
    "            'federal_funds_rate': econ_info.get('latest_value', 'N/A')\n",
    "        }\n",
    "        \n",
    "        # Calculate recommendation score (0-100 scale)\n",
    "        score = 50  # Start neutral\n",
    "        confidence_factors = []\n",
    "        \n",
    "        # Price momentum analysis\n",
    "        price_change = recommendation_metrics['price_change_pct']\n",
    "        if price_change > 5:\n",
    "            score += 10\n",
    "            confidence_factors.append(\"Strong positive price momentum\")\n",
    "        elif price_change > 2:\n",
    "            score += 5\n",
    "            confidence_factors.append(\"Positive price momentum\")\n",
    "        elif price_change < -5:\n",
    "            score -= 10\n",
    "            confidence_factors.append(\"Negative price momentum\")\n",
    "        elif price_change < -2:\n",
    "            score -= 5\n",
    "            confidence_factors.append(\"Weak price momentum\")\n",
    "        \n",
    "        # Valuation analysis\n",
    "        try:\n",
    "            pe_ratio = float(alpha_info.get('pe_ratio', 0)) if alpha_info.get('pe_ratio', 'N/A') != 'N/A' else None\n",
    "            if pe_ratio:\n",
    "                if pe_ratio < 15:\n",
    "                    score += 8\n",
    "                    confidence_factors.append(\"Attractive P/E valuation\")\n",
    "                elif pe_ratio < 25:\n",
    "                    score += 3\n",
    "                    confidence_factors.append(\"Reasonable P/E valuation\")\n",
    "                elif pe_ratio > 40:\n",
    "                    score -= 8\n",
    "                    confidence_factors.append(\"High P/E valuation concern\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Volume analysis\n",
    "        current_volume = recommendation_metrics['volume']\n",
    "        avg_volume = recommendation_metrics['avg_volume']\n",
    "        if avg_volume > 0:\n",
    "            volume_ratio = current_volume / avg_volume\n",
    "            if volume_ratio > 1.5:\n",
    "                score += 5\n",
    "                confidence_factors.append(\"High trading volume indicates interest\")\n",
    "            elif volume_ratio < 0.5:\n",
    "                score -= 3\n",
    "                confidence_factors.append(\"Low volume may indicate lack of interest\")\n",
    "        \n",
    "        # 52-week range analysis\n",
    "        current_price = recommendation_metrics['current_price']\n",
    "        week_52_high = float(recommendation_metrics['52_week_high']) if recommendation_metrics['52_week_high'] else current_price\n",
    "        week_52_low = float(recommendation_metrics['52_week_low']) if recommendation_metrics['52_week_low'] else current_price\n",
    "        \n",
    "        if week_52_high > week_52_low:\n",
    "            price_position = (current_price - week_52_low) / (week_52_high - week_52_low)\n",
    "            if price_position < 0.3:\n",
    "                score += 7\n",
    "                confidence_factors.append(\"Trading near 52-week low - potential upside\")\n",
    "            elif price_position > 0.8:\n",
    "                score -= 5\n",
    "                confidence_factors.append(\"Trading near 52-week high - limited upside\")\n",
    "        \n",
    "        # Economic environment factor\n",
    "        try:\n",
    "            fed_rate = float(econ_info.get('latest_value', 0))\n",
    "            if fed_rate > 5:\n",
    "                score -= 5\n",
    "                confidence_factors.append(\"High interest rates headwind\")\n",
    "            elif fed_rate < 2:\n",
    "                score += 3\n",
    "                confidence_factors.append(\"Low interest rates supportive\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Determine recommendation based on score\n",
    "        if score >= 75:\n",
    "            recommendation = \"Strong Buy\"\n",
    "            confidence = \"High\"\n",
    "        elif score >= 60:\n",
    "            recommendation = \"Buy\" \n",
    "            confidence = \"High\" if score >= 65 else \"Medium\"\n",
    "        elif score >= 40:\n",
    "            recommendation = \"Hold\"\n",
    "            confidence = \"Medium\"\n",
    "        elif score >= 25:\n",
    "            recommendation = \"Sell\"\n",
    "            confidence = \"Medium\" if score >= 30 else \"High\"\n",
    "        else:\n",
    "            recommendation = \"Strong Sell\"\n",
    "            confidence = \"High\"\n",
    "        \n",
    "        # Calculate target price estimate\n",
    "        target_price = current_price\n",
    "        if recommendation in [\"Strong Buy\", \"Buy\"]:\n",
    "            target_price = current_price * (1 + (score - 50) / 200)\n",
    "        elif recommendation in [\"Sell\", \"Strong Sell\"]:\n",
    "            target_price = current_price * (1 - (50 - score) / 200)\n",
    "        \n",
    "        # Prepare recommendation result\n",
    "        result = {\n",
    "            'symbol': symbol,\n",
    "            'recommendation': recommendation,\n",
    "            'confidence': confidence,\n",
    "            'recommendation_score': round(score, 1),\n",
    "            'current_price': current_price,\n",
    "            'target_price': round(target_price, 2),\n",
    "            'price_target_change_pct': round(((target_price - current_price) / current_price) * 100, 2),\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'key_factors': confidence_factors[:5],  # Top 5 factors\n",
    "            'risk_level': 'High' if abs(score - 50) > 25 else 'Medium' if abs(score - 50) > 15 else 'Low',\n",
    "            'investment_horizon': '3-6 months',\n",
    "            'metrics_analyzed': recommendation_metrics,\n",
    "            'analysis_context': analysis_context[:500] if analysis_context else \"Standalone recommendation analysis\"\n",
    "        }\n",
    "        \n",
    "        result_json = json.dumps(result, indent=2)\n",
    "        \n",
    "        print(f\"get_investment_recommendation - Generated new recommendation for {symbol}\")\n",
    "        \n",
    "        # Cache the result (shorter cache for recommendations due to volatility)\n",
    "        agent_memory.cache_data('investment_recommendation', symbol, result_json, context_hash=hash(analysis_context))\n",
    "        \n",
    "        return result_json\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating investment recommendation for {symbol}: {str(e)}\"\n",
    "\n",
    "print(\"Data source tools created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a469f",
   "metadata": {},
   "source": [
    "\n",
    "This data acquisition and analysis script defines a suite of tool functions that form the data backbone to our Investment Agent ecosystem. Each tool implemented as a LangChain @tool—connects to real world financial data sources such as Yahoo Finance, NewsAPI, FRED, Alpha Vantage, and SEC EDGAR, incorporating intelligent caching through a shared agent_memory system. This caching layer optimizes performance, prevents redundant API calls, and ensures data consistency across our agent network.\n",
    "The Yahoo Finance Tool (get_stock_data) retrieves stock price and key financial metrics using the yfinance API, performing validation, feature extraction such as current price, P/E ratio, 52-week range, and serialization for downstream analysis. The NewsAPI Tool (get_stock_news) extracts top market headlines over a configurable period, returning clean, structured article summaries for the NewsProcessingChain to classify and analyze sentiment. The FRED Tool (get_economic_data) integrates macroeconomic indicators such as GDP or interest rates to provide broader market context. The Alpha Vantage Tool (get_alpha_vantage_data) supplies financial ratios like PEG, EPS, and market capitalization, directly supporting the Fundamental Analyst agent. Meanwhile, the SEC Filings Tool (get_sec_filings) dynamically fetches regulatory filings like 10-K and 10-Q from the EDGAR database using CIK lookups, extracting relevant business, compliance, and risk data. Finally, the Investment Recommendation Tool (get_investment_recommendation) synthesizes all sources—market data, fundamentals, and macroeconomic context—into a scored Buy/Hold/Sell recommendation with a confidence rating, price target, and risk level.\n",
    "\n",
    "Across all functions, our investment agent follows a common architectural pattern built on four pillars: (1) a caching layer that reuses results for efficiency, (2) data normalization converting external API responses to structured JSON for model input, (3) robust error handling to mitigate API or formatting failures, and (4) transparent logging for traceability. Together, these tools constitute the data ingestion and fusion layer of the Investment Research Agent architecture, enabling real-time, low latency intelligence that seamlessly integrates market data, news sentiment, macroeconomic trends, and regulatory insights into a unified analytical pipeline for autonomous financial reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43182421",
   "metadata": {},
   "source": [
    "## 4. Base Agent Class with Core Functions - TODO: Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5287f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt configuration class created!\n"
     ]
    }
   ],
   "source": [
    "class PromptConfiguration:\n",
    "    \"\"\"Central configuration class for all prompts used in the investment research system\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_planning_prompt(role: str, task: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As an {role}, create a detailed research plan for: {task}\n",
    "        \n",
    "        Consider these aspects:\n",
    "        1. Data gathering (price data, news, economic indicators, fundamentals)\n",
    "        2. Analysis techniques (technical, fundamental, sentiment)\n",
    "        3. Risk assessment\n",
    "        4. Market context evaluation\n",
    "        \n",
    "        Return a numbered list of specific, actionable research steps.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_reflection_prompt(analysis_result: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Please evaluate this investment analysis for quality and completeness:\n",
    "        \n",
    "        Analysis: {analysis_result}\n",
    "        \n",
    "        Provide a structured evaluation covering:\n",
    "        1. Completeness (1-10): Are all key aspects covered?\n",
    "        2. Data Quality (1-10): Is the data comprehensive and current?\n",
    "        3. Logic (1-10): Is the reasoning sound and well-structured?\n",
    "        4. Actionability (1-10): Are the conclusions practical and specific?\n",
    "        5. Risk Assessment (1-10): Are risks properly identified and evaluated?\n",
    "        \n",
    "        Also provide:\n",
    "        - Overall Score (1-10)\n",
    "        - Key Strengths (2-3 points)\n",
    "        - Areas for Improvement (2-3 points)\n",
    "        - Specific Recommendations for enhancement\n",
    "        \n",
    "        Format as JSON with these exact keys: completeness, data_quality, logic, actionability, risk_assessment, overall_score, strengths, improvements, recommendations\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_news_classification_prompt(title: str, description: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Classify this news article:\n",
    "        Title: {title}\n",
    "        Description: {description}\n",
    "        \n",
    "        Provide classification in JSON format:\n",
    "        {{\n",
    "            \"category\": \"earnings|product|market|regulation|management|merger|other\",\n",
    "            \"sentiment\": \"positive|negative|neutral\",\n",
    "            \"importance\": \"high|medium|low\",\n",
    "            \"reasoning\": \"brief explanation\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_insights_extraction_prompt(classified_articles: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Extract key insights from these classified news articles:\n",
    "        \n",
    "        {classified_articles}\n",
    "        \n",
    "        Provide insights in JSON format:\n",
    "        {{\n",
    "            \"key_themes\": [\"list of main themes\"],\n",
    "            \"sentiment_distribution\": {{\"positive\": 0, \"negative\": 0, \"neutral\": 0}},\n",
    "            \"high_importance_items\": [\"list of high importance findings\"],\n",
    "            \"potential_catalysts\": [\"events that could drive stock price\"],\n",
    "            \"risk_factors\": [\"identified risks or concerns\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_news_summarization_prompt(insights: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Create a comprehensive news analysis summary for {symbol} based on these insights:\n",
    "        \n",
    "        {insights}\n",
    "        \n",
    "        Provide a professional investment-focused summary covering:\n",
    "        1. Executive Summary (2-3 sentences)\n",
    "        2. Key Developments and Themes\n",
    "        3. Sentiment Analysis\n",
    "        4. Potential Stock Price Catalysts\n",
    "        5. Risk Factors and Concerns\n",
    "        6. Investment Implications\n",
    "        \n",
    "        Keep the analysis concise but comprehensive, suitable for investment decision-making.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod \n",
    "    def get_technical_analysis_prompt(symbol: str, stock_data: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Conduct technical analysis for {symbol} based on this stock data:\n",
    "        \n",
    "        {stock_data}\n",
    "        \n",
    "        Provide analysis covering:\n",
    "        1. Price trends and patterns\n",
    "        2. Volume analysis  \n",
    "        3. Support and resistance levels\n",
    "        4. Technical indicators (if calculable from data)\n",
    "        5. Short-term price outlook\n",
    "        6. Key technical levels to watch\n",
    "        \n",
    "        Format as a professional technical analysis suitable for investment decisions.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_fundamental_analysis_prompt(symbol: str, stock_data: str, alpha_overview: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Conduct fundamental analysis for {symbol} based on this data:\n",
    "        \n",
    "        Stock Data: {stock_data}\n",
    "        Company Overview: {alpha_overview}\n",
    "        \n",
    "        Provide analysis covering:\n",
    "        1. Financial health assessment\n",
    "        2. Valuation metrics analysis\n",
    "        3. Business model evaluation\n",
    "        4. Competitive position\n",
    "        5. Growth prospects\n",
    "        6. Risk factors\n",
    "        7. Fair value estimate\n",
    "        \n",
    "        Format as a professional fundamental analysis suitable for investment decisions.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_sentiment_analysis_prompt(symbol: str, news_analysis: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Enhance this news analysis for {symbol} with sentiment insights:\n",
    "        \n",
    "        News Analysis: {news_analysis}\n",
    "        \n",
    "        Provide enhanced analysis focusing on:\n",
    "        1. Overall sentiment assessment\n",
    "        2. Market perception trends\n",
    "        3. Potential impact on stock price\n",
    "        4. Investor sentiment drivers\n",
    "        5. Sentiment-based risks and opportunities\n",
    "        \n",
    "        Format as professional sentiment analysis for investment decisions.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_sec_filings_analysis_prompt(symbol: str, sec_data: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Analyze SEC filings for {symbol} based on this data:\n",
    "        \n",
    "        SEC Data: {sec_data}\n",
    "        \n",
    "        Provide analysis covering:\n",
    "        1. Key regulatory disclosures\n",
    "        2. Risk factor analysis\n",
    "        3. Management discussion insights\n",
    "        4. Financial statement highlights\n",
    "        5. Compliance and governance assessment\n",
    "        6. Material changes or events\n",
    "        \n",
    "        Format as professional regulatory analysis for investment decisions.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_investment_recommendation_prompt(symbol: str, recommendation_data: str, analysis_context: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Generate investment recommendation for {symbol} based on:\n",
    "        \n",
    "        Recommendation Data: {recommendation_data}\n",
    "        Analysis Context: {analysis_context}\n",
    "        \n",
    "        Provide comprehensive recommendation covering:\n",
    "        1. Investment thesis\n",
    "        2. Recommendation (Buy/Hold/Sell) with rationale\n",
    "        3. Price target and timeline\n",
    "        4. Risk assessment\n",
    "        5. Portfolio allocation suggestions\n",
    "        6. Key catalysts and monitoring points\n",
    "        \n",
    "        Format as professional investment recommendation suitable for decision-making.\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_routing_prompt(request: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Route this investment research request for {symbol}: {request}\n",
    "        \n",
    "        Available specialists:\n",
    "        - technical: Technical analysis (price trends, indicators, charts)\n",
    "        - fundamental: Fundamental analysis (financials, valuation, business)\n",
    "        - news: News and sentiment analysis\n",
    "        - sec: SEC filings and regulatory analysis\n",
    "        - recommendation: Investment recommendation synthesis\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"specialists_needed\": [\"list of specialist types needed\"],\n",
    "            \"priority_order\": [\"order of execution\"],\n",
    "            \"reasoning\": \"explanation of routing decision\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_evaluation_prompt(analysis: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        As an Investment Analysis Quality Evaluator, assess this investment analysis for {symbol}:\n",
    "        \n",
    "        Analysis:\n",
    "        {analysis}\n",
    "        \n",
    "        Evaluate on these criteria (1-10 scale):\n",
    "        1. Completeness: Are all key investment aspects covered?\n",
    "        2. Data Integration: How well are different data sources synthesized?\n",
    "        3. Risk Assessment: Is risk analysis comprehensive and realistic?\n",
    "        4. Actionability: Are recommendations specific and implementable?\n",
    "        5. Logic and Reasoning: Is the analysis logical and well-structured?\n",
    "        6. Market Context: Is broader market context considered?\n",
    "        7. Clarity: Is the analysis clear and professional?\n",
    "        \n",
    "        Provide feedback in JSON format:\n",
    "        {{\n",
    "            \"scores\": {{\n",
    "                \"completeness\": X,\n",
    "                \"data_integration\": X,\n",
    "                \"risk_assessment\": X,\n",
    "                \"actionability\": X,\n",
    "                \"logic_reasoning\": X,\n",
    "                \"market_context\": X,\n",
    "                \"clarity\": X\n",
    "            }},\n",
    "            \"overall_score\": X,\n",
    "            \"grade\": \"A|B|C|D|F\",\n",
    "            \"strengths\": [\"list of key strengths\"],\n",
    "            \"weaknesses\": [\"list of areas needing improvement\"],\n",
    "            \"specific_improvements\": [\"detailed suggestions for enhancement\"],\n",
    "            \"missing_elements\": [\"what's missing from the analysis\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_refinement_prompt(original_analysis: str, evaluation: str, symbol: str) -> str:\n",
    "        return f\"\"\"\n",
    "        Improve this investment analysis for {symbol} based on the evaluation feedback:\n",
    "        \n",
    "        Original Analysis:\n",
    "        {original_analysis}\n",
    "        \n",
    "        Evaluation Feedback:\n",
    "        {evaluation}\n",
    "        \n",
    "        Create an improved version that addresses these issues:\n",
    "        1. Fix identified weaknesses\n",
    "        2. Add missing elements\n",
    "        3. Implement specific improvements\n",
    "        4. Enhance overall quality and completeness\n",
    "        5. Maintain professional investment analysis standards\n",
    "        \n",
    "        Focus particularly on areas that scored below 7/10 in the evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "print(\"Prompt configuration class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff70180",
   "metadata": {},
   "source": [
    "The PromptConfiguration class serves as the centralized prompt management system to our Investment Agent framework. It defines and standardizes every prompt template used across the agent ecosystem thus, ensuring consistent structure, tone, and output format when interacting with the Large Language Model (LLM). By consolidating prompt definitions in one class, it enforces modularity, maintainability, and reproducibility of the agent’s behavior, enabling each analytical component such as planning, evaluation, reflection, technical analysis and more, to operate under a unified communication schema. All methods are static, meaning they can be called globally without object instantiation, allowing other system modules like EvaluatorOptimizer, NewsProcessingChain, RoutingCoordinator, and SpecialistAgent subclasses to access standardized LLM instructions seamlessly.\n",
    "\n",
    " ach static method in the class corresponds to a specific functional task in the agent workflow, returning a structured prompt string formatted for LLM input. For example, get_planning_prompt() generates step-by-step research instructions for an agent role; get_reflection_prompt() guides self-evaluation in JSON format with predefined scoring keys and get_evaluation_prompt() drives the evaluator-optimizer feedback cycle by quantifying analysis quality on a 1–10 scale. Analytical modules rely on domain-specific prompts like get_technical_analysis_prompt() (for price and volume patterns), get_fundamental_analysis_prompt() (for valuation and financial health), get_sentiment_analysis_prompt() (for market tone extraction), and get_sec_filings_analysis_prompt() (for compliance and risk factors).\n",
    "\n",
    "Additionally, prompts such as get_news_classification_prompt(), get_insights_extraction_prompt(), and get_news_summarization_prompt(), we  specifically designed them for prompt chaining, transforming raw news into structured sentiment insights. The get_investment_recommendation_prompt() combines multi-agent context and outputs into actionable buy/hold/sell decisions, while get_routing_prompt() guides the RoutingCoordinator in selecting and prioritizing the right specialists for a given task. Lastly, the get_refinement_prompt() enables iterative self improvement, instructing the LLM to rewrite an analysis using prior evaluation feedback to enhance weak areas.\n",
    "\n",
    "In essence, PromptConfiguration abstracts prompt engineering into a reusable, domain aware configuration layer. It acts as the semantic interface between structured data and natural language reasoning ensuring that all agents in the system communicate with the LLM using precise, domain optimized, and JSON compatible instructions. This design enhances cross agent interoperability, improves LLM output reliability, and establishes the foundation for consistent, auditable AI driven investment research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca9fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Investment Research Agent class created!\n"
     ]
    }
   ],
   "source": [
    "class InvestmentResearchAgent:\n",
    "    \"\"\"Base Investment Research Agent with planning, tool usage, self-reflection, and learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, role: str, memory: AgentMemory):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.memory = memory\n",
    "        self.llm = llm\n",
    "        self.session_memory = ConversationBufferWindowMemory(\n",
    "            k=10, \n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        )\n",
    "        self.tools = [get_stock_data, get_stock_news, get_economic_data, get_alpha_vantage_data, get_sec_filings, get_investment_recommendation]\n",
    "        self.execution_log = []\n",
    "    \n",
    "    def invoke_llm_with_logging(self, prompt: str, context: str = \"\") -> tuple:\n",
    "        \"\"\"Invoke LLM and log the model information along with response\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            # Extract model information\n",
    "            # Extract model information\n",
    "            model_name = getattr(response, 'response_metadata', {}).get('model_name', 'Unknown')\n",
    "            if not model_name or model_name == 'Unknown':\n",
    "                # Try to get model from LLM configuration\n",
    "                model_name = getattr(self.llm, 'deployment_name', getattr(self.llm, 'model_name', 'Azure OpenAI Model'))\n",
    "            \n",
    "            # Get token count information\n",
    "            token_count = 'N/A'\n",
    "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "                token_count = response.usage_metadata.get('total_tokens', 'N/A')\n",
    "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
    "                usage = response.response_metadata.get('token_usage', {})\n",
    "                if usage:\n",
    "                    token_count = usage.get('total_tokens', 'N/A')\n",
    "            \n",
    "            # Print model information with token count\n",
    "            print(f\"InvestmentResearchAgent - LLM Model Used: {model_name} | Tokens: {token_count}\")\n",
    "\n",
    "            return response.content, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error in {context}: {e}\")\n",
    "            return f\"Error in LLM call: {str(e)}\", \"Error\"\n",
    "        \n",
    "    def plan_research(self, task: str) -> List[str]:\n",
    "        \"\"\"Plan research steps for a given task.\"\"\"\n",
    "        planning_prompt = PromptConfiguration.get_planning_prompt(self.role, task)\n",
    "        \n",
    "        try:\n",
    "            plan_text, model_name = self.invoke_llm_with_logging(planning_prompt, f\"{self.name} - Research Planning\")\n",
    "            \n",
    "            # Extract numbered steps\n",
    "            steps = []\n",
    "            for line in plan_text.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and (line[0].isdigit() or line.startswith('-') or line.startswith('*')):\n",
    "                    steps.append(line)\n",
    "            \n",
    "            # Log the plan\n",
    "            self.execution_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'action': 'plan_created',\n",
    "                'task': task,\n",
    "                'plan': steps,\n",
    "                'model_used': model_name\n",
    "            })\n",
    "            \n",
    "            return steps\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in planning: {e}\")\n",
    "            return [\"1. Gather basic stock data\", \"2. Analyze recent news\", \"3. Review economic context\", \"4. Synthesize findings\"]\n",
    "    \n",
    "    def use_tool_dynamically(self, tool_name: str, **kwargs) -> str:\n",
    "        \"\"\"Use tools dynamically based on context.\"\"\"\n",
    "        tool_map = {\n",
    "            'stock_data': get_stock_data,\n",
    "            'news': get_stock_news,\n",
    "            'economic': get_economic_data,\n",
    "            'alpha_vantage': get_alpha_vantage_data,\n",
    "            'sec_filings': get_sec_filings\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if tool_name in tool_map:\n",
    "                result = tool_map[tool_name].invoke(kwargs)\n",
    "                \n",
    "                # Log tool usage\n",
    "                self.execution_log.append({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'action': 'tool_used',\n",
    "                    'tool': tool_name,\n",
    "                    'parameters': kwargs,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "                return result\n",
    "            else:\n",
    "                return f\"Tool '{tool_name}' not available\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.execution_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'action': 'tool_used',\n",
    "                'tool': tool_name,\n",
    "                'parameters': kwargs,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            return f\"Error using tool '{tool_name}': {str(e)}\"\n",
    "    \n",
    "    def self_reflect(self, analysis_result: str) -> Dict[str, Any]:\n",
    "        \"\"\"Self-reflect on the quality of analysis output.\"\"\"\n",
    "        reflection_prompt = PromptConfiguration.get_reflection_prompt(analysis_result)\n",
    "        \n",
    "        try:\n",
    "            reflection_text, model_name = self.invoke_llm_with_logging(reflection_prompt, f\"{self.name} - Self Reflection\")\n",
    "            \n",
    "            # Try to parse as JSON, fallback to structured text parsing\n",
    "            try:\n",
    "                reflection_data = json.loads(reflection_text)\n",
    "            except:\n",
    "                # Fallback parsing\n",
    "                reflection_data = {\n",
    "                    'overall_score': 7,\n",
    "                    'strengths': [\"Analysis provided\"],\n",
    "                    'improvements': [\"Could be more detailed\"],\n",
    "                    'recommendations': [\"Gather more data points\"]\n",
    "                }\n",
    "            \n",
    "            # Log reflection\n",
    "            self.execution_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'action': 'self_reflection',\n",
    "                'reflection': reflection_data,\n",
    "                'model_used': model_name\n",
    "            })\n",
    "            \n",
    "            return reflection_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in self-reflection: {e}\")\n",
    "            return {\n",
    "                'overall_score': 5,\n",
    "                'strengths': [\"Attempt made\"],\n",
    "                'improvements': [\"Technical issues encountered\"],\n",
    "                'recommendations': [\"Retry analysis\"]\n",
    "            }\n",
    "    \n",
    "    def learn_from_experience(self, task: str, result: str, reflection: Dict[str, Any]):\n",
    "        \"\"\"Learn from the current analysis and store insights for future use.\"\"\"\n",
    "        try:\n",
    "            # Create learning content\n",
    "            learning_content = f\"\"\"\n",
    "            Task: {task}\n",
    "            Analysis Quality Score: {reflection.get('overall_score', 'N/A')}\n",
    "            Key Insights: {', '.join(reflection.get('strengths', []))}\n",
    "            Improvement Areas: {', '.join(reflection.get('improvements', []))}\n",
    "            Recommendations: {', '.join(reflection.get('recommendations', []))}\n",
    "            Execution Log: {len(self.execution_log)} actions taken\n",
    "            \"\"\"\n",
    "            \n",
    "            # Store in memory\n",
    "            metadata = {\n",
    "                'type': 'learning_experience',\n",
    "                'agent': self.name,\n",
    "                'task': task,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'quality_score': reflection.get('overall_score', 0)\n",
    "            }\n",
    "            \n",
    "            self.memory.add_memory(learning_content, metadata)\n",
    "            \n",
    "            print(f\"{self.name} learned from experience (Score: {reflection.get('overall_score', 'N/A')})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in learning: {e}\")\n",
    "    \n",
    "    def retrieve_relevant_experience(self, task: str) -> List[str]:\n",
    "        \"\"\"Retrieve relevant past experiences for the current task.\"\"\"\n",
    "        try:\n",
    "            memories = self.memory.search_memory(task, k=3)\n",
    "            experiences = []\n",
    "            \n",
    "            for memory in memories:\n",
    "                if memory.metadata.get('type') == 'learning_experience':\n",
    "                    experiences.append(memory.page_content)\n",
    "            \n",
    "            return experiences\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving experience: {e}\")\n",
    "            return []\n",
    "\n",
    "print(\"Base Investment Research Agent class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe727d08",
   "metadata": {},
   "source": [
    "The InvestmentResearchAgent class script serves as the foundational architecture to our autonomous investment agents. It provides the core infrastructure for planning, dynamic tool usage, self reflection, and experiential learning, allowing each specialized sub agent to perform intelligent, context aware financial analysis. The agent integrates directly with a Large Language Model (LLM) for reasoning and text generation, and with an AgentMemory system for long term learning and recall. It also maintains a conversation buffer (ConversationBufferWindowMemory) to preserve short term context, ensuring that analyses are coherent across multiple interactions.\n",
    "\n",
    " The plan_research() method uses an LLM prompt to generate structured multi step research plans for a given task, extracting numbered steps and logging them for traceability. The use_tool_dynamically() method acts as a dynamic API router, mapping semantic tool names such as stock_data, news, economic, alpha_vantage, sec_filings to specific tool functions and invoking them with contextual parameters. This enables the agent to autonomously query live data such as market prices, news, filings, and economic indicators based on the current research objective.\n",
    "\n",
    "The self-assessment layer is implemented through self_reflect(), which prompts the LLM to critique its own analysis output. It returns a structured reflection containing an overall quality score, key strengths, improvement areas, and actionable recommendations. These reflections are stored alongside metadata in the execution log for continuous tracking. The learning subsystem, implemented in learn_from_experience(), encodes these reflections into persistent vector memory with tags such as task name, timestamp, and quality score. Later, retrieve_relevant_experience() allows the agent to recall similar past analyses, enabling experience based reasoning and adaptive improvement over time.\n",
    "\n",
    "Finally, the invoke_llm_with_logging() method wraps all LLM interactions with detailed telemetry including model name, token usage, and execution context ensuring transparent monitoring and reproducibility. Together, these mechanisms make InvestmentResearchAgent a self improving AI research entity capable of planning its workflow, executing tools intelligently, evaluating its performance, and learning from historical results forming the cognitive backbone for higher level specialist agents like TechnicalAnalyst, FundamentalAnalyst, and NewsAnalyst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584927e",
   "metadata": {},
   "source": [
    "## 5. Workflow Pattern 1: Prompt Chaining (News Processing Pipeline) - TODO: Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709ceefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Processing Chain (Prompt Chaining) created!\n"
     ]
    }
   ],
   "source": [
    "class NewsProcessingChain:\n",
    "    \"\"\"Implements Prompt Chaining: Ingest → Preprocess → Classify → Extract → Summarize\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def invoke_llm_with_logging(self, prompt: str, context: str = \"\") -> tuple:\n",
    "        \"\"\"Invoke LLM and log the model information along with response\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            # Extract model information\n",
    "            model_name = getattr(response, 'response_metadata', {}).get('model_name', 'Unknown')\n",
    "            if not model_name or model_name == 'Unknown':\n",
    "                # Try to get model from LLM configuration\n",
    "                model_name = getattr(self.llm, 'deployment_name', getattr(self.llm, 'model_name', 'Azure OpenAI Model'))\n",
    "            \n",
    "            # Get token count information\n",
    "            token_count = 'N/A'\n",
    "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "                token_count = response.usage_metadata.get('total_tokens', 'N/A')\n",
    "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
    "                usage = response.response_metadata.get('token_usage', {})\n",
    "                if usage:\n",
    "                    token_count = usage.get('total_tokens', 'N/A')\n",
    "            \n",
    "            # Print model information\n",
    "            print(f\"NewsProcessingChain - LLM Model Used: {model_name} | Tokens: {token_count} | Context: {context}\")\n",
    "\n",
    "            return response.content, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error in {context}: {e}\")\n",
    "            return f\"Error in LLM call: {str(e)}\", \"Error\"\n",
    "    \n",
    "    def ingest_news(self, symbol: str) -> List[Dict]:\n",
    "        \"\"\"Step 1: Ingest news data\"\"\"\n",
    "        try:\n",
    "            news_data = get_stock_news.invoke({\"symbol\": symbol, \"days\": 7})\n",
    "            \n",
    "            # Handle empty or None response\n",
    "            if not news_data or news_data.strip() == \"\":\n",
    "                print(f\"No news data returned for {symbol}\")\n",
    "                return []\n",
    "            \n",
    "            # Check if response is an error message (string starting with \"Error\")\n",
    "            if isinstance(news_data, str) and news_data.startswith(\"Error\"):\n",
    "                print(f\"News API error: {news_data}\")\n",
    "                return []\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                parsed_data = json.loads(news_data)\n",
    "                \n",
    "                # Handle different response formats\n",
    "                if isinstance(parsed_data, list):\n",
    "                    return parsed_data\n",
    "                elif isinstance(parsed_data, dict):\n",
    "                    # If it's a dict with news items, extract them\n",
    "                    if 'news' in parsed_data:\n",
    "                        return parsed_data['news']\n",
    "                    elif 'articles' in parsed_data:\n",
    "                        return parsed_data['articles']\n",
    "                    else:\n",
    "                        # Return as single item list\n",
    "                        return [parsed_data]\n",
    "                else:\n",
    "                    print(f\"Unexpected news data format: {type(parsed_data)}\")\n",
    "                    return []\n",
    "                    \n",
    "            except json.JSONDecodeError as je:\n",
    "                print(f\"JSON parsing error: {je}\")\n",
    "                print(f\"Raw response: {news_data[:200]}...\")  # Show first 200 chars\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error ingesting news: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def preprocess_news(self, news_articles: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Step 2: Preprocess and clean news articles\"\"\"\n",
    "        processed_articles = []\n",
    "        \n",
    "        for article in news_articles:\n",
    "            # Clean and structure the article\n",
    "            processed_article = {\n",
    "                'title': article.get('title', '').strip(),\n",
    "                'description': article.get('description', '').strip(),\n",
    "                'source': article.get('source', 'Unknown'),\n",
    "                'published_at': article.get('published_at', ''),\n",
    "                'url': article.get('url', ''),\n",
    "                'combined_text': f\"{article.get('title', '')} {article.get('description', '')}\".strip()\n",
    "            }\n",
    "            \n",
    "            # Only include articles with meaningful content\n",
    "            if processed_article['combined_text'] and len(processed_article['combined_text']) > 20:\n",
    "                processed_articles.append(processed_article)\n",
    "        \n",
    "        return processed_articles\n",
    "    \n",
    "    def classify_news(self, processed_articles: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Step 3: Classify news articles by type and sentiment\"\"\"\n",
    "        classified_articles = []\n",
    "        \n",
    "        for article in processed_articles:\n",
    "            classify_prompt = PromptConfiguration.get_news_classification_prompt(\n",
    "                article['title'], \n",
    "                article['description']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                classification_text, model_name = self.invoke_llm_with_logging(\n",
    "                    classify_prompt, \n",
    "                    \"News Classification\"\n",
    "                )\n",
    "                \n",
    "                # Try to parse JSON, fallback to default values\n",
    "                try:\n",
    "                    classification = json.loads(classification_text)\n",
    "                except:\n",
    "                    classification = {\n",
    "                        \"category\": \"other\",\n",
    "                        \"sentiment\": \"neutral\",\n",
    "                        \"importance\": \"medium\",\n",
    "                        \"reasoning\": \"Classification parsing failed\"\n",
    "                    }\n",
    "                \n",
    "                article.update(classification)\n",
    "                article['model_used'] = model_name\n",
    "                classified_articles.append(article)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error classifying article: {e}\")\n",
    "                article.update({\n",
    "                    \"category\": \"other\",\n",
    "                    \"sentiment\": \"neutral\",\n",
    "                    \"importance\": \"medium\",\n",
    "                    \"reasoning\": \"Error in classification\",\n",
    "                    \"model_used\": \"Error\"\n",
    "                })\n",
    "                classified_articles.append(article)\n",
    "        \n",
    "        return classified_articles\n",
    "    \n",
    "    def extract_insights(self, classified_articles: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Step 4: Extract key insights from classified articles\"\"\"\n",
    "        classified_articles_str = json.dumps(classified_articles, indent=2)[:2000]  # Truncate to avoid token limits\n",
    "        insights_prompt = PromptConfiguration.get_insights_extraction_prompt(classified_articles_str)\n",
    "        \n",
    "        try:\n",
    "            insights_text, model_name = self.invoke_llm_with_logging(\n",
    "                insights_prompt, \n",
    "                \"Insights Extraction\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                insights = json.loads(insights_text)\n",
    "                insights['model_used'] = model_name\n",
    "            except:\n",
    "                # Fallback insights\n",
    "                sentiment_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
    "                for article in classified_articles:\n",
    "                    sentiment = article.get('sentiment', 'neutral')\n",
    "                    sentiment_counts[sentiment] += 1\n",
    "                \n",
    "                insights = {\n",
    "                    \"key_themes\": [\"General market activity\"],\n",
    "                    \"sentiment_distribution\": sentiment_counts,\n",
    "                    \"high_importance_items\": [item['title'] for item in classified_articles if item.get('importance') == 'high'],\n",
    "                    \"potential_catalysts\": [\"Market developments\"],\n",
    "                    \"risk_factors\": [\"Market volatility\"],\n",
    "                    \"model_used\": model_name\n",
    "                }\n",
    "            \n",
    "            return insights\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting insights: {e}\")\n",
    "            return {\n",
    "                \"key_themes\": [\"Analysis error\"],\n",
    "                \"sentiment_distribution\": {\"positive\": 0, \"negative\": 0, \"neutral\": len(classified_articles)},\n",
    "                \"high_importance_items\": [],\n",
    "                \"potential_catalysts\": [],\n",
    "                \"risk_factors\": [\"Analysis uncertainty\"],\n",
    "                \"model_used\": \"Error\"\n",
    "            }\n",
    "    \n",
    "    def summarize_analysis(self, insights: Dict[str, Any], symbol: str) -> str:\n",
    "        \"\"\"Step 5: Summarize the complete news analysis\"\"\"\n",
    "        insights_str = json.dumps(insights, indent=2)\n",
    "        summarize_prompt = PromptConfiguration.get_news_summarization_prompt(insights_str, symbol)\n",
    "        \n",
    "        try:\n",
    "            summary, model_name = self.invoke_llm_with_logging(\n",
    "                summarize_prompt, \n",
    "                f\"News Summary for {symbol}\"\n",
    "            )\n",
    "            return summary\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization: {e}\")\n",
    "            return f\"News analysis for {symbol} completed with {len(insights.get('key_themes', []))} key themes identified. Sentiment distribution shows mixed signals. Further analysis recommended.\"\n",
    "    \n",
    "    def process_news_chain(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the complete news processing chain\"\"\"\n",
    "        print(f\"Starting news processing chain for {symbol}...\")\n",
    "        \n",
    "        # Step 1: Ingest\n",
    "        print(\"Step 1: Ingesting news...\")\n",
    "        raw_news = self.ingest_news(symbol)\n",
    "        \n",
    "        # Step 2: Preprocess\n",
    "        print(\"Step 2: Preprocessing news...\")\n",
    "        processed_news = self.preprocess_news(raw_news)\n",
    "        \n",
    "        # Step 3: Classify\n",
    "        print(\"Step 3: Classifying news...\")\n",
    "        classified_news = self.classify_news(processed_news)\n",
    "        \n",
    "        # Step 4: Extract\n",
    "        print(\"Step 4: Extracting insights...\")\n",
    "        insights = self.extract_insights(classified_news)\n",
    "        \n",
    "        # Step 5: Summarize\n",
    "        print(\"Step 5: Creating summary...\")\n",
    "        summary = self.summarize_analysis(insights, symbol)\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'raw_articles_count': len(raw_news),\n",
    "            'processed_articles_count': len(processed_news),\n",
    "            'classified_articles': classified_news,\n",
    "            'insights': insights,\n",
    "            'summary': summary,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize news processing chain\n",
    "news_chain = NewsProcessingChain(llm)\n",
    "print(\"News Processing Chain (Prompt Chaining) created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcc573",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The NewsProcessingChain class implements an end-to-end prompt chaining pipeline that automates real time financial news intelligence using a stepwise process: Ingest → Preprocess → Classify → Extract → Summarize. all implemented into our Investment Agent’s analytical ecosystem. This transforms unstructured market news into structured, sentiment aware insights suitable for investment decision-making. The system leverages a large language model (LLM) at multiple stages for reasoning, classification, and summarization while maintaining traceability through detailed logging of model metadata (name, tokens used, and context). This design enables modular, transparent, and explainable news driven analysis within financial AI workflows.\n",
    "\n",
    "This script begins with ingest_news(), which fetches recent financial headlines via the get_stock_news tool, handling API errors, empty responses, and multiple data formats (JSON lists, dicts, or nested structures). Next, preprocess_news() cleans, standardizes, and concatenates article titles and descriptions into unified text blocks, filtering out low-information entries. In classify_news(), each processed article is passed through an LLM prompt that assigns categorical labels such as category, sentiment, importance, and reasoning, with JSON parsing fallbacks to handle malformed outputs. Once classified, extract_insights() aggregates all articles to derive macro level analytics key themes, sentiment distribution, high importance events, catalysts, and risk factors using another LLM prompt or default statistical fallbacks when parsing fails. The summarize_analysis() stage then converts these extracted insights into a concise, human readable market summary tailored to the target stock symbol.\n",
    "\n",
    "Finally, process_news_chain() orchestrates all five steps sequentially while logging execution progress and returning a structured output dictionary containing article counts, classified data, extracted insights, and the final LLM-generated summary. By chaining multiple reasoning prompts together and combining symbolic logic with LLM inference, this component effectively bridges raw text data and actionable market intelligence. It enables the agent to detect emerging catalysts, measure sentiment bias, and contextualize market activity with transparency and reproducibility—making it a critical subsystem in the Investment Research Agent’s multi-agent analytical architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff796d",
   "metadata": {},
   "source": [
    "## 6. Workflow Pattern 2: Routing (Specialist Agents) - TODO: Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d469dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing System (Specialist Agents) created!\n"
     ]
    }
   ],
   "source": [
    "class SpecialistAgent(InvestmentResearchAgent):\n",
    "    \"\"\"Base class for specialist agents\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, role: str, specialization: str, memory: AgentMemory):\n",
    "        super().__init__(name, role, memory)\n",
    "        self.specialization = specialization\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform specialized analysis - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TechnicalAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for technical analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"TechnicalAnalyst\", \"Technical Analysis Specialist\", \"technical_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform technical analysis with caching\"\"\"\n",
    "        try:\n",
    "            # Generate request hash for caching\n",
    "            import hashlib\n",
    "            request_hash = hashlib.md5(f\"technical_{symbol}_6mo\".encode()).hexdigest()\n",
    "            \n",
    "            # Check for cached analysis first\n",
    "            cached_analysis = self.memory.get_cached_analysis('technical', symbol, request_hash)\n",
    "            if cached_analysis:\n",
    "                return cached_analysis\n",
    "            \n",
    "            # Get stock data for technical analysis\n",
    "            stock_data = self.use_tool_dynamically('stock_data', symbol=symbol, period='6mo')\n",
    "            \n",
    "            technical_prompt = PromptConfiguration.get_technical_analysis_prompt(symbol, stock_data)\n",
    "            \n",
    "            analysis, model_name = self.invoke_llm_with_logging(\n",
    "                technical_prompt, \n",
    "                f\"Technical Analysis for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'technical',\n",
    "                'symbol': symbol,\n",
    "                'analysis': analysis,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['stock_price_data', 'volume_data'],\n",
    "                'model_used': model_name\n",
    "            }\n",
    "            \n",
    "            # Cache the analysis result\n",
    "            self.memory.cache_analysis('technical', symbol, result, request_hash)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'technical',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"Technical analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True,\n",
    "                'model_used': 'Error'\n",
    "            }\n",
    "\n",
    "class FundamentalAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for fundamental analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"FundamentalAnalyst\", \"Fundamental Analysis Specialist\", \"fundamental_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform fundamental analysis with caching\"\"\"\n",
    "        try:\n",
    "            # Generate request hash for caching\n",
    "            import hashlib\n",
    "            request_hash = hashlib.md5(f\"fundamental_{symbol}_overview\".encode()).hexdigest()\n",
    "            \n",
    "            # Check for cached analysis first\n",
    "            cached_analysis = self.memory.get_cached_analysis('fundamental', symbol, request_hash)\n",
    "            if cached_analysis:\n",
    "                return cached_analysis\n",
    "            \n",
    "            # Get fundamental data\n",
    "            stock_data = self.use_tool_dynamically('stock_data', symbol=symbol)\n",
    "            alpha_overview = self.use_tool_dynamically('alpha_vantage', symbol=symbol, function='OVERVIEW')\n",
    "            \n",
    "            fundamental_prompt = PromptConfiguration.get_fundamental_analysis_prompt(symbol, stock_data, alpha_overview)\n",
    "            \n",
    "            analysis, model_name = self.invoke_llm_with_logging(\n",
    "                fundamental_prompt, \n",
    "                f\"Fundamental Analysis for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'fundamental',\n",
    "                'symbol': symbol,\n",
    "                'analysis': analysis,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['company_financials', 'market_data', 'ratios'],\n",
    "                'model_used': model_name\n",
    "            }\n",
    "            \n",
    "            # Cache the analysis result\n",
    "            self.memory.cache_analysis('fundamental', symbol, result, request_hash)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'fundamental',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"Fundamental analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True,\n",
    "                'model_used': 'Error'\n",
    "            }\n",
    "\n",
    "class NewsAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for news and sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"NewsAnalyst\", \"News and Sentiment Analysis Specialist\", \"news_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform news and sentiment analysis\"\"\"\n",
    "        try:\n",
    "            # Use the news processing chain\n",
    "            news_analysis = news_chain.process_news_chain(symbol)\n",
    "            \n",
    "            news_analysis_str = json.dumps(news_analysis, indent=2)[:1500]\n",
    "            sentiment_prompt = PromptConfiguration.get_sentiment_analysis_prompt(symbol, news_analysis_str)\n",
    "            \n",
    "            enhanced_analysis, model_name = self.invoke_llm_with_logging(\n",
    "                sentiment_prompt, \n",
    "                f\"Sentiment Analysis for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'news_sentiment',\n",
    "                'symbol': symbol,\n",
    "                'analysis': enhanced_analysis,\n",
    "                'raw_news_analysis': news_analysis,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['news_articles', 'sentiment_data'],\n",
    "                'model_used': model_name\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'news_sentiment',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"News analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True,\n",
    "                'model_used': 'Error'\n",
    "            }\n",
    "\n",
    "class SECFilingsAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for SEC filings and regulatory analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"SECFilingsAnalyst\", \"SEC Filings and Regulatory Analysis Specialist\", \"sec_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform SEC filings analysis\"\"\"\n",
    "        try:\n",
    "            # Get SEC filings data\n",
    "            sec_data = self.use_tool_dynamically('get_sec_filings', symbol=symbol)\n",
    "            \n",
    "            sec_analysis_str = json.dumps(sec_data, indent=2)[:2000]\n",
    "            sec_prompt = PromptConfiguration.get_sec_filings_analysis_prompt(symbol, sec_analysis_str)\n",
    "            \n",
    "            analysis, model_name = self.invoke_llm_with_logging(\n",
    "                sec_prompt, \n",
    "                f\"SEC Filings Analysis for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'sec_filings',\n",
    "                'symbol': symbol,\n",
    "                'analysis': analysis,\n",
    "                'raw_sec_data': sec_data,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['sec_filings', 'regulatory_data', 'risk_factors'],\n",
    "                'model_used': model_name\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'sec_filings',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"SEC filings analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True,\n",
    "                'model_used': 'Error'\n",
    "            }\n",
    "\n",
    "class InvestmentRecommendationAnalyst(SpecialistAgent):\n",
    "    \"\"\"Specialist agent for investment recommendations\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"InvestmentRecommendationAnalyst\", \"Investment Recommendation Specialist\", \"recommendation_analysis\", memory)\n",
    "    \n",
    "    def analyze(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform investment recommendation analysis with caching\"\"\"\n",
    "        try:\n",
    "            # Generate request hash for caching\n",
    "            import hashlib\n",
    "            request_hash = hashlib.md5(f\"recommendation_{symbol}_comprehensive\".encode()).hexdigest()\n",
    "            \n",
    "            # Check for cached analysis first\n",
    "            cached_analysis = self.memory.get_cached_analysis('recommendation', symbol, request_hash)\n",
    "            if cached_analysis:\n",
    "                return cached_analysis\n",
    "            \n",
    "            # Get comprehensive data for recommendation\n",
    "            analysis_context = \"\"\n",
    "            if data.get('specialist_analyses'):\n",
    "                # Use other specialist analyses as context\n",
    "                context_parts = []\n",
    "                for spec_type, spec_analysis in data['specialist_analyses'].items():\n",
    "                    if spec_analysis.get('analysis'):\n",
    "                        context_parts.append(f\"{spec_type}: {spec_analysis['analysis'][:200]}...\")\n",
    "                analysis_context = \" | \".join(context_parts)\n",
    "            \n",
    "            # Get investment recommendation using the tool\n",
    "            recommendation_data = self.use_tool_dynamically('get_investment_recommendation', symbol=symbol, analysis_context=analysis_context)\n",
    "            \n",
    "            # Parse the recommendation data\n",
    "            try:\n",
    "                recommendation_info = json.loads(recommendation_data)\n",
    "            except:\n",
    "                recommendation_info = {\"error\": \"Failed to parse recommendation data\"}\n",
    "            \n",
    "            # Generate detailed analysis using the recommendation prompt\n",
    "            recommendation_prompt = PromptConfiguration.get_investment_recommendation_prompt(symbol, recommendation_data, analysis_context)\n",
    "            \n",
    "            detailed_analysis, model_name = self.invoke_llm_with_logging(\n",
    "                recommendation_prompt, \n",
    "                f\"Investment Recommendation for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'investment_recommendation',\n",
    "                'symbol': symbol,\n",
    "                'analysis': detailed_analysis,\n",
    "                'recommendation_data': recommendation_info,\n",
    "                'recommendation': recommendation_info.get('recommendation', 'Hold'),\n",
    "                'confidence': recommendation_info.get('confidence', 'Medium'),\n",
    "                'target_price': recommendation_info.get('target_price', 0),\n",
    "                'current_price': recommendation_info.get('current_price', 0),\n",
    "                'price_target_change_pct': recommendation_info.get('price_target_change_pct', 0),\n",
    "                'risk_level': recommendation_info.get('risk_level', 'Medium'),\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_used': ['comprehensive_analysis', 'price_data', 'valuation_metrics', 'economic_indicators'],\n",
    "                'model_used': model_name\n",
    "            }\n",
    "            \n",
    "            # Cache the analysis result\n",
    "            self.memory.cache_analysis('recommendation', symbol, result, request_hash)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'specialist': self.name,\n",
    "                'analysis_type': 'investment_recommendation',\n",
    "                'symbol': symbol,\n",
    "                'analysis': f\"Investment recommendation analysis error: {str(e)}\",\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': True,\n",
    "                'model_used': 'Error'\n",
    "            }\n",
    "\n",
    "class RoutingCoordinator:\n",
    "    \"\"\"Coordinates routing of analysis tasks to appropriate specialists\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        self.memory = memory\n",
    "        self.specialists = {\n",
    "            'technical': TechnicalAnalyst(memory),\n",
    "            'fundamental': FundamentalAnalyst(memory),\n",
    "            'news': NewsAnalyst(memory),\n",
    "            'sec': SECFilingsAnalyst(memory),\n",
    "            'recommendation': InvestmentRecommendationAnalyst(memory)\n",
    "        }\n",
    "        self.llm = llm\n",
    "    \n",
    "    def invoke_llm_with_logging(self, prompt: str, context: str = \"\") -> tuple:\n",
    "        \"\"\"Invoke LLM and log the model information along with response\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            # Extract model information\n",
    "            # Extract model information\n",
    "            model_name = getattr(response, 'response_metadata', {}).get('model_name', 'Unknown')\n",
    "            if not model_name or model_name == 'Unknown':\n",
    "                # Try to get model from LLM configuration\n",
    "                model_name = getattr(self.llm, 'deployment_name', getattr(self.llm, 'model_name', 'Azure OpenAI Model'))\n",
    "            \n",
    "            # Get token count information\n",
    "            token_count = 'N/A'\n",
    "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "                token_count = response.usage_metadata.get('total_tokens', 'N/A')\n",
    "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
    "                usage = response.response_metadata.get('token_usage', {})\n",
    "                if usage:\n",
    "                    token_count = usage.get('total_tokens', 'N/A')\n",
    "            \n",
    "            # Print model information\n",
    "            print(f\"RoutingCoordinator - LLM Model Used: {model_name} | Tokens: {token_count} | Context: {context}\")\n",
    "            \n",
    "            return response.content, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error in {context}: {e}\")\n",
    "            return f\"Error in LLM call: {str(e)}\", \"Error\"\n",
    "    \n",
    "    def route_analysis(self, request: str, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route analysis request to appropriate specialists\"\"\"\n",
    "        routing_prompt = PromptConfiguration.get_routing_prompt(request, symbol)\n",
    "        \n",
    "        try:\n",
    "            routing_text, model_name = self.invoke_llm_with_logging(\n",
    "                routing_prompt, \n",
    "                f\"Routing Decision for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                routing_decision = json.loads(routing_text)\n",
    "            except:\n",
    "                # Default routing - use all specialists\n",
    "                routing_decision = {\n",
    "                    \"specialists_needed\": [\"technical\", \"fundamental\", \"news\", \"sec\", \"recommendation\"],\n",
    "                    \"priority_order\": [\"fundamental\", \"technical\", \"news\", \"sec\", \"recommendation\"],\n",
    "                    \"reasoning\": \"Default comprehensive analysis including regulatory filings and investment recommendation\"\n",
    "                }\n",
    "            \n",
    "            routing_decision['routing_model_used'] = model_name\n",
    "            \n",
    "            # Execute analysis with selected specialists\n",
    "            results = {}\n",
    "            for specialist_type in routing_decision.get('specialists_needed', []):\n",
    "                if specialist_type in self.specialists:\n",
    "                    print(f\"Routing to {specialist_type} specialist...\")\n",
    "                    \n",
    "                    # For recommendation specialist, pass other analyses as context\n",
    "                    analysis_data = {'specialist_analyses': results} if specialist_type == 'recommendation' else {}\n",
    "                    \n",
    "                    specialist_result = self.specialists[specialist_type].analyze(analysis_data, symbol)\n",
    "                    results[specialist_type] = specialist_result\n",
    "            \n",
    "            return {\n",
    "                'routing_decision': routing_decision,\n",
    "                'specialist_analyses': results,\n",
    "                'symbol': symbol,\n",
    "                'request': request,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in routing: {e}\")\n",
    "            return {\n",
    "                'error': f\"Routing error: {str(e)}\",\n",
    "                'symbol': symbol,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize routing system\n",
    "routing_coordinator = RoutingCoordinator(agent_memory)\n",
    "print(\"Routing System (Specialist Agents) created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf183c",
   "metadata": {},
   "source": [
    "The SpecialistAgent architecture defines our investment agent framework and decomposes complex investment research into domain specific analytical units. Each subclass represents an autonomous specialist that focuses on a distinct aspect of financial intelligence like, technical trends, fundamentals, news sentiment, regulatory filings, and investment recommendations. The base class SpecialistAgent inherits from InvestmentResearchAgent and establishes the shared structure and interface (analyze()), while individual specialist subclasses implement specialized logic. All agents integrate with the AgentMemory system, which provides caching, persistence, and recall of past analyses, ensuring both computational efficiency and historical context.\n",
    "\n",
    "The TechnicalAnalyst agent performs chart and pattern driven assessments by retrieving stock price and volume data for the past six months, generating a structured LLM prompt, and caching results to avoid redundant API calls. The FundamentalAnalyst agent retrieves financial statements and key ratios from tools such as Alpha Vantage, performing intrinsic value analysis and company health assessment. The NewsAnalyst integrates a news_chain pipeline that collects, filters, and summarizes financial news, then applies LLM based sentiment scoring to determine market tone. The SECFilingsAnalyst agent focuses on regulatory insights by analyzing SEC filings (10-K, 10-Q, 8-K), extracting risk factors and compliance data for the symbol. Finally, the InvestmentRecommendationAnalyst synthesizes all other agents’ outputs, generates a comprehensive investment decision (Buy/Hold/Sell), and calculates target price, confidence, and risk level—making it the system’s decision-making layer.\n",
    "\n",
    "At the coordination level, the RoutingCoordinator acts as the control hub, dynamically assigning tasks to appropriate specialists. It leverages an LLM to parse routing prompts and determine which analyses are relevant for a given request, using JSON-based routing decisions that specify the necessary agents and their priority order. The coordinator orchestrates concurrent or sequential execution of specialists, passes inter agent context (e.g., previous analyses to the recommendation agent), and logs each LLM call with model metadata and token usage for traceability. This architecture ensures extensibility new specialists can be added without modifying core logic and creates a self-organizing analytical ecosystem where agents collaborate through prompt chaining, memory based caching, and tool driven reasoning, producing a unified and explainable investment intelligence report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2e8ef",
   "metadata": {},
   "source": [
    "## 7. Workflow Pattern 3: Evaluator-Optimizer (Analysis Refinement Loop) - TODO: Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f860643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator-Optimizer system created!\n"
     ]
    }
   ],
   "source": [
    "class EvaluatorOptimizer:\n",
    "    \"\"\"Implements Evaluator-Optimizer pattern: Generate → Evaluate → Refine\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, memory: AgentMemory):\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        self.max_iterations = 3\n",
    "    \n",
    "    def invoke_llm_with_logging(self, prompt: str, context: str = \"\") -> tuple:\n",
    "        \"\"\"Invoke LLM and log the model information along with response\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            # Extract model information\n",
    "            model_name = getattr(response, 'response_metadata', {}).get('model_name', 'Unknown')\n",
    "            if not model_name or model_name == 'Unknown':\n",
    "                # Try to get model from LLM configuration\n",
    "                model_name = getattr(self.llm, 'deployment_name', getattr(self.llm, 'model_name', 'Azure OpenAI Model'))\n",
    "            \n",
    "            # Get token count information\n",
    "            token_count = 'N/A'\n",
    "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "                token_count = response.usage_metadata.get('total_tokens', 'N/A')\n",
    "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
    "                usage = response.response_metadata.get('token_usage', {})\n",
    "                if usage:\n",
    "                    token_count = usage.get('total_tokens', 'N/A')\n",
    "            \n",
    "            # Print model information\n",
    "            print(f\"EvaluatorOptimizer - LLM Model Used: {model_name} | Tokens: {token_count} | Context: {context}\")\n",
    "\n",
    "            return response.content, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error in {context}: {e}\")\n",
    "            return f\"Error in LLM call: {str(e)}\", \"Error\"\n",
    "    \n",
    "    def generate_initial_analysis(self, symbol: str, specialist_analyses: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate initial comprehensive analysis\"\"\"\n",
    "        specialist_analyses_str = json.dumps(specialist_analyses, indent=2)[:3000]\n",
    "        \n",
    "        # Create a comprehensive analysis prompt\n",
    "        generate_prompt = f\"\"\"\n",
    "        Create a comprehensive investment analysis for {symbol} based on these specialist analyses:\n",
    "        \n",
    "        {specialist_analyses_str}\n",
    "        \n",
    "        Provide a structured analysis covering:\n",
    "        1. Executive Summary\n",
    "        2. Technical Assessment\n",
    "        3. Fundamental Analysis\n",
    "        4. News and Sentiment\n",
    "        5. Regulatory and SEC Filing Insights\n",
    "        6. Investment Recommendation\n",
    "        7. Risk Assessment\n",
    "        8. Price Targets and Timeline\n",
    "        \n",
    "        Make it professional, actionable, and comprehensive for investment decision-making.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            analysis, model_name = self.invoke_llm_with_logging(\n",
    "                generate_prompt, \n",
    "                f\"Initial Analysis Generation for {symbol}\"\n",
    "            )\n",
    "            return analysis\n",
    "        except Exception as e:\n",
    "            return f\"Error generating initial analysis: {str(e)}\"\n",
    "    \n",
    "    def evaluate_analysis_quality(self, analysis: str, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the quality of the analysis using Azure OpenAI\"\"\"\n",
    "        evaluation_prompt = PromptConfiguration.get_evaluation_prompt(analysis, symbol)\n",
    "        \n",
    "        try:\n",
    "            evaluation_text, model_name = self.invoke_llm_with_logging(\n",
    "                evaluation_prompt, \n",
    "                f\"Analysis Quality Evaluation for {symbol}\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                evaluation = json.loads(evaluation_text)\n",
    "                evaluation['evaluation_model_used'] = model_name\n",
    "            except:\n",
    "                # Fallback evaluation\n",
    "                evaluation = {\n",
    "                    \"scores\": {\n",
    "                        \"completeness\": 7,\n",
    "                        \"data_integration\": 6,\n",
    "                        \"risk_assessment\": 6,\n",
    "                        \"actionability\": 7,\n",
    "                        \"logic_reasoning\": 7,\n",
    "                        \"market_context\": 6,\n",
    "                        \"clarity\": 7\n",
    "                    },\n",
    "                    \"overall_score\": 6.5,\n",
    "                    \"grade\": \"B\",\n",
    "                    \"strengths\": [\"Basic analysis provided\"],\n",
    "                    \"weaknesses\": [\"Could be more detailed\"],\n",
    "                    \"specific_improvements\": [\"Add more quantitative analysis\"],\n",
    "                    \"missing_elements\": [\"Market comparisons\"],\n",
    "                    \"evaluation_model_used\": model_name\n",
    "                }\n",
    "            \n",
    "            return evaluation\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\")\n",
    "            return {\n",
    "                \"overall_score\": 5,\n",
    "                \"grade\": \"C\",\n",
    "                \"strengths\": [\"Attempt made\"],\n",
    "                \"weaknesses\": [\"Evaluation error\"],\n",
    "                \"specific_improvements\": [\"Retry analysis\"],\n",
    "                \"missing_elements\": [\"Complete analysis\"],\n",
    "                \"evaluation_model_used\": \"Error\"\n",
    "            }\n",
    "    \n",
    "    def refine_analysis(self, original_analysis: str, evaluation: Dict[str, Any], symbol: str) -> str:\n",
    "        \"\"\"Refine analysis based on evaluation feedback\"\"\"\n",
    "        evaluation_str = json.dumps(evaluation, indent=2)\n",
    "        refinement_prompt = PromptConfiguration.get_refinement_prompt(original_analysis, evaluation_str, symbol)\n",
    "        \n",
    "        try:\n",
    "            refined_analysis, model_name = self.invoke_llm_with_logging(\n",
    "                refinement_prompt, \n",
    "                f\"Analysis Refinement for {symbol}\"\n",
    "            )\n",
    "            return refined_analysis\n",
    "        except Exception as e:\n",
    "            return f\"Error in refinement: {str(e)}. Original analysis maintained.\"\n",
    "    \n",
    "    def optimize_analysis(self, symbol: str, specialist_analyses: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the complete evaluator-optimizer loop\"\"\"\n",
    "        print(f\"Starting analysis optimization for {symbol}...\")\n",
    "        \n",
    "        iterations = []\n",
    "        current_analysis = self.generate_initial_analysis(symbol, specialist_analyses)\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"Optimization iteration {iteration + 1}/{self.max_iterations}\")\n",
    "            \n",
    "            # Evaluate current analysis\n",
    "            evaluation = self.evaluate_analysis_quality(current_analysis, symbol)\n",
    "            \n",
    "            iteration_data = {\n",
    "                'iteration': iteration + 1,\n",
    "                'analysis': current_analysis,\n",
    "                'evaluation': evaluation,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Check if quality is acceptable (grade A or B, or score > 8)\n",
    "            overall_score = evaluation.get('overall_score', 0)\n",
    "            grade = evaluation.get('grade', 'F')\n",
    "            \n",
    "            if grade in ['A', 'B'] or overall_score >= 8.0:\n",
    "                print(f\"Analysis quality acceptable (Grade: {grade}, Score: {overall_score})\")\n",
    "                iteration_data['optimization_complete'] = True\n",
    "                iterations.append(iteration_data)\n",
    "                break\n",
    "            \n",
    "            # Refine analysis if quality is not acceptable\n",
    "            if iteration < self.max_iterations - 1:  # Don't refine on last iteration\n",
    "                print(f\"Refining analysis (Grade: {grade}, Score: {overall_score})\")\n",
    "                current_analysis = self.refine_analysis(current_analysis, evaluation, symbol)\n",
    "                iteration_data['refinement_applied'] = True\n",
    "            else:\n",
    "                print(f\"Maximum iterations reached. Final grade: {grade}, Score: {overall_score}\")\n",
    "                iteration_data['max_iterations_reached'] = True\n",
    "            \n",
    "            iterations.append(iteration_data)\n",
    "        \n",
    "        # Store learning in memory\n",
    "        final_evaluation = iterations[-1]['evaluation']\n",
    "        learning_content = f\"\"\"\n",
    "        Analysis Optimization for {symbol}:\n",
    "        Iterations: {len(iterations)}\n",
    "        Final Grade: {final_evaluation.get('grade', 'N/A')}\n",
    "        Final Score: {final_evaluation.get('overall_score', 'N/A')}\n",
    "        Strengths: {', '.join(final_evaluation.get('strengths', []))}\n",
    "        Improvements: {', '.join(final_evaluation.get('specific_improvements', []))}\n",
    "        \"\"\"\n",
    "        \n",
    "        optimization_metadata = {\n",
    "            'type': 'optimization_learning',\n",
    "            'symbol': symbol,\n",
    "            'iterations': len(iterations),\n",
    "            'final_score': final_evaluation.get('overall_score', 0),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.memory.add_memory(learning_content, optimization_metadata)\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'optimization_iterations': iterations,\n",
    "            'final_analysis': current_analysis,\n",
    "            'final_evaluation': final_evaluation,\n",
    "            'improvement_achieved': len(iterations) > 1,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize evaluator-optimizer\n",
    "evaluator_optimizer = EvaluatorOptimizer(llm, agent_memory)\n",
    "print(\"Evaluator-Optimizer system created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d3080",
   "metadata": {},
   "source": [
    "The EvaluatorOptimizer class implements a self improving analytical loop that enables our Investment Research Agent to automatically assess and refine its own outputs through an iterative Generate → Evaluate → Refine process. This class acts as the system’s intelligence feedback engine, ensuring that every investment analysis reaches a high standard of quality before completion. It integrates directly with the language model (llm) and the agent’s persistent memory (AgentMemory) to produce, critique, and enhance investment reports using structured prompts and performance-based refinement.\n",
    "\n",
    "Technically, the process begins with the generate_initial_analysis() method, which synthesizes a comprehensive investment report by merging outputs from specialist agents (technical, fundamental, sentiment, and regulatory). The analysis is then passed to evaluate_analysis_quality(), which uses a pre-defined evaluation prompt to have the LLM grade the report across multiple dimensions completeness, data integration, risk assessment, actionability, logic, context, and clarity returning both scores and qualitative feedback (strengths, weaknesses, and improvement suggestions). If the resulting grade is below “A/B” or the overall score is under 8.0, the system triggers refine_analysis(), which regenerates an improved version guided by the previous evaluation. This loop continues for up to three iterations (max_iterations = 3) or until quality thresholds are met.\n",
    "\n",
    "The invoke_llm_with_logging() method ensures that every model invocation is auditable by recording the model name, token usage, and execution context, which supports transparency and debugging. Each iteration’s data analysis text, evaluation results, timestamps, and improvement flags are stored as structured records in memory for traceability and future learning. Once optimization concludes, the system saves a summary of the final grade, score, and insights into the FAISS memory vector store. In effect, the EvaluatorOptimizer transforms the LLM from a static text generator into a dynamic self evaluator, capable of continuous learning, performance tracking, and autonomous quality assurance within the investment research pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eeca8",
   "metadata": {},
   "source": [
    "## 8. Main Investment Research Agent Coordinator - TODO: Nelson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b40e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Investment Research Agent Coordinator initialized!\n"
     ]
    }
   ],
   "source": [
    "class MainInvestmentResearchAgent(InvestmentResearchAgent):\n",
    "    \"\"\"Main coordinator agent that orchestrates all workflows and patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: AgentMemory):\n",
    "        super().__init__(\"MainCoordinator\", \"Investment Research Coordinator\", memory)\n",
    "        self.routing_coordinator = routing_coordinator\n",
    "        self.evaluator_optimizer = evaluator_optimizer\n",
    "        self.news_chain = news_chain\n",
    "        \n",
    "    def conduct_comprehensive_research(self, symbol: str, request: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Conduct comprehensive investment research using all three workflow patterns:\n",
    "        1. Prompt Chaining (News Processing)\n",
    "        2. Routing (Specialist Analysis) \n",
    "        3. Evaluator-Optimizer (Analysis Refinement)\n",
    "        \"\"\"\n",
    "        print(f\"\\nStarting comprehensive investment research for {symbol}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Planning\n",
    "        if not request:\n",
    "            request = f\"Conduct comprehensive investment analysis for {symbol} including technical, fundamental, and sentiment analysis\"\n",
    "        \n",
    "        research_plan = self.plan_research(request)\n",
    "        print(f\"\\nResearch Plan Created ({len(research_plan)} steps)\")\n",
    "        \n",
    "        # Step 2: Retrieve past experience\n",
    "        past_experiences = self.retrieve_relevant_experience(request)\n",
    "        if past_experiences:\n",
    "            print(f\"Retrieved {len(past_experiences)} relevant past experiences\")\n",
    "        \n",
    "        # Step 3: Execute Routing Workflow (Specialist Analysis)\n",
    "        print(f\"\\nExecuting Routing Workflow...\")\n",
    "        routing_results = self.routing_coordinator.route_analysis(request, symbol)\n",
    "        \n",
    "        # Step 4: Execute Prompt Chaining (News Processing) - included in news specialist\n",
    "        print(f\"\\nNews Processing Chain completed within specialist analysis\")\n",
    "\n",
    "        # Step 5: Execute Evaluator-Optimizer Workflow\n",
    "        print(f\"\\nExecuting Evaluator-Optimizer Workflow...\")\n",
    "        specialist_analyses = routing_results.get('specialist_analyses', {})\n",
    "        optimization_results = self.evaluator_optimizer.optimize_analysis(symbol, specialist_analyses)\n",
    "        \n",
    "        # Step 6: Self-reflect on overall process\n",
    "        print(f\"\\nConducting self-reflection...\")\n",
    "        final_analysis = optimization_results.get('final_analysis', '')\n",
    "        reflection = self.self_reflect(final_analysis)\n",
    "        \n",
    "        # Step 7: Learn from this experience\n",
    "        print(f\"\\nLearning from this research experience...\")\n",
    "        self.learn_from_experience(request, final_analysis, reflection)\n",
    "        \n",
    "        # Compile comprehensive results\n",
    "        comprehensive_results = {\n",
    "            'symbol': symbol,\n",
    "            'request': request,\n",
    "            'research_plan': research_plan,\n",
    "            'past_experiences': past_experiences,\n",
    "            'routing_results': routing_results,\n",
    "            'optimization_results': optimization_results,\n",
    "            'self_reflection': reflection,\n",
    "            'execution_log': self.execution_log,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'agent_name': self.name\n",
    "        }\n",
    "\n",
    "        print(f\"\\nComprehensive research completed!\")\n",
    "        print(f\"Final Analysis Quality Score: {reflection.get('overall_score', 'N/A')}/10\")\n",
    "        \n",
    "        return comprehensive_results\n",
    "    \n",
    "    def generate_investment_report(self, research_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a formatted investment report from research results\"\"\"\n",
    "        symbol = research_results.get('symbol', 'N/A')\n",
    "        final_analysis = research_results.get('optimization_results', {}).get('final_analysis', '')\n",
    "        reflection = research_results.get('self_reflection', {})\n",
    "        \n",
    "        report_template = f\"\"\"\n",
    "        # Investment Research Report: {symbol}\n",
    "        **Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        **Research Quality Score:** {reflection.get('overall_score', 'N/A')}/10\n",
    "\n",
    "        ## Executive Summary\n",
    "        {final_analysis[:500]}...\n",
    "\n",
    "        ## Key Research Insights\n",
    "        - **Research Plan Execution:** {len(research_results.get('research_plan', []))} planned steps completed\n",
    "        - **Specialist Analyses:** {len(research_results.get('routing_results', {}).get('specialist_analyses', {}))} specialist reports generated\n",
    "        - **Analysis Iterations:** {len(research_results.get('optimization_results', {}).get('optimization_iterations', []))} optimization cycles\n",
    "        - **Quality Improvements:** {'Yes' if research_results.get('optimization_results', {}).get('improvement_achieved', False) else 'No'}\n",
    "\n",
    "        ## Analysis Quality Assessment\n",
    "        - **Strengths:** {', '.join(reflection.get('strengths', ['Analysis completed']))}\n",
    "        - **Recommendations:** {', '.join(reflection.get('recommendations', ['Continue monitoring']))}\n",
    "\n",
    "        ## Full Analysis\n",
    "        {final_analysis}\n",
    "\n",
    "        ---\n",
    "        *This report was generated by an AI Investment Research Agent.*\n",
    "        \"\"\"\n",
    "        \n",
    "        return report_template\n",
    "\n",
    "# Initialize main research agent\n",
    "main_research_agent = MainInvestmentResearchAgent(agent_memory)\n",
    "print(\"Main Investment Research Agent Coordinator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0b0c6",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Agent Testing - TODO: Swapnil\n",
    "\n",
    "Let's test the complete investment research agent with all four types of analysis on Apple Inc. (AAPL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02c269d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Configuration:\n",
      "  Symbol: AAPL (Apple Inc.)\n",
      "  Logging: Suppressed for clean output\n",
      "  Ready for individual analysis testing!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Configuration\n",
    "TEST_SYMBOL = \"AAPL\"  # Apple Inc. - change this to test other symbols\n",
    "print(f\"Testing Configuration:\")\n",
    "print(f\"  Symbol: {TEST_SYMBOL} (Apple Inc.)\")\n",
    "print(f\"  Logging: Suppressed for clean output\")\n",
    "print(f\"  Ready for individual analysis testing!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d0492",
   "metadata": {},
   "source": [
    "### Test 1: Quick Analysis\n",
    "\n",
    "This test provides a fast summary analysis with key metrics and investment recommendation for rapid decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c4b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUICK ANALYSIS TEST\n",
      "==================================================\n",
      "Testing Symbol: AAPL\n",
      "Analysis Type: Fast summary with key metrics and recommendation\n",
      "RoutingCoordinator - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 425 | Context: Routing Decision for AAPL\n",
      "Routing to fundamental specialist...\n",
      "RoutingCoordinator - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 425 | Context: Routing Decision for AAPL\n",
      "Routing to fundamental specialist...\n",
      "get_stock_data - Fetched fresh stock data for AAPL from Yahoo Finance\n",
      "Cached stock_data data for AAPL\n",
      "get_stock_data - Fetched fresh stock data for AAPL from Yahoo Finance\n",
      "Cached stock_data data for AAPL\n",
      "get_alpha_vantage_data - Fetched fresh Alpha Vantage data for AAPL function OVERVIEW\n",
      "Cached alpha_vantage data for AAPL\n",
      "get_alpha_vantage_data - Fetched fresh Alpha Vantage data for AAPL function OVERVIEW\n",
      "Cached alpha_vantage data for AAPL\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-4.1-mini-2025-04-14 | Tokens: 2441\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-4.1-mini-2025-04-14 | Tokens: 2441\n",
      "Cached fundamental analysis for AAPL\n",
      "Routing to technical specialist...\n",
      "Cached fundamental analysis for AAPL\n",
      "Routing to technical specialist...\n",
      "get_stock_data - Fetched fresh stock data for AAPL from Yahoo Finance\n",
      "Cached stock_data data for AAPL\n",
      "get_stock_data - Fetched fresh stock data for AAPL from Yahoo Finance\n",
      "Cached stock_data data for AAPL\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 2863\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 2863\n",
      "Cached technical analysis for AAPL\n",
      "Routing to news specialist...\n",
      "Starting news processing chain for AAPL...\n",
      "Step 1: Ingesting news...\n",
      "Cached technical analysis for AAPL\n",
      "Routing to news specialist...\n",
      "Starting news processing chain for AAPL...\n",
      "Step 1: Ingesting news...\n",
      "get_stock_news - Fetched fresh news for AAPL from NewsAPI\n",
      "Cached stock_news data for AAPL\n",
      "Step 2: Preprocessing news...\n",
      "Step 3: Classifying news...\n",
      "get_stock_news - Fetched fresh news for AAPL from NewsAPI\n",
      "Cached stock_news data for AAPL\n",
      "Step 2: Preprocessing news...\n",
      "Step 3: Classifying news...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 400 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 400 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 270 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 270 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 263 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 263 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 259 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 259 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 349 | Context: News Classification\n",
      "Step 4: Extracting insights...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 349 | Context: News Classification\n",
      "Step 4: Extracting insights...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 1227 | Context: Insights Extraction\n",
      "Step 5: Creating summary...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 1227 | Context: Insights Extraction\n",
      "Step 5: Creating summary...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 1625 | Context: News Summary for AAPL\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 1625 | Context: News Summary for AAPL\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 1790\n",
      "Routing to sec specialist...\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 1790\n",
      "Routing to sec specialist...\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 1797\n",
      "Routing to recommendation specialist...\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 1797\n",
      "Routing to recommendation specialist...\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 1484\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 1484\n",
      "Cached recommendation analysis for AAPL\n",
      "Quick Analysis COMPLETED\n",
      "   Execution Time: 140.43 seconds\n",
      "   Analysis Length: 49,328 characters\n",
      "   Specialists Activated: ['fundamental', 'technical', 'news', 'sec', 'recommendation']\n",
      "\n",
      "FINAL AGENT RESPONSE:\n",
      "============================================================\n",
      "\n",
      "FUNDAMENTAL SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "**Fundamental Analysis Report: Apple Inc. (AAPL)**  \n",
      "Date: [Current Date]\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Financial Health Assessment\n",
      "\n",
      "Apple Inc. demonstrates a robust financial position, underscored by its massive market capitalization (~$3.7 trillion) which signifies strong investor confidence and industry leadership. Recent stock price data indicates a current price of $249.34, close to its 52-week high of $259.24, highlighting market strength.\n",
      "\n",
      "Key indicators:  \n",
      "- **Earnings Per Share (EPS):** $6.59, reflecting solid profitability.  \n",
      "- **Dividend Yield:** 0.41%, denoting a modest but consistent return to shareholders.  \n",
      "- **Trading Volume:** Current volume runs at ~33.8 million shares, below the average volume of ~53.9 million, suggesting stable trading activity without extreme volatility.\n",
      "\n",
      "The company’s substantial free cash flow (not directly specified here but known historically) supports ongoing R&D, dividend payouts, and share repurchases, which collectively underpin financial stability. The mod...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 6915 total]\n",
      "\n",
      "TECHNICAL SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "Summary\n",
      "- Apple (AAPL) is in a medium‑term uptrend from the lows in the low $200s earlier in the history provided, with a recent run toward the mid‑$250s and a modest pullback to $249.34. Price action shows higher highs and higher lows over the period, but the most recent candles indicate consolidation after the move up.\n",
      "- Volume on the latest print (33.8M) is below the provided average daily volume (~54M), suggesting the most recent pullback is occurring on lighter turnover — typically a healthier, non-panicky retracement rather than a distribution spike.\n",
      "- Key technical levels: near‑term resistance around the mid‑$250s (256–259), support in the $245 area, then $238–230 beneath that. A break above 259 would extend the near‑term uptrend; a break below ~245/238 would increase the risk of a deeper correction.\n",
      "\n",
      "1) Price trends and patterns\n",
      "- Uptrend across the sample: The multi‑week sequence moves from low $210s into the mid‑$250s, indicating an overall bullish trend. Recent price swing: ...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 6297 total]\n",
      "\n",
      "NEWS SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "Professional Sentiment Analysis — AAPL\n",
      "\n",
      "Summary of dataset\n",
      "- Source set: 5 processed articles (all classified). The provided sample includes a high-importance, neutral management piece (Tesla/Elon Musk) and a geopolitics-focused article fragment referencing China/U.S. tensions. No AAPL-specific headlines were provided in full.\n",
      "- Because the sample mixes broader market/governance and geopolitical coverage rather than Apple-specific reporting, this analysis emphasizes how market- and sentiment-moving themes reflected in the corpus are likely to map onto Apple’s stock sentiment and risk profile.\n",
      "\n",
      "1) Overall sentiment assessment\n",
      "- Net sentiment: Neutral to mildly negative.\n",
      "  - Rationale: The explicit article available is neutral (management/governance analysis). The partial geopolitical article implies heightened macro risk and potential negative market reactions. With no explicit positive Apple-specific product or earnings coverage provided, the balance tilts to neutral with downside risk...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 6274 total]\n",
      "\n",
      "SEC SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "I cannot access SEC filings for AAPL in this session because the data source/tool (get_sec_filings) is not available. Below is a professional regulatory analysis framework you can use once the filings are accessible, followed by typical itemized considerations for Apple Inc. (AAPL) that you should review in the actual documents. This structure will help you extract key disclosures, risk factors, MD&A insights, financial highlights, governance observations, and material events relevant to investment decisions.\n",
      "\n",
      "1) Key regulatory disclosures (what to extract)\n",
      "- Form 10-K / 10-Q disclosures:\n",
      "  - Company overview, business segments (e.g., Devices, Services, Wearables), and significant accounting policies.\n",
      "  - Selected financial data and critical accounting estimates (CAEs) that affect revenue recognition, inventory, reserves, and goodwill impairment.\n",
      "  - Legal proceedings: ongoing litigation, settlements, contingencies, and loss accruals.\n",
      "  - Risk factors: consolidated list of material ris...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 8061 total]\n",
      "\n",
      "RECOMMENDATION SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "Investment Recommendation: Apple Inc. (AAPL)\n",
      "\n",
      "Date: [Current Date]\n",
      "\n",
      "1) Investment Thesis\n",
      "- Durable competitive advantages: Apple remains a leading ecosystem company with strong brand equity, sticky services, high-margin hardware, and a robust installed base. The company’s stock tends to reflect the monetization of its ecosystem (iPhone, Services, Wearables, Mac) and resilient demand, even amid slower hardware cycles.\n",
      "- Strong financial health: While exact figures are not available in this session, Apple is widely recognized for robust cash generation, substantial liquidity, and disciplined capital allocation (buybacks, dividends, and value-added investments). This foundation supports continued product innovation and services growth.\n",
      "- Technical backdrop: The stock has shown a medium-term uptrend from a prior low in the low $200s, with a recent move toward the mid-$250s and a modest pullback to approximately $249.34. This suggests constructive price momentum with a potential retest of n...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 5567 total]\n",
      "============================================================\n",
      "Cached recommendation analysis for AAPL\n",
      "Quick Analysis COMPLETED\n",
      "   Execution Time: 140.43 seconds\n",
      "   Analysis Length: 49,328 characters\n",
      "   Specialists Activated: ['fundamental', 'technical', 'news', 'sec', 'recommendation']\n",
      "\n",
      "FINAL AGENT RESPONSE:\n",
      "============================================================\n",
      "\n",
      "FUNDAMENTAL SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "**Fundamental Analysis Report: Apple Inc. (AAPL)**  \n",
      "Date: [Current Date]\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Financial Health Assessment\n",
      "\n",
      "Apple Inc. demonstrates a robust financial position, underscored by its massive market capitalization (~$3.7 trillion) which signifies strong investor confidence and industry leadership. Recent stock price data indicates a current price of $249.34, close to its 52-week high of $259.24, highlighting market strength.\n",
      "\n",
      "Key indicators:  \n",
      "- **Earnings Per Share (EPS):** $6.59, reflecting solid profitability.  \n",
      "- **Dividend Yield:** 0.41%, denoting a modest but consistent return to shareholders.  \n",
      "- **Trading Volume:** Current volume runs at ~33.8 million shares, below the average volume of ~53.9 million, suggesting stable trading activity without extreme volatility.\n",
      "\n",
      "The company’s substantial free cash flow (not directly specified here but known historically) supports ongoing R&D, dividend payouts, and share repurchases, which collectively underpin financial stability. The mod...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 6915 total]\n",
      "\n",
      "TECHNICAL SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "Summary\n",
      "- Apple (AAPL) is in a medium‑term uptrend from the lows in the low $200s earlier in the history provided, with a recent run toward the mid‑$250s and a modest pullback to $249.34. Price action shows higher highs and higher lows over the period, but the most recent candles indicate consolidation after the move up.\n",
      "- Volume on the latest print (33.8M) is below the provided average daily volume (~54M), suggesting the most recent pullback is occurring on lighter turnover — typically a healthier, non-panicky retracement rather than a distribution spike.\n",
      "- Key technical levels: near‑term resistance around the mid‑$250s (256–259), support in the $245 area, then $238–230 beneath that. A break above 259 would extend the near‑term uptrend; a break below ~245/238 would increase the risk of a deeper correction.\n",
      "\n",
      "1) Price trends and patterns\n",
      "- Uptrend across the sample: The multi‑week sequence moves from low $210s into the mid‑$250s, indicating an overall bullish trend. Recent price swing: ...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 6297 total]\n",
      "\n",
      "NEWS SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "Professional Sentiment Analysis — AAPL\n",
      "\n",
      "Summary of dataset\n",
      "- Source set: 5 processed articles (all classified). The provided sample includes a high-importance, neutral management piece (Tesla/Elon Musk) and a geopolitics-focused article fragment referencing China/U.S. tensions. No AAPL-specific headlines were provided in full.\n",
      "- Because the sample mixes broader market/governance and geopolitical coverage rather than Apple-specific reporting, this analysis emphasizes how market- and sentiment-moving themes reflected in the corpus are likely to map onto Apple’s stock sentiment and risk profile.\n",
      "\n",
      "1) Overall sentiment assessment\n",
      "- Net sentiment: Neutral to mildly negative.\n",
      "  - Rationale: The explicit article available is neutral (management/governance analysis). The partial geopolitical article implies heightened macro risk and potential negative market reactions. With no explicit positive Apple-specific product or earnings coverage provided, the balance tilts to neutral with downside risk...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 6274 total]\n",
      "\n",
      "SEC SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "I cannot access SEC filings for AAPL in this session because the data source/tool (get_sec_filings) is not available. Below is a professional regulatory analysis framework you can use once the filings are accessible, followed by typical itemized considerations for Apple Inc. (AAPL) that you should review in the actual documents. This structure will help you extract key disclosures, risk factors, MD&A insights, financial highlights, governance observations, and material events relevant to investment decisions.\n",
      "\n",
      "1) Key regulatory disclosures (what to extract)\n",
      "- Form 10-K / 10-Q disclosures:\n",
      "  - Company overview, business segments (e.g., Devices, Services, Wearables), and significant accounting policies.\n",
      "  - Selected financial data and critical accounting estimates (CAEs) that affect revenue recognition, inventory, reserves, and goodwill impairment.\n",
      "  - Legal proceedings: ongoing litigation, settlements, contingencies, and loss accruals.\n",
      "  - Risk factors: consolidated list of material ris...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 8061 total]\n",
      "\n",
      "RECOMMENDATION SPECIALIST ANALYSIS:\n",
      "----------------------------------------\n",
      "Investment Recommendation: Apple Inc. (AAPL)\n",
      "\n",
      "Date: [Current Date]\n",
      "\n",
      "1) Investment Thesis\n",
      "- Durable competitive advantages: Apple remains a leading ecosystem company with strong brand equity, sticky services, high-margin hardware, and a robust installed base. The company’s stock tends to reflect the monetization of its ecosystem (iPhone, Services, Wearables, Mac) and resilient demand, even amid slower hardware cycles.\n",
      "- Strong financial health: While exact figures are not available in this session, Apple is widely recognized for robust cash generation, substantial liquidity, and disciplined capital allocation (buybacks, dividends, and value-added investments). This foundation supports continued product innovation and services growth.\n",
      "- Technical backdrop: The stock has shown a medium-term uptrend from a prior low in the low $200s, with a recent move toward the mid-$250s and a modest pullback to approximately $249.34. This suggests constructive price momentum with a potential retest of n...\n",
      "\n",
      "[Response truncated - showing first 1000 characters of 5567 total]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Quick Analysis\n",
    "print(\"QUICK ANALYSIS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Symbol: {TEST_SYMBOL}\")\n",
    "print(\"Analysis Type: Fast summary with key metrics and recommendation\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with suppress_llm_logs():\n",
    "        quick_request = f\"Provide quick investment summary for {TEST_SYMBOL} with key metrics and recommendation\"\n",
    "        \n",
    "        quick_results = routing_coordinator.route_analysis(quick_request, TEST_SYMBOL)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    execution_time = time.time() - start_time\n",
    "    analysis_length = len(str(quick_results))\n",
    "    specialists_activated = list(quick_results.get('specialist_analyses', {}).keys())\n",
    "    \n",
    "    print(f\"Quick Analysis COMPLETED\")\n",
    "    print(f\"   Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   Analysis Length: {analysis_length:,} characters\")\n",
    "    print(f\"   Specialists Activated: {specialists_activated}\")\n",
    "    \n",
    "    # Print final agent response\n",
    "    print(f\"\\nFINAL AGENT RESPONSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    for specialist, analysis in quick_results.get('specialist_analyses', {}).items():\n",
    "        print(f\"\\n{specialist.upper()} SPECIALIST ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        analysis_content = analysis.get('analysis', 'No analysis content available')\n",
    "        # Truncate very long responses for readability\n",
    "        if len(analysis_content) > 1000:\n",
    "            print(f\"{analysis_content[:1000]}...\")\n",
    "            print(f\"\\n[Response truncated - showing first 1000 characters of {len(analysis_content)} total]\")\n",
    "        else:\n",
    "            print(analysis_content)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store results for summary\n",
    "    quick_test_result = {\n",
    "        \"success\": True,\n",
    "        \"execution_time\": execution_time,\n",
    "        \"analysis_length\": analysis_length,\n",
    "        \"specialists_activated\": specialists_activated,\n",
    "        \"agent_response\": quick_results\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Quick Analysis FAILED: {str(e)}\")\n",
    "    quick_test_result = {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acb237",
   "metadata": {},
   "source": [
    "### Test 2: Comprehensive Analysis (Full System Test)\n",
    "\n",
    "This test runs the complete investment research agent with all specialists and optimization cycles. It provides the most thorough analysis combining technical, fundamental, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965212bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE ANALYSIS TEST\n",
      "==================================================\n",
      "Testing Symbol: AAPL\n",
      "Analysis Type: Full system test with all specialists\n",
      "\n",
      "Starting comprehensive investment research for AAPL\n",
      "============================================================\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 2208\n",
      "\n",
      "Research Plan Created (101 steps)\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 2208\n",
      "\n",
      "Research Plan Created (101 steps)\n",
      "\n",
      "Executing Routing Workflow...\n",
      "\n",
      "Executing Routing Workflow...\n",
      "RoutingCoordinator - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 443 | Context: Routing Decision for AAPL\n",
      "Routing to fundamental specialist...\n",
      "RoutingCoordinator - LLM Model Used: gpt-5-mini-2025-08-07 | Tokens: 443 | Context: Routing Decision for AAPL\n",
      "Routing to fundamental specialist...\n",
      "Using cached fundamental analysis for AAPL\n",
      "Routing to sec specialist...\n",
      "Using cached fundamental analysis for AAPL\n",
      "Routing to sec specialist...\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 2197\n",
      "Routing to news specialist...\n",
      "Starting news processing chain for AAPL...\n",
      "Step 1: Ingesting news...\n",
      "Using cached stock_news data for AAPL\n",
      "Step 2: Preprocessing news...\n",
      "Step 3: Classifying news...\n",
      "InvestmentResearchAgent - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 2197\n",
      "Routing to news specialist...\n",
      "Starting news processing chain for AAPL...\n",
      "Step 1: Ingesting news...\n",
      "Using cached stock_news data for AAPL\n",
      "Step 2: Preprocessing news...\n",
      "Step 3: Classifying news...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 329 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 329 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 195 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 195 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 263 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 263 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 329 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 329 | Context: News Classification\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 360 | Context: News Classification\n",
      "Step 4: Extracting insights...\n",
      "NewsProcessingChain - LLM Model Used: gpt-5-nano-2025-08-07 | Tokens: 360 | Context: News Classification\n",
      "Step 4: Extracting insights...\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Comprehensive Analysis\n",
    "print(\"COMPREHENSIVE ANALYSIS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Symbol: {TEST_SYMBOL}\")\n",
    "print(\"Analysis Type: Full system test with all specialists\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with suppress_llm_logs():\n",
    "        # Run comprehensive research with all specialists\n",
    "        comprehensive_request = f\"Conduct comprehensive investment analysis for {TEST_SYMBOL} including technical, fundamental, and sentiment analysis with detailed recommendations\"\n",
    "        \n",
    "        comprehensive_results = main_research_agent.conduct_comprehensive_research(\n",
    "            symbol=TEST_SYMBOL, \n",
    "            request=comprehensive_request\n",
    "        )\n",
    "        \n",
    "        # Generate investment report\n",
    "        investment_report = main_research_agent.generate_investment_report(comprehensive_results)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    execution_time = time.time() - start_time\n",
    "    quality_score = comprehensive_results.get('self_reflection', {}).get('overall_score', 'N/A')\n",
    "    specialists_used = len(comprehensive_results.get('routing_results', {}).get('specialist_analyses', {}))\n",
    "    optimization_cycles = len(comprehensive_results.get('optimization_results', {}).get('optimization_iterations', []))\n",
    "    report_length = len(investment_report)\n",
    "    \n",
    "    print(f\"Comprehensive Analysis COMPLETED\")\n",
    "    print(f\"   Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   Quality Score: {quality_score}/10\")\n",
    "    print(f\"   Specialists Used: {specialists_used}\")\n",
    "    print(f\"   Optimization Cycles: {optimization_cycles}\")\n",
    "    print(f\"   Report Length: {report_length:,} characters\")\n",
    "    \n",
    "    # Print final agent response (investment report)\n",
    "    print(f\"\\nFINAL INVESTMENT REPORT:\")\n",
    "    print(\"=\" * 60)\n",
    "    # Truncate very long reports for readability\n",
    "    if len(investment_report) > 2000:\n",
    "        print(f\"{investment_report[:2000]}...\")\n",
    "        print(f\"\\n[Report truncated - showing first 2000 characters of {len(investment_report)} total]\")\n",
    "    else:\n",
    "        print(investment_report)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store results for summary\n",
    "    comprehensive_test_result = {\n",
    "        \"success\": True,\n",
    "        \"execution_time\": execution_time,\n",
    "        \"quality_score\": quality_score,\n",
    "        \"specialists_used\": specialists_used,\n",
    "        \"optimization_cycles\": optimization_cycles,\n",
    "        \"report_length\": report_length,\n",
    "        \"full_results\": comprehensive_results,\n",
    "        \"investment_report\": investment_report\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Comprehensive Analysis FAILED: {str(e)}\")\n",
    "    comprehensive_test_result = {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1e7fc",
   "metadata": {},
   "source": [
    "### Test 3: Technical Analysis\n",
    "\n",
    "This test focuses specifically on technical analysis including chart patterns, indicators, price trends, and momentum analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Technical Analysis\n",
    "print(\"TECHNICAL ANALYSIS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Symbol: {TEST_SYMBOL}\")\n",
    "print(\"Analysis Type: Technical patterns, indicators, and trends\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with suppress_llm_logs():\n",
    "        technical_request = f\"Perform detailed technical analysis on {TEST_SYMBOL} stock including chart patterns, indicators, and price trends\"\n",
    "        \n",
    "        technical_results = routing_coordinator.route_analysis(technical_request, TEST_SYMBOL)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    execution_time = time.time() - start_time\n",
    "    analysis_length = len(str(technical_results))\n",
    "    specialists_activated = list(technical_results.get('specialist_analyses', {}).keys())\n",
    "    routing_reasoning = technical_results.get('routing_decision', {}).get('reasoning', 'N/A')\n",
    "    \n",
    "    print(f\"Technical Analysis COMPLETED\")\n",
    "    print(f\"   Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   Analysis Length: {analysis_length:,} characters\")\n",
    "    print(f\"   Specialists Activated: {specialists_activated}\")\n",
    "    \n",
    "    # Print final agent response\n",
    "    print(f\"\\nFINAL AGENT RESPONSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Routing Reasoning: {routing_reasoning}\")\n",
    "    print()\n",
    "    for specialist, analysis in technical_results.get('specialist_analyses', {}).items():\n",
    "        print(f\"\\n{specialist.upper()} SPECIALIST ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        analysis_content = analysis.get('analysis', 'No analysis content available')\n",
    "        # Truncate very long responses for readability\n",
    "        if len(analysis_content) > 1000:\n",
    "            print(f\"{analysis_content[:1000]}...\")\n",
    "            print(f\"\\n[Response truncated - showing first 1000 characters of {len(analysis_content)} total]\")\n",
    "        else:\n",
    "            print(analysis_content)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store results for summary\n",
    "    technical_test_result = {\n",
    "        \"success\": True,\n",
    "        \"execution_time\": execution_time,\n",
    "        \"analysis_length\": analysis_length,\n",
    "        \"specialists_activated\": specialists_activated,\n",
    "        \"routing_reasoning\": routing_reasoning,\n",
    "        \"agent_response\": technical_results\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Technical Analysis FAILED: {str(e)}\")\n",
    "    technical_test_result = {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1118ed9",
   "metadata": {},
   "source": [
    "### Test 4: Fundamental Analysis\n",
    "\n",
    "This test focuses on fundamental analysis including financial metrics, valuation ratios, business performance, and competitive positioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Fundamental Analysis\n",
    "print(\"FUNDAMENTAL ANALYSIS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Symbol: {TEST_SYMBOL}\")\n",
    "print(\"Analysis Type: Financial metrics, valuation, and business performance\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with suppress_llm_logs():\n",
    "        fundamental_request = f\"Analyze {TEST_SYMBOL} fundamentals including financial metrics, valuation, business performance, and competitive position\"\n",
    "        \n",
    "        fundamental_results = routing_coordinator.route_analysis(fundamental_request, TEST_SYMBOL)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    execution_time = time.time() - start_time\n",
    "    analysis_length = len(str(fundamental_results))\n",
    "    specialists_activated = list(fundamental_results.get('specialist_analyses', {}).keys())\n",
    "    routing_reasoning = fundamental_results.get('routing_decision', {}).get('reasoning', 'N/A')\n",
    "    \n",
    "    print(f\"Fundamental Analysis COMPLETED\")\n",
    "    print(f\"   Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   Analysis Length: {analysis_length:,} characters\")\n",
    "    print(f\"   Specialists Activated: {specialists_activated}\")\n",
    "    \n",
    "    # Print final agent response\n",
    "    print(f\"\\nFINAL AGENT RESPONSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Routing Reasoning: {routing_reasoning}\")\n",
    "    print()\n",
    "    for specialist, analysis in fundamental_results.get('specialist_analyses', {}).items():\n",
    "        print(f\"\\n{specialist.upper()} SPECIALIST ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        analysis_content = analysis.get('analysis', 'No analysis content available')\n",
    "        # Truncate very long responses for readability\n",
    "        if len(analysis_content) > 1000:\n",
    "            print(f\"{analysis_content[:1000]}...\")\n",
    "            print(f\"\\n[Response truncated - showing first 1000 characters of {len(analysis_content)} total]\")\n",
    "        else:\n",
    "            print(analysis_content)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store results for summary\n",
    "    fundamental_test_result = {\n",
    "        \"success\": True,\n",
    "        \"execution_time\": execution_time,\n",
    "        \"analysis_length\": analysis_length,\n",
    "        \"specialists_activated\": specialists_activated,\n",
    "        \"routing_reasoning\": routing_reasoning,\n",
    "        \"agent_response\": fundamental_results\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fundamental Analysis FAILED: {str(e)}\")\n",
    "    fundamental_test_result = {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c705ccd",
   "metadata": {},
   "source": [
    "### Test 5: News & Sentiment Analysis\n",
    "\n",
    "This test focuses on news analysis and sentiment evaluation including recent media coverage, market sentiment, and investor opinion analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: News & Sentiment Analysis\n",
    "print(\"NEWS & SENTIMENT ANALYSIS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Symbol: {TEST_SYMBOL}\")\n",
    "print(\"Analysis Type: News coverage and market sentiment\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with suppress_llm_logs():\n",
    "        news_request = f\"Analyze recent news and market sentiment for {TEST_SYMBOL} including media coverage and investor sentiment\"\n",
    "        \n",
    "        # Run routing analysis for news\n",
    "        news_results = routing_coordinator.route_analysis(news_request, TEST_SYMBOL)\n",
    "        \n",
    "        # Also test news processing chain directly\n",
    "        news_articles = news_chain.ingest_news(TEST_SYMBOL)\n",
    "        news_analysis = news_chain.process_news_chain(TEST_SYMBOL)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    execution_time = time.time() - start_time\n",
    "    analysis_length = len(str(news_results))\n",
    "    specialists_activated = list(news_results.get('specialist_analyses', {}).keys())\n",
    "    articles_processed = len(news_articles)\n",
    "    news_chain_analysis_length = len(str(news_analysis))\n",
    "    \n",
    "    print(f\"News & Sentiment Analysis COMPLETED\")\n",
    "    print(f\"   Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   Analysis Length: {analysis_length:,} characters\")\n",
    "    print(f\"   Specialists Activated: {specialists_activated}\")\n",
    "    print(f\"   Articles Processed: {articles_processed}\")\n",
    "    print(f\"   News Chain Analysis: {news_chain_analysis_length:,} characters\")\n",
    "    \n",
    "    # Print final agent response\n",
    "    print(f\"\\nFINAL AGENT RESPONSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Articles Found: {articles_processed}\")\n",
    "    print()\n",
    "    for specialist, analysis in news_results.get('specialist_analyses', {}).items():\n",
    "        print(f\"\\n{specialist.upper()} SPECIALIST ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        analysis_content = analysis.get('analysis', 'No analysis content available')\n",
    "        # Truncate very long responses for readability\n",
    "        if len(analysis_content) > 1000:\n",
    "            print(f\"{analysis_content[:1000]}...\")\n",
    "            print(f\"\\n[Response truncated - showing first 1000 characters of {len(analysis_content)} total]\")\n",
    "        else:\n",
    "            print(analysis_content)\n",
    "    \n",
    "    print(f\"\\nNEWS PROCESSING CHAIN ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    news_analysis_str = str(news_analysis)\n",
    "    if len(news_analysis_str) > 1000:\n",
    "        print(f\"{news_analysis_str[:1000]}...\")\n",
    "        print(f\"\\n[Analysis truncated - showing first 1000 characters of {len(news_analysis_str)} total]\")\n",
    "    else:\n",
    "        print(news_analysis_str)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store results for summary\n",
    "    news_test_result = {\n",
    "        \"success\": True,\n",
    "        \"execution_time\": execution_time,\n",
    "        \"analysis_length\": analysis_length,\n",
    "        \"specialists_activated\": specialists_activated,\n",
    "        \"articles_processed\": articles_processed,\n",
    "        \"news_chain_analysis\": news_chain_analysis_length,\n",
    "        \"agent_response\": news_results,\n",
    "        \"news_chain_response\": news_analysis\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"News & Sentiment Analysis FAILED: {str(e)}\")\n",
    "    news_test_result = {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e7b4e",
   "metadata": {},
   "source": [
    "### Test 6: Investment Recommendation Analysis\n",
    "\n",
    "This test focuses on generating specific investment recommendations including Strong Buy, Buy, Hold, Sell, or Strong Sell ratings with confidence levels and target prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b362dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Investment Recommendation Analysis\n",
    "print(\"INVESTMENT RECOMMENDATION ANALYSIS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Symbol: {TEST_SYMBOL}\")\n",
    "print(\"Analysis Type: Investment recommendations with buy/sell/hold ratings\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with suppress_llm_logs():\n",
    "        recommendation_request = f\"Provide detailed investment recommendation for {TEST_SYMBOL} including buy/sell/hold rating, target price, and confidence level\"\n",
    "        \n",
    "        # Run routing analysis for investment recommendation\n",
    "        recommendation_results = routing_coordinator.route_analysis(recommendation_request, TEST_SYMBOL)\n",
    "        \n",
    "        # Also test the investment recommendation tool directly\n",
    "        direct_recommendation = get_investment_recommendation.invoke({\"symbol\": TEST_SYMBOL, \"analysis_context\": \"Standalone recommendation test\"})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    execution_time = time.time() - start_time\n",
    "    analysis_length = len(str(recommendation_results))\n",
    "    specialists_activated = list(recommendation_results.get('specialist_analyses', {}).keys())\n",
    "    routing_reasoning = recommendation_results.get('routing_decision', {}).get('reasoning', 'N/A')\n",
    "    \n",
    "    # Extract recommendation details if available\n",
    "    recommendation_specialist_data = recommendation_results.get('specialist_analyses', {}).get('recommendation', {})\n",
    "    investment_recommendation = recommendation_specialist_data.get('recommendation', 'N/A')\n",
    "    confidence_level = recommendation_specialist_data.get('confidence', 'N/A')\n",
    "    target_price = recommendation_specialist_data.get('target_price', 'N/A')\n",
    "    current_price = recommendation_specialist_data.get('current_price', 'N/A')\n",
    "    risk_level = recommendation_specialist_data.get('risk_level', 'N/A')\n",
    "    \n",
    "    print(f\"Investment Recommendation Analysis COMPLETED\")\n",
    "    print(f\"   Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"   Analysis Length: {analysis_length:,} characters\")\n",
    "    print(f\"   Specialists Activated: {specialists_activated}\")\n",
    "    print(f\"   Investment Recommendation: {investment_recommendation}\")\n",
    "    print(f\"   Confidence Level: {confidence_level}\")\n",
    "    print(f\"   Target Price: ${target_price}\")\n",
    "    print(f\"   Current Price: ${current_price}\")\n",
    "    print(f\"   Risk Level: {risk_level}\")\n",
    "    \n",
    "    # Print final agent response\n",
    "    print(f\"\\nFINAL AGENT RESPONSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Routing Reasoning: {routing_reasoning}\")\n",
    "    print()\n",
    "    for specialist, analysis in recommendation_results.get('specialist_analyses', {}).items():\n",
    "        print(f\"\\n{specialist.upper()} SPECIALIST ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Special handling for recommendation specialist\n",
    "        if specialist == 'recommendation':\n",
    "            rec_data = analysis.get('recommendation_data', {})\n",
    "            print(f\"RECOMMENDATION: {rec_data.get('recommendation', 'N/A')}\")\n",
    "            print(f\"CONFIDENCE: {rec_data.get('confidence', 'N/A')}\")\n",
    "            print(f\"TARGET PRICE: ${rec_data.get('target_price', 'N/A')}\")\n",
    "            print(f\"CURRENT PRICE: ${rec_data.get('current_price', 'N/A')}\")\n",
    "            print(f\"PRICE CHANGE EXPECTED: {rec_data.get('price_target_change_pct', 'N/A')}%\")\n",
    "            print(f\"RISK LEVEL: {rec_data.get('risk_level', 'N/A')}\")\n",
    "            print(f\"KEY FACTORS: {', '.join(rec_data.get('key_factors', ['N/A'])[:3])}\")\n",
    "            print()\n",
    "        \n",
    "        analysis_content = analysis.get('analysis', 'No analysis content available')\n",
    "        # Truncate very long responses for readability\n",
    "        if len(analysis_content) > 1000:\n",
    "            print(f\"{analysis_content[:1000]}...\")\n",
    "            print(f\"\\n[Response truncated - showing first 1000 characters of {len(analysis_content)} total]\")\n",
    "        else:\n",
    "            print(analysis_content)\n",
    "    \n",
    "    print(f\"\\nDIRECT RECOMMENDATION TOOL OUTPUT:\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        direct_rec_data = json.loads(direct_recommendation)\n",
    "        print(f\"DIRECT RECOMMENDATION: {direct_rec_data.get('recommendation', 'N/A')}\")\n",
    "        print(f\"CONFIDENCE: {direct_rec_data.get('confidence', 'N/A')}\")\n",
    "        print(f\"SCORE: {direct_rec_data.get('recommendation_score', 'N/A')}/100\")\n",
    "        print(f\"TARGET PRICE: ${direct_rec_data.get('target_price', 'N/A')}\")\n",
    "    except:\n",
    "        print(\"Failed to parse direct recommendation data\")\n",
    "        if len(direct_recommendation) > 500:\n",
    "            print(f\"{direct_recommendation[:500]}...\")\n",
    "        else:\n",
    "            print(direct_recommendation)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store results for summary\n",
    "    recommendation_test_result = {\n",
    "        \"success\": True,\n",
    "        \"execution_time\": execution_time,\n",
    "        \"analysis_length\": analysis_length,\n",
    "        \"specialists_activated\": specialists_activated,\n",
    "        \"routing_reasoning\": routing_reasoning,\n",
    "        \"investment_recommendation\": investment_recommendation,\n",
    "        \"confidence_level\": confidence_level,\n",
    "        \"target_price\": target_price,\n",
    "        \"current_price\": current_price,\n",
    "        \"risk_level\": risk_level,\n",
    "        \"agent_response\": recommendation_results,\n",
    "        \"direct_recommendation\": direct_recommendation\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Investment Recommendation Analysis FAILED: {str(e)}\")\n",
    "    recommendation_test_result = {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d557fa",
   "metadata": {},
   "source": [
    "### Testing Summary & Results\n",
    "\n",
    "This cell provides a comprehensive summary of all test results, showing success rates, execution times, and key metrics for each analysis type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Summary & Results\n",
    "print(\"INVESTMENT RESEARCH AGENT - TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test Symbol: {TEST_SYMBOL} (Apple Inc.)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all test results\n",
    "test_results = {}\n",
    "\n",
    "# Check if test results exist (they should be defined from running the individual tests)\n",
    "try:\n",
    "    test_results[\"comprehensive\"] = comprehensive_test_result\n",
    "except NameError:\n",
    "    test_results[\"comprehensive\"] = {\"success\": False, \"error\": \"Test not run\"}\n",
    "\n",
    "try:\n",
    "    test_results[\"technical\"] = technical_test_result\n",
    "except NameError:\n",
    "    test_results[\"technical\"] = {\"success\": False, \"error\": \"Test not run\"}\n",
    "\n",
    "try:\n",
    "    test_results[\"fundamental\"] = fundamental_test_result\n",
    "except NameError:\n",
    "    test_results[\"fundamental\"] = {\"success\": False, \"error\": \"Test not run\"}\n",
    "\n",
    "try:\n",
    "    test_results[\"news_sentiment\"] = news_test_result\n",
    "except NameError:\n",
    "    test_results[\"news_sentiment\"] = {\"success\": False, \"error\": \"Test not run\"}\n",
    "\n",
    "try:\n",
    "    test_results[\"quick\"] = quick_test_result\n",
    "except NameError:\n",
    "    test_results[\"quick\"] = {\"success\": False, \"error\": \"Test not run\"}\n",
    "\n",
    "try:\n",
    "    test_results[\"investment_recommendation\"] = recommendation_test_result\n",
    "except NameError:\n",
    "    test_results[\"investment_recommendation\"] = {\"success\": False, \"error\": \"Test not run\"}\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_tests = len(test_results)\n",
    "successful_tests = sum(1 for r in test_results.values() if r.get('success', False))\n",
    "success_rate = (successful_tests/total_tests)*100 if total_tests > 0 else 0\n",
    "\n",
    "# Display overall summary\n",
    "print(f\"OVERALL RESULTS:\")\n",
    "print(f\"   Total Tests: {total_tests}\")\n",
    "print(f\"   Successful: {successful_tests}\")\n",
    "print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "\n",
    "status = \"ALL SYSTEMS OPERATIONAL\" if success_rate == 100 else \"SOME ISSUES DETECTED\" if success_rate >= 50 else \"SYSTEM ISSUES\"\n",
    "print(f\"   Status: {status}\")\n",
    "\n",
    "# Display individual test results\n",
    "print(f\"\\nDETAILED RESULTS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for test_name, result in test_results.items():\n",
    "    status_text = \"PASS\" if result.get('success', False) else \"FAIL\"\n",
    "    exec_time = result.get('execution_time', 0)\n",
    "    \n",
    "    test_display_name = test_name.replace('_', ' ').title()\n",
    "    print(f\"{test_display_name:<25} {status_text:<8} ({exec_time:>6.2f}s)\")\n",
    "    \n",
    "    if result.get('success', False):\n",
    "        # Show specific metrics for each test type\n",
    "        if test_name == \"comprehensive\":\n",
    "            quality = result.get('quality_score', 'N/A')\n",
    "            specialists = result.get('specialists_used', 0)\n",
    "            cycles = result.get('optimization_cycles', 0)\n",
    "            print(f\"     Quality Score: {quality}/10  |  Specialists: {specialists}  |  Opt Cycles: {cycles}\")\n",
    "        \n",
    "        elif test_name in [\"technical\", \"fundamental\", \"quick\"]:\n",
    "            specialists = result.get('specialists_activated', [])\n",
    "            length = result.get('analysis_length', 0)\n",
    "            print(f\"     Specialists: {specialists}  |  Analysis: {length:,} chars\")\n",
    "        \n",
    "        elif test_name == \"news_sentiment\":\n",
    "            specialists = result.get('specialists_activated', [])\n",
    "            articles = result.get('articles_processed', 0)\n",
    "            print(f\"     Specialists: {specialists}  |  Articles: {articles}\")\n",
    "        \n",
    "        elif test_name == \"investment_recommendation\":\n",
    "            specialists = result.get('specialists_activated', [])\n",
    "            recommendation = result.get('investment_recommendation', 'N/A')\n",
    "            confidence = result.get('confidence_level', 'N/A')\n",
    "            print(f\"     Specialists: {specialists}  |  Recommendation: {recommendation} ({confidence})\")\n",
    "    \n",
    "    else:\n",
    "        error_msg = result.get('error', 'Unknown error')\n",
    "        print(f\"     Error: {error_msg[:50]}...\")\n",
    "\n",
    "# Show performance statistics\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "execution_times = [r.get('execution_time', 0) for r in test_results.values() if r.get('success', False)]\n",
    "if execution_times:\n",
    "    avg_time = sum(execution_times) / len(execution_times)\n",
    "    max_time = max(execution_times)\n",
    "    min_time = min(execution_times)\n",
    "    \n",
    "    print(f\"   Average Execution Time: {avg_time:.2f} seconds\")\n",
    "    print(f\"   Fastest Test: {min_time:.2f} seconds\")\n",
    "    print(f\"   Slowest Test: {max_time:.2f} seconds\")\n",
    "    print(f\"   Total Execution Time: {sum(execution_times):.2f} seconds\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING COMPLETED - READY FOR INVESTMENT ANALYSIS!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3246b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run All Tests Sequentially\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all investment research agent tests sequentially\"\"\"\n",
    "    \n",
    "    print(\"RUNNING ALL INVESTMENT RESEARCH AGENT TESTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"This may take several minutes to complete...\")\n",
    "    print(\"Tip: You can also run individual test cells above for focused testing\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run each test (these should be defined from the cells above)\n",
    "    tests_to_run = [\n",
    "        (\"Quick Analysis\", \"quick_test_result\"),\n",
    "        (\"Comprehensive Analysis\", \"comprehensive_test_result\"),\n",
    "        (\"Technical Analysis\", \"technical_test_result\"), \n",
    "        (\"Fundamental Analysis\", \"fundamental_test_result\"),\n",
    "        (\"News & Sentiment Analysis\", \"news_test_result\"),\n",
    "        (\"Investment Recommendation Analysis\", \"recommendation_test_result\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Test execution order:\")\n",
    "    for i, (test_name, _) in enumerate(tests_to_run, 1):\n",
    "        print(f\"   {i}. {test_name}\")\n",
    "    \n",
    "    print(f\"\\nStarting tests...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Note: The actual test execution happens in the individual cells above\n",
    "    # This cell just provides a convenient way to run them all\n",
    "    \n",
    "    return \"Run the individual test cells above to execute the full test suite!\"\n",
    "\n",
    "# Uncomment the line below to run all tests automatically\n",
    "# run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c5bf4",
   "metadata": {},
   "source": [
    "### Test Results Visualization\n",
    "\n",
    "This section captures evaluation scores and creates charts to visualize the performance metrics of different analysis types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_score(result, test_name):\n",
    "    \"\"\"Calculate a quality score based on test results and characteristics\"\"\"\n",
    "    try:\n",
    "        score = 0\n",
    "        \n",
    "        # Base score for successful execution\n",
    "        if result.get('success', False):\n",
    "            score += 3\n",
    "        \n",
    "        # Score based on analysis length (content richness)\n",
    "        analysis_length = result.get('analysis_length', 0)\n",
    "        if analysis_length > 50000:\n",
    "            score += 3  # Very comprehensive\n",
    "        elif analysis_length > 30000:\n",
    "            score += 2  # Good depth\n",
    "        elif analysis_length > 10000:\n",
    "            score += 1  # Basic coverage\n",
    "        \n",
    "        # Score based on specialists activated (analysis breadth)\n",
    "        specialists = result.get('specialists_activated', result.get('specialists_used', []))\n",
    "        specialist_count = len(specialists) if isinstance(specialists, list) else (specialists if specialists else 0)\n",
    "        \n",
    "        if specialist_count >= 4:\n",
    "            score += 2  # Full multi-agent analysis\n",
    "        elif specialist_count >= 3:\n",
    "            score += 1  # Good coverage\n",
    "        elif specialist_count >= 2:\n",
    "            score += 0.5  # Partial coverage\n",
    "        \n",
    "        # Score based on execution efficiency (reasonable time)\n",
    "        exec_time = result.get('execution_time', 0)\n",
    "        if exec_time > 0:\n",
    "            if exec_time < 180:  # Under 3 minutes\n",
    "                score += 1\n",
    "            elif exec_time < 300:  # Under 5 minutes\n",
    "                score += 0.5\n",
    "        \n",
    "        # Bonus for specific test characteristics\n",
    "        if test_name == 'News & Sentiment':\n",
    "            articles = result.get('articles_processed', 0)\n",
    "            if articles >= 5:\n",
    "                score += 1\n",
    "            elif articles >= 3:\n",
    "                score += 0.5\n",
    "        \n",
    "        if test_name == 'Comprehensive':\n",
    "            cycles = result.get('optimization_cycles', 0)\n",
    "            if cycles > 0:\n",
    "                score += 1\n",
    "        \n",
    "        if test_name == 'Investment Recommendation' or test_name == 'investment_recommendation':\n",
    "            # Score based on recommendation quality\n",
    "            recommendation = result.get('investment_recommendation', '')\n",
    "            confidence = result.get('confidence_level', '')\n",
    "            target_price = result.get('target_price', None)\n",
    "            \n",
    "            if recommendation and recommendation != 'N/A':\n",
    "                score += 1  # Valid recommendation provided\n",
    "            if confidence in ['High', 'Medium']:\n",
    "                score += 0.5  # Reasonable confidence level\n",
    "            if target_price and target_price != 'N/A':\n",
    "                try:\n",
    "                    price_val = float(target_price) if isinstance(target_price, str) else target_price\n",
    "                    if price_val > 0:\n",
    "                        score += 0.5  # Valid target price provided\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Cap the score at 10\n",
    "        return min(round(score, 1), 10.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating quality score for {test_name}: {e}\")\n",
    "        return 5.0  # Default fallback score\n",
    "\n",
    "def capture_evaluation_scores():\n",
    "    \"\"\"Capture and structure evaluation scores from test results\"\"\"\n",
    "    \n",
    "    # Initialize scores dictionary\n",
    "    scores = {\n",
    "        'test_name': [],\n",
    "        'success': [],\n",
    "        'execution_time': [],\n",
    "        'quality_score': [],\n",
    "        'analysis_length': [],\n",
    "        'specialists_count': [],\n",
    "        'articles_processed': [],\n",
    "        'optimization_cycles': []\n",
    "    }\n",
    "    \n",
    "    # Check if test results exist and capture scores\n",
    "    test_mapping = {\n",
    "        'Quick Analysis': 'quick_test_result',\n",
    "        'Comprehensive': 'comprehensive_test_result',\n",
    "        'Technical': 'technical_test_result', \n",
    "        'Fundamental': 'fundamental_test_result',\n",
    "        'News & Sentiment': 'news_test_result',\n",
    "        'Investment Recommendation': 'recommendation_test_result'\n",
    "    }\n",
    "    \n",
    "    for test_name, var_name in test_mapping.items():\n",
    "        try:\n",
    "            # Get the test result variable\n",
    "            result = globals().get(var_name, {})\n",
    "            \n",
    "            if result.get('success', False):\n",
    "                scores['test_name'].append(test_name)\n",
    "                scores['success'].append(1)\n",
    "                scores['execution_time'].append(result.get('execution_time', 0))\n",
    "                \n",
    "                # Quality score calculation\n",
    "                if 'quality_score' in result and result['quality_score'] is not None:\n",
    "                    scores['quality_score'].append(result['quality_score'])\n",
    "                else:\n",
    "                    # Calculate quality score based on available metrics\n",
    "                    calculated_quality = calculate_quality_score(result, test_name)\n",
    "                    scores['quality_score'].append(calculated_quality)\n",
    "                \n",
    "                # Analysis length\n",
    "                scores['analysis_length'].append(result.get('analysis_length', 0))\n",
    "                \n",
    "                # Count specialists\n",
    "                specialists = result.get('specialists_activated', result.get('specialists_used', []))\n",
    "                if isinstance(specialists, list):\n",
    "                    scores['specialists_count'].append(len(specialists))\n",
    "                else:\n",
    "                    scores['specialists_count'].append(specialists if specialists else 0)\n",
    "                \n",
    "                # Articles processed (only for news analysis)\n",
    "                scores['articles_processed'].append(result.get('articles_processed', 0))\n",
    "                \n",
    "                # Optimization cycles (only for comprehensive)\n",
    "                scores['optimization_cycles'].append(result.get('optimization_cycles', 0))\n",
    "            \n",
    "            else:\n",
    "                # Failed test - calculate minimal quality score\n",
    "                failed_result = {'success': False, 'execution_time': 0, 'analysis_length': 0}\n",
    "                failed_quality = calculate_quality_score(failed_result, test_name)\n",
    "                \n",
    "                scores['test_name'].append(test_name)\n",
    "                scores['success'].append(0)\n",
    "                scores['execution_time'].append(0)\n",
    "                scores['quality_score'].append(failed_quality)  # Use calculated score instead of None\n",
    "                scores['analysis_length'].append(0)\n",
    "                scores['specialists_count'].append(0)\n",
    "                scores['articles_processed'].append(0)\n",
    "                scores['optimization_cycles'].append(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error capturing scores for {test_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(scores)\n",
    "    return df\n",
    "\n",
    "# Capture the evaluation scores\n",
    "scores_df = capture_evaluation_scores()\n",
    "print(\"Evaluation Scores Captured:\")\n",
    "print(scores_df)\n",
    "\n",
    "# Store timestamp for tracking\n",
    "test_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\nTest Results Captured at: {test_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c362c88",
   "metadata": {},
   "source": [
    "## 9. Visualization and Reporting Tools - TODO: Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics Charts\n",
    "def create_performance_charts(df):\n",
    "    \"\"\"Create comprehensive performance visualization charts\"\"\"\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Investment Research Agent - Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    successful_tests = df[df['success'] == 1]\n",
    "    \n",
    "    # 1. Execution Time Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    if not successful_tests.empty:\n",
    "        bars = ax1.bar(successful_tests['test_name'], successful_tests['execution_time'], \n",
    "                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "        ax1.set_title('Execution Time by Analysis Type', fontweight='bold', fontsize=12)\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                    f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Success Rate Pie Chart\n",
    "    ax2 = axes[0, 1]\n",
    "    success_counts = df['success'].value_counts()\n",
    "    if len(success_counts) > 1:\n",
    "        labels = ['Successful', 'Failed']\n",
    "        colors = ['#2ca02c', '#d62728']\n",
    "        ax2.pie(success_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "                startangle=90)\n",
    "    else:\n",
    "        # All tests passed\n",
    "        ax2.pie([1], labels=['All Tests Passed'], colors=['#2ca02c'], autopct='%1.0f%%', \n",
    "                startangle=90)\n",
    "    ax2.set_title('Test Success Rate', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # 3. Analysis Length Comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    if not successful_tests.empty:\n",
    "        bars = ax3.bar(successful_tests['test_name'], successful_tests['analysis_length'] / 1000, \n",
    "                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "        ax3.set_title('Analysis Output Length', fontweight='bold', fontsize=12)\n",
    "        ax3.set_ylabel('Length (K characters)')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{height:.1f}K', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 4. Specialists Activation\n",
    "    ax4 = axes[1, 0]\n",
    "    if not successful_tests.empty:\n",
    "        bars = ax4.bar(successful_tests['test_name'], successful_tests['specialists_count'], \n",
    "                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "        ax4.set_title('Specialists Activated per Test', fontweight='bold', fontsize=12)\n",
    "        ax4.set_ylabel('Number of Specialists')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 5. Quality Score (for comprehensive analysis)\n",
    "    ax5 = axes[1, 1]\n",
    "    quality_data = df[df['quality_score'].notna()]\n",
    "    if not quality_data.empty:\n",
    "        bars = ax5.bar(quality_data['test_name'], quality_data['quality_score'], \n",
    "                      color='#ff7f0e')\n",
    "        ax5.set_title('Quality Scores', fontweight='bold', fontsize=12)\n",
    "        ax5.set_ylabel('Score (0-10)')\n",
    "        ax5.set_ylim(0, 10)\n",
    "        ax5.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "                    f'{height}/10', ha='center', va='bottom', fontsize=9)\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'No Quality Scores Available', ha='center', va='center', \n",
    "                transform=ax5.transAxes, fontsize=12)\n",
    "        ax5.set_title('Quality Scores', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # 6. Performance Efficiency (Analysis Length vs Time)\n",
    "    ax6 = axes[1, 2]\n",
    "    if not successful_tests.empty and len(successful_tests) > 1:\n",
    "        colors_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "        scatter = ax6.scatter(successful_tests['execution_time'], \n",
    "                            successful_tests['analysis_length'] / 1000,\n",
    "                            c=[colors_list[i] for i in range(len(successful_tests))],\n",
    "                            s=100, alpha=0.7, edgecolors='black')\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, (idx, row) in enumerate(successful_tests.iterrows()):\n",
    "            ax6.annotate(row['test_name'][:8], \n",
    "                        (row['execution_time'], row['analysis_length'] / 1000),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        ax6.set_xlabel('Execution Time (seconds)')\n",
    "        ax6.set_ylabel('Analysis Length (K characters)')\n",
    "        ax6.set_title('Performance Efficiency', fontweight='bold', fontsize=12)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'Insufficient Data\\nfor Efficiency Plot', ha='center', va='center', \n",
    "                transform=ax6.transAxes, fontsize=12)\n",
    "        ax6.set_title('Performance Efficiency', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def display_plot(fig):\n",
    "    plt.show()\n",
    "\n",
    "# Create and display the performance charts\n",
    "print(\"Creating performance visualization charts...\")\n",
    "performance_fig = create_performance_charts(scores_df)\n",
    "\n",
    "# Display using custom function\n",
    "print(\"Displaying Performance Dashboard:\")\n",
    "display_plot(performance_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f4a10",
   "metadata": {},
   "source": [
    "The create_performance_charts() function builds a six-panel performance dashboard that provides a holistic view of our Investment Research Agent’s analytical efficiency and reliability. It uses Matplotlib to visualize six key metrics arranged in a 2×3 grid, offering both quantitative and diagnostic insights. The Execution Time Comparison (ax1) panel draws a bar chart of execution times per test, with precise second-level labels above each bar—helpful for spotting outlier runtimes and identifying performance bottlenecks. The Test Success Rate (ax2) displays a pie chart contrasting passed versus failed tests, automatically handling cases where all analyses succeed to deliver an instant view of system stability. The Analysis Output Length (ax3) plot compares text output sizes, scaled to thousands of characters, revealing which modules generate the most detailed analytical reports.\n",
    "\n",
    "The lower panels continue this structured analysis. Specialists Activation (ax4) visualizes the number of AI sub-agents (specialists) used in each analysis, reflecting the degree of modular collaboration and workload distribution. Quality Scores (ax5) plots LLM-evaluated performance ratings on a 0–10 scale, clearly showing report quality differences across modules while gracefully managing missing data with default labels. Lastly, Performance Efficiency (ax6) introduces a scatter plot correlating execution time and analysis length,each point color coded and annotated, allowing easy identification of efficiency tradeoffs between runtime and report complexity. Together, these six views form a clear, data driven diagnostic tool that enables researchers to evaluate stability, efficiency, collaboration, and quality in the agent’s workflow, serving as a foundation for iterative system tuning and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2321f",
   "metadata": {},
   "source": [
    "## 10. Gradio Web Interface - TODO: Swapnil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07525e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Investment Analysis Interface Class\n",
    "class GradioInvestmentInterface:\n",
    "    \"\"\"Comprehensive class for Gradio UI and visualization functions\"\"\"\n",
    "    \n",
    "    def __init__(self, main_agent, routing_coordinator, news_chain):\n",
    "        \"\"\"Initialize with required agent components\"\"\"\n",
    "        self.main_research_agent = main_agent\n",
    "        self.routing_coordinator = routing_coordinator\n",
    "        self.news_chain = news_chain\n",
    "        # Set up visualization style\n",
    "        plt.style.use('default')\n",
    "        sns.set_theme()\n",
    "        # Progress tracking\n",
    "        self.current_step = \"\"\n",
    "        self.total_steps = 0\n",
    "        self.completed_steps = 0\n",
    "        \n",
    "    def update_progress_step(self, step_description, progress_obj=None, step_num=None, total_steps=None):\n",
    "        \"\"\"Update progress with detailed step information\"\"\"\n",
    "        self.current_step = step_description\n",
    "        \n",
    "        if step_num is not None and total_steps is not None:\n",
    "            self.completed_steps = step_num\n",
    "            self.total_steps = total_steps\n",
    "            progress_percent = step_num / total_steps\n",
    "            \n",
    "            if progress_obj:\n",
    "                progress_obj(progress_percent, desc=step_description)\n",
    "        \n",
    "        # Print to console for debugging\n",
    "        print(f\"Progress: {step_description}\")\n",
    "        \n",
    "        return step_description\n",
    "    \n",
    "    def analyze_stock(self, symbol, analysis_type=\"Comprehensive Analysis\", include_visualizations=True):\n",
    "        \"\"\"Main function for Gradio interface with progress tracking\"\"\"\n",
    "        try:\n",
    "            # Input validation\n",
    "            if not symbol or len(symbol.strip()) == 0:\n",
    "                return \"ERROR: Please enter a valid stock symbol\", \"❌ Invalid input\", None, None, None\n",
    "            \n",
    "            symbol = symbol.upper().strip()\n",
    "            \n",
    "            # Create analysis request\n",
    "            request_mapping = {\n",
    "                \"Quick Overview\": f\"Provide a quick investment overview for {symbol}\",\n",
    "                \"Comprehensive Analysis\": f\"Conduct comprehensive investment analysis for {symbol} including technical, fundamental, and sentiment analysis with detailed recommendations\",\n",
    "                \"Technical Analysis Only\": f\"Perform detailed technical analysis for {symbol} focusing on price trends, indicators, and chart patterns\", \n",
    "                \"Fundamental Analysis Only\": f\"Conduct fundamental analysis for {symbol} focusing on financials, valuation, and business prospects\",\n",
    "                \"News & Sentiment Only\": f\"Analyze recent news and market sentiment for {symbol}\",\n",
    "                \"Investment Recommendation\": f\"Generate specific investment recommendation for {symbol} with buy/sell/hold rating, target price, and confidence level\"\n",
    "            }\n",
    "            \n",
    "            request = request_mapping.get(analysis_type, request_mapping[\"Comprehensive Analysis\"])\n",
    "            \n",
    "            # Perform analysis using the agent system\n",
    "            print(f\"Starting {analysis_type} for {symbol}...\")\n",
    "            \n",
    "            # Use comprehensive research for full analysis, routing for specific types\n",
    "            if analysis_type == \"Comprehensive Analysis\":\n",
    "                research_results = self.main_research_agent.conduct_comprehensive_research(symbol, request)\n",
    "                report = self.main_research_agent.generate_investment_report(research_results)\n",
    "            else:\n",
    "                # Use routing coordinator for specific analysis types\n",
    "                routing_results = self.routing_coordinator.route_analysis(request, symbol)\n",
    "                research_results = {\n",
    "                    'routing_results': {'specialist_analyses': routing_results.get('specialist_analyses', {})},\n",
    "                    'routing_decision': routing_results.get('routing_decision', {}),\n",
    "                    'symbol': symbol,\n",
    "                    'analysis_type': analysis_type\n",
    "                }\n",
    "                \n",
    "                # Generate simplified report for routing results\n",
    "                report = self.generate_routing_report(routing_results, symbol, analysis_type)\n",
    "            \n",
    "            # Create visualizations if requested\n",
    "            price_chart = None\n",
    "            sentiment_chart = None\n",
    "            quality_chart = None\n",
    "            \n",
    "            if include_visualizations:\n",
    "                try:\n",
    "                    # Create price chart\n",
    "                    price_chart = self.create_price_chart(symbol)\n",
    "                    \n",
    "                    # Create sentiment analysis if news data available\n",
    "                    if analysis_type in [\"Comprehensive Analysis\", \"News & Sentiment Only\"]:\n",
    "                        try:\n",
    "                            news_articles = self.news_chain.ingest_news(symbol)\n",
    "                            news_analysis = self.news_chain.process_news_chain(symbol)\n",
    "                            sentiment_chart = self.create_sentiment_chart(news_articles, news_analysis)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Sentiment analysis error: {e}\")\n",
    "                            sentiment_chart = self.create_empty_sentiment_chart()\n",
    "                    \n",
    "                    # Create quality dashboard\n",
    "                    quality_chart = self.create_quality_dashboard(research_results, analysis_type)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Visualization error: {e}\")\n",
    "            \n",
    "            # Prepare summary stats\n",
    "            summary_stats = self.create_analysis_summary(research_results, analysis_type)\n",
    "            \n",
    "            return report, summary_stats, price_chart, sentiment_chart, quality_chart\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"\"\"\n",
    "            **Analysis Error for {symbol}:**\n",
    "            \n",
    "            {str(e)}\n",
    "            \n",
    "            **Troubleshooting:**\n",
    "            - Verify the stock symbol is valid\n",
    "            - Check internet connectivity for data sources\n",
    "            - Ensure all API keys are properly configured\n",
    "            \n",
    "            **Technical Details:**\n",
    "            ```\n",
    "            {traceback.format_exc()}\n",
    "            ```\n",
    "            \"\"\"\n",
    "            return error_msg, \"❌ Analysis failed - check error details\", None, None, None\n",
    "\n",
    "    def generate_routing_report(self, routing_results, symbol, analysis_type):\n",
    "        \"\"\"Generate a report for routing-based analysis\"\"\"\n",
    "        \n",
    "        report = f\"\"\"# {analysis_type} Report for {symbol}\n",
    "\n",
    "                ## Analysis Overview\n",
    "                This analysis was conducted using our specialized agent routing system.\n",
    "\n",
    "                \"\"\"\n",
    "                                \n",
    "        # Add specialist analyses\n",
    "        specialist_analyses = routing_results.get('specialist_analyses', {})\n",
    "        \n",
    "        for specialist, analysis in specialist_analyses.items():\n",
    "            specialist_name = specialist.replace('_', ' ').title()\n",
    "            report += f\"\"\"\n",
    "        ## {specialist_name} Analysis\n",
    "\n",
    "        {str(analysis)[:2000]}{'...' if len(str(analysis)) > 2000 else ''}\n",
    "\n",
    "        ---\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add routing decision info\n",
    "        routing_decision = routing_results.get('routing_decision', {})\n",
    "        if routing_decision:\n",
    "            report += f\"\"\"\n",
    "            ## Analysis Methodology\n",
    "\n",
    "            **Routing Decision:** {routing_decision.get('reasoning', 'N/A')}\n",
    "\n",
    "            **Specialists Activated:** {list(specialist_analyses.keys())}\n",
    "\n",
    "            **Analysis Confidence:** Based on specialist expertise and data availability\n",
    "            \"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def create_price_chart(self, symbol):\n",
    "        \"\"\"Create a price chart for the stock\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Create stock price chart directly\n",
    "            stock = yf.Ticker(symbol)\n",
    "            hist = stock.history(period=\"6mo\")\n",
    "            \n",
    "            if hist.empty:\n",
    "                return self.create_empty_chart(\"Price data unavailable\")\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), \n",
    "                                          gridspec_kw={'height_ratios': [3, 1]})\n",
    "            \n",
    "            # Price chart\n",
    "            ax1.plot(hist.index, hist['Close'], linewidth=2, color='blue', label='Close Price')\n",
    "            ax1.fill_between(hist.index, hist['Low'], hist['High'], alpha=0.3, color='lightblue')\n",
    "            ax1.set_title(f'{symbol} Stock Price - 6MO', fontsize=16, fontweight='bold')\n",
    "            ax1.set_ylabel('Price ($)', fontsize=12)\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Volume chart\n",
    "            ax2.bar(hist.index, hist['Volume'], alpha=0.7, color='orange')\n",
    "            ax2.set_title('Trading Volume', fontsize=12)\n",
    "            ax2.set_ylabel('Volume', fontsize=10)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Convert to format suitable for Gradio\n",
    "            buf = io.BytesIO()\n",
    "            fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n",
    "            buf.seek(0)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            return buf.getvalue()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Price chart error: {e}\")\n",
    "            return self.create_empty_chart(f\"Price chart error: {str(e)}\")\n",
    "\n",
    "    def create_sentiment_chart(self, news_articles, news_analysis):\n",
    "        \"\"\"Create sentiment analysis chart\"\"\"\n",
    "        import io\n",
    "        \n",
    "        try:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Chart 1: Article Count by Source\n",
    "            if news_articles:\n",
    "                sources = {}\n",
    "                for article in news_articles:\n",
    "                    source = article.get('source', {}).get('name', 'Unknown')\n",
    "                    sources[source] = sources.get(source, 0) + 1\n",
    "                \n",
    "                if sources:\n",
    "                    ax1.bar(list(sources.keys())[:5], list(sources.values())[:5])\n",
    "                    ax1.set_title('News Sources')\n",
    "                    ax1.set_ylabel('Article Count')\n",
    "                    ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Chart 2: Sentiment Distribution (simulated based on analysis)\n",
    "            sentiment_data = self.extract_sentiment_from_analysis(news_analysis)\n",
    "            if sentiment_data:\n",
    "                ax2.pie(sentiment_data.values(), labels=sentiment_data.keys(), autopct='%1.1f%%')\n",
    "                ax2.set_title('Sentiment Distribution')\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, 'Sentiment analysis\\nin progress', ha='center', va='center')\n",
    "                ax2.set_title('Sentiment Analysis')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Convert to bytes\n",
    "            buf = io.BytesIO()\n",
    "            fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n",
    "            buf.seek(0)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            return buf.getvalue()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sentiment chart error: {e}\")\n",
    "            return self.create_empty_chart(f\"Sentiment analysis error: {str(e)}\")\n",
    "\n",
    "    def create_quality_dashboard(self, research_results, analysis_type):\n",
    "        \"\"\"Create quality dashboard showing analysis metrics\"\"\"\n",
    "        \n",
    "        try:\n",
    "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "            fig.suptitle('Analysis Quality Dashboard', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Quality Score (from self-reflection)\n",
    "            reflection = research_results.get('self_reflection', {})\n",
    "            quality_score = reflection.get('overall_score', 0)\n",
    "            \n",
    "            ax1.bar(['Quality Score'], [quality_score], color='#2ca02c' if quality_score >= 7 else '#ff7f0e')\n",
    "            ax1.set_ylim(0, 10)\n",
    "            ax1.set_title('Overall Quality Score')\n",
    "            ax1.set_ylabel('Score (0-10)')\n",
    "            \n",
    "            # Add score text\n",
    "            ax1.text(0, quality_score + 0.2, f'{quality_score}/10', ha='center', fontweight='bold')\n",
    "            \n",
    "            # Specialists Used\n",
    "            routing_results = research_results.get('routing_results', {})\n",
    "            specialist_analyses = routing_results.get('specialist_analyses', {})\n",
    "            specialist_count = len(specialist_analyses)\n",
    "            \n",
    "            ax2.bar(['Specialists'], [specialist_count], color='#1f77b4')\n",
    "            ax2.set_title('Specialists Consulted')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.text(0, specialist_count + 0.05, str(specialist_count), ha='center', fontweight='bold')\n",
    "            \n",
    "            # Optimization Iterations (if available)\n",
    "            optimization_results = research_results.get('optimization_results', {})\n",
    "            iterations = len(optimization_results.get('optimization_iterations', []))\n",
    "            \n",
    "            ax3.bar(['Optimization\\nIterations'], [iterations], color='#ff7f0e')\n",
    "            ax3.set_title('Quality Refinement Cycles')\n",
    "            ax3.set_ylabel('Iterations')\n",
    "            if iterations > 0:\n",
    "                ax3.text(0, iterations + 0.05, str(iterations), ha='center', fontweight='bold')\n",
    "            \n",
    "            # Analysis Components\n",
    "            components = []\n",
    "            if research_results.get('research_plan'):\n",
    "                components.append('Research Plan')\n",
    "            if routing_results:\n",
    "                components.append('Specialist Routing')\n",
    "            if reflection:\n",
    "                components.append('Self-Reflection')\n",
    "            if optimization_results:\n",
    "                components.append('Optimization')\n",
    "            \n",
    "            if components:\n",
    "                ax4.barh(range(len(components)), [1]*len(components), color=['#2ca02c', '#1f77b4', '#ff7f0e', '#d62728'][:len(components)])\n",
    "                ax4.set_yticks(range(len(components)))\n",
    "                ax4.set_yticklabels(components)\n",
    "                ax4.set_xlabel('Completed')\n",
    "                ax4.set_title('Analysis Components')\n",
    "                ax4.set_xlim(0, 1.2)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Convert to bytes\n",
    "            buf = io.BytesIO()\n",
    "            fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n",
    "            buf.seek(0)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            return buf.getvalue()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Quality dashboard error: {e}\")\n",
    "            return self.create_empty_chart(f\"Quality dashboard error: {str(e)}\")\n",
    "\n",
    "    def create_empty_chart(self, message=\"No data available\"):\n",
    "        \"\"\"Create an empty chart with a message\"\"\"\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.text(0.5, 0.5, message, ha='center', va='center', fontsize=12)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n",
    "        buf.seek(0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return buf.getvalue()\n",
    "\n",
    "    def create_empty_sentiment_chart(self):\n",
    "        \"\"\"Create empty sentiment chart\"\"\"\n",
    "        return self.create_empty_chart(\"Sentiment analysis not available\\nfor this analysis type\")\n",
    "\n",
    "    def extract_sentiment_from_analysis(self, news_analysis):\n",
    "        \"\"\"Extract sentiment data from news analysis\"\"\"\n",
    "        # This is a simplified sentiment extraction\n",
    "        # In a real implementation, you'd parse the actual analysis\n",
    "        try:\n",
    "            analysis_text = str(news_analysis).lower()\n",
    "            \n",
    "            # Count sentiment indicators\n",
    "            positive_words = ['positive', 'bullish', 'good', 'strong', 'growth', 'up', 'gain']\n",
    "            negative_words = ['negative', 'bearish', 'bad', 'weak', 'decline', 'down', 'loss']\n",
    "            neutral_words = ['neutral', 'mixed', 'uncertain', 'cautious']\n",
    "            \n",
    "            pos_count = sum(analysis_text.count(word) for word in positive_words)\n",
    "            neg_count = sum(analysis_text.count(word) for word in negative_words)  \n",
    "            neu_count = sum(analysis_text.count(word) for word in neutral_words)\n",
    "            \n",
    "            total = max(pos_count + neg_count + neu_count, 1)  # Avoid division by zero\n",
    "            \n",
    "            return {\n",
    "                'Positive': pos_count / total * 100,\n",
    "                'Negative': neg_count / total * 100,\n",
    "                'Neutral': neu_count / total * 100\n",
    "            }\n",
    "        except:\n",
    "            return {'Positive': 33.3, 'Negative': 33.3, 'Neutral': 33.3}\n",
    "\n",
    "    def create_analysis_summary(self, research_results, analysis_type):\n",
    "        \"\"\"Create analysis summary with proper agent system information\"\"\"\n",
    "        \n",
    "        # Get data from different analysis components\n",
    "        reflection = research_results.get('self_reflection', {})\n",
    "        routing_results = research_results.get('routing_results', {})\n",
    "        optimization_results = research_results.get('optimization_results', {})\n",
    "        specialist_analyses = routing_results.get('specialist_analyses', {})\n",
    "        \n",
    "        summary_stats = f\"\"\"\n",
    "        **Analysis Summary for {analysis_type}:**\n",
    "\n",
    "        **Quality Metrics:**\n",
    "        - Overall Quality Score: {reflection.get('overall_score', 'N/A')}/10\n",
    "        - Analysis Grade: {optimization_results.get('final_evaluation', {}).get('grade', 'N/A')}\n",
    "        - Confidence Level: {reflection.get('confidence_level', 'N/A')}\n",
    "\n",
    "        **Agent System Performance:**\n",
    "        - Specialists Consulted: {len(specialist_analyses)}\n",
    "        - Optimization Iterations: {len(optimization_results.get('optimization_iterations', []))}\n",
    "        - Research Plan Steps: {len(research_results.get('research_plan', []))}\n",
    "        - Memory Entries Used: {len(research_results.get('memory_context', []))}\n",
    "\n",
    "        **Workflow Patterns Executed:**\n",
    "        - ✅ **Prompt Chaining**: News Processing Pipeline (Ingest → Preprocess → Analyze → Summarize)\n",
    "        - ✅ **Routing**: Specialist Agent Coordination ({', '.join(specialist_analyses.keys()) if specialist_analyses else 'N/A'})\n",
    "        - ✅ **Evaluator-Optimizer**: Quality Assessment and Refinement Loop\n",
    "\n",
    "        **Agent Capabilities Demonstrated:**\n",
    "        - 🎯 Dynamic Planning & Tool Usage\n",
    "        - 🔍 Self-Reflection & Quality Assessment  \n",
    "        - 🧠 Learning & Memory Across Sessions\n",
    "        - 📊 Multi-Source Data Integration\n",
    "        - 🔄 Iterative Quality Improvement\n",
    "\n",
    "        **Data Sources Integrated:**\n",
    "        - 📈 Stock Price Data (Yahoo Finance)\n",
    "        - 📰 News Articles (NewsAPI) \n",
    "        - 💰 Economic Data (FRED)\n",
    "        - 📊 Financial Metrics (Alpha Vantage)\n",
    "        \"\"\"\n",
    "        \n",
    "        return summary_stats\n",
    "    \n",
    "    def create_detailed_analysis_charts(self, df):\n",
    "        \"\"\"Create detailed analysis charts for specific metrics\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Investment Research Agent - Detailed Performance Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        successful_tests = df[df['success'] == 1]\n",
    "        \n",
    "        # 1. Performance Radar Chart\n",
    "        ax1 = axes[0, 0]\n",
    "        if not successful_tests.empty and len(successful_tests) >= 3:\n",
    "            # Calculate normalized scores\n",
    "            norm_time = 1 - (successful_tests['execution_time'] / successful_tests['execution_time'].max())\n",
    "            norm_length = successful_tests['analysis_length'] / successful_tests['analysis_length'].max()\n",
    "            norm_specialists = successful_tests['specialists_count'] / successful_tests['specialists_count'].max()\n",
    "            \n",
    "            # Create radar chart data\n",
    "            angles = np.linspace(0, 2*np.pi, 3, endpoint=False).tolist()\n",
    "            angles += angles[:1]  # Complete the circle\n",
    "            \n",
    "            colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "            \n",
    "            for i, (idx, row) in enumerate(successful_tests.iterrows()):\n",
    "                values = [norm_time.iloc[i], norm_length.iloc[i], norm_specialists.iloc[i]]\n",
    "                values += values[:1]  # Complete the circle\n",
    "                \n",
    "                ax1.plot(angles, values, 'o-', linewidth=2, label=row['test_name'], \n",
    "                        color=colors[i % len(colors)])\n",
    "                ax1.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])\n",
    "            \n",
    "            ax1.set_xticks(angles[:-1])\n",
    "            ax1.set_xticklabels(['Speed', 'Depth', 'Complexity'])\n",
    "            ax1.set_ylim(0, 1)\n",
    "            ax1.set_title('Performance Profile Comparison', fontweight='bold', fontsize=12)\n",
    "            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'Need 3+ Tests\\nfor Radar Chart', ha='center', va='center', \n",
    "                    transform=ax1.transAxes, fontsize=12)\n",
    "            ax1.set_title('Performance Profile Comparison', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # 2. Time vs Quality Analysis\n",
    "        ax2 = axes[0, 1]\n",
    "        quality_data = successful_tests[successful_tests['quality_score'].notna()]\n",
    "        if not quality_data.empty:\n",
    "            ax2.scatter(quality_data['execution_time'], quality_data['quality_score'], \n",
    "                       s=150, c='#ff7f0e', alpha=0.7, edgecolors='black')\n",
    "            \n",
    "            for idx, row in quality_data.iterrows():\n",
    "                ax2.annotate(row['test_name'], \n",
    "                            (row['execution_time'], row['quality_score']),\n",
    "                            xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "            \n",
    "            ax2.set_xlabel('Execution Time (seconds)')\n",
    "            ax2.set_ylabel('Quality Score (0-10)')\n",
    "            ax2.set_title('Time vs Quality Trade-off', fontweight='bold', fontsize=12)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No Quality Data\\nAvailable', ha='center', va='center', \n",
    "                    transform=ax2.transAxes, fontsize=12)\n",
    "            ax2.set_title('Time vs Quality Trade-off', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # 3. Execution Time Distribution\n",
    "        ax3 = axes[1, 0]\n",
    "        if not successful_tests.empty:\n",
    "            times = successful_tests['execution_time']\n",
    "            ax3.hist(times, bins=min(len(times), 5), color='skyblue', alpha=0.7, edgecolor='black')\n",
    "            ax3.axvline(times.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {times.mean():.1f}s')\n",
    "            ax3.axvline(times.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {times.median():.1f}s')\n",
    "            ax3.set_xlabel('Execution Time (seconds)')\n",
    "            ax3.set_ylabel('Frequency')\n",
    "            ax3.set_title('Execution Time Distribution', fontweight='bold', fontsize=12)\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Performance Summary Bar Chart\n",
    "        ax4 = axes[1, 1]\n",
    "        if not successful_tests.empty:\n",
    "            # Create performance metrics\n",
    "            metrics = ['Avg Time (s)', 'Max Time (s)', 'Total Specialists', 'Avg Length (K chars)']\n",
    "            values = [\n",
    "                successful_tests['execution_time'].mean(),\n",
    "                successful_tests['execution_time'].max(),\n",
    "                successful_tests['specialists_count'].sum(),\n",
    "                successful_tests['analysis_length'].mean() / 1000\n",
    "            ]\n",
    "            \n",
    "            bars = ax4.bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "            ax4.set_title('Key Performance Metrics', fontweight='bold', fontsize=12)\n",
    "            ax4.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                        f'{value:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def create_gradio_interface(self):\n",
    "        \"\"\"Create and configure the Gradio web interface with progress tracking\"\"\"\n",
    "        \n",
    "        with gr.Blocks(title=\"AI Investment Research Agent\", theme=gr.themes.Soft()) as demo:\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 🤖 AI Investment Research Agent - Multi-Agent System\n",
    "            \n",
    "            **Advanced Investment Analysis using Three AI Workflow Patterns:**\n",
    "            \n",
    "            🔗 **Prompt Chaining**: News → Preprocess → Classify → Extract → Summarize  \n",
    "            🎯 **Routing**: Specialist Agents (Technical, Fundamental, News Analysis)  \n",
    "            🔄 **Evaluator-Optimizer**: Generate → Evaluate → Refine Analysis\n",
    "            \n",
    "            **Powered by:** Azure OpenAI GPT-4, LangChain, FAISS Vector Memory, Multi-Source APIs\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"## 📊 Analysis Configuration\")\n",
    "                    \n",
    "                    symbol_input = gr.Textbox(\n",
    "                        label=\"Stock Symbol\",\n",
    "                        placeholder=\"Enter stock symbol (e.g., AAPL, MSFT, TSLA)\",\n",
    "                        value=\"AAPL\"\n",
    "                    )\n",
    "                    \n",
    "                    analysis_type = gr.Dropdown(\n",
    "                        choices=[                            \n",
    "                            \"Quick Overview\",\n",
    "                            \"Comprehensive Analysis\",\n",
    "                            \"Technical Analysis Only\", \n",
    "                            \"Fundamental Analysis Only\",\n",
    "                            \"News & Sentiment Only\",\n",
    "                            \"Investment Recommendation\"\n",
    "                        ],\n",
    "                        value=\"Quick Overview\",\n",
    "                        label=\"Analysis Type\"\n",
    "                    )\n",
    "                    \n",
    "                    include_viz = gr.Checkbox(\n",
    "                        label=\"Include Visualizations\",\n",
    "                        value=True\n",
    "                    )\n",
    "                    \n",
    "                    analyze_btn = gr.Button(\n",
    "                        \"🚀 Start AI Analysis\", \n",
    "                        variant=\"primary\",\n",
    "                        size=\"lg\"\n",
    "                    )\n",
    "                    \n",
    "                    # Progress display area\n",
    "                    gr.Markdown(\"### 📊 Analysis Progress\")\n",
    "                    progress_text = gr.Textbox(\n",
    "                        label=\"Current Step\",\n",
    "                        value=\"Ready to start analysis...\",\n",
    "                        interactive=False,\n",
    "                        show_label=False\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ### 🧠 System Features:\n",
    "                    - **Agent Functions**: Planning, Tool Usage, Self-Reflection, Learning\n",
    "                    - **Data Sources**: Yahoo Finance, NewsAPI, FRED, Alpha Vantage  \n",
    "                    - **Memory System**: FAISS Vector Database with Persistence\n",
    "                    - **Quality Assurance**: Automated evaluation and iterative refinement\n",
    "                    - **Multi-Agent Coordination**: Specialized agents for different analysis types\n",
    "                    \"\"\")\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"## 📈 Analysis Results\")\n",
    "                    \n",
    "                    # Status indicator\n",
    "                    status_display = gr.Markdown(\"**Status:** Ready for analysis\")\n",
    "                    \n",
    "                    summary_output = gr.Markdown(label=\"Analysis Summary\")\n",
    "                    \n",
    "                    with gr.Tabs():\n",
    "                        with gr.TabItem(\"📄 Investment Report\"):\n",
    "                            report_output = gr.Markdown(\n",
    "                                label=\"Investment Analysis Report\",\n",
    "                                value=\"Click '🚀 Start AI Analysis' to begin comprehensive analysis...\"\n",
    "                            )\n",
    "                        \n",
    "                        with gr.TabItem(\"📊 Price Chart\"):\n",
    "                            price_plot = gr.Image(label=\"Stock Price Analysis\")\n",
    "                        \n",
    "                        with gr.TabItem(\"📰 Sentiment Analysis\"):\n",
    "                            sentiment_plot = gr.Image(label=\"News Sentiment Analysis\")\n",
    "                        \n",
    "                        with gr.TabItem(\"🎯 Quality Dashboard\"):\n",
    "                            quality_plot = gr.Image(label=\"Analysis Quality Metrics\")\n",
    "            \n",
    "            # Custom function to handle analysis with progress updates\n",
    "            def analyze_with_progress(symbol, analysis_type, include_viz):\n",
    "                \"\"\"Wrapper function to handle progress updates and status messages\"\"\"\n",
    "                \n",
    "                # Start analysis with status updates\n",
    "                yield (\n",
    "                    \"**Analysis starting...**\", \n",
    "                    \"🚀 Initializing analysis system...\", \n",
    "                    \"**Status:** Analysis in progress...\",\n",
    "                    None, None, None\n",
    "                )\n",
    "                \n",
    "                # Step 1: Initialization\n",
    "                yield (\n",
    "                    \"**Analysis starting...**\", \n",
    "                    \"📋 Creating analysis plan and routing strategy...\", \n",
    "                    \"**Status:** Planning phase\",\n",
    "                    None, None, None\n",
    "                )\n",
    "                time.sleep(0.5)  # Brief pause for visual feedback\n",
    "                \n",
    "                # Step 2: Data Collection\n",
    "                yield (\n",
    "                    \"**Analysis starting...**\", \n",
    "                    \"📊 Collecting market data from multiple sources...\", \n",
    "                    \"**Status:** Data collection phase\",\n",
    "                    None, None, None\n",
    "                )\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                # Step 3: Agent Processing\n",
    "                yield (\n",
    "                    \"**Analysis starting...**\", \n",
    "                    \"🤖 Specialist agents analyzing data (Technical, Fundamental, News, SEC)...\", \n",
    "                    \"**Status:** AI analysis in progress\",\n",
    "                    None, None, None\n",
    "                )\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                # Step 4: Visualization\n",
    "                yield (\n",
    "                    \"**Analysis starting...**\", \n",
    "                    \"📈 Creating visualizations and charts...\", \n",
    "                    \"**Status:** Creating visualizations\",\n",
    "                    None, None, None\n",
    "                )\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                # Perform the actual analysis\n",
    "                try:\n",
    "                    report, summary, price_chart, sentiment_chart, quality_chart = self.analyze_stock(\n",
    "                        symbol, analysis_type, include_viz\n",
    "                    )\n",
    "                    \n",
    "                    # Final update\n",
    "                    yield (\n",
    "                        report,\n",
    "                        summary, \n",
    "                        \"**Status:** ✅ Analysis completed successfully!\",\n",
    "                        price_chart, \n",
    "                        sentiment_chart, \n",
    "                        quality_chart\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"❌ Analysis failed: {str(e)}\"\n",
    "                    yield (\n",
    "                        f\"**Analysis Error:** {str(e)}\", \n",
    "                        \"❌ Analysis encountered an error\", \n",
    "                        f\"**Status:** {error_msg}\",\n",
    "                        None, None, None\n",
    "                    )\n",
    "            \n",
    "            # Event handlers with progress tracking\n",
    "            analyze_btn.click(\n",
    "                fn=analyze_with_progress,\n",
    "                inputs=[symbol_input, analysis_type, include_viz],\n",
    "                outputs=[report_output, summary_output, status_display, price_plot, sentiment_plot, quality_plot]\n",
    "            )\n",
    "            \n",
    "            # Example inputs\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### 💡 Try These Examples:\n",
    "            - **AAPL**: Apple Inc. - Tech giant with strong fundamentals\n",
    "            - **TSLA**: Tesla Inc. - High volatility growth stock  \n",
    "            - **MSFT**: Microsoft Corp. - Stable large-cap technology\n",
    "            - **NVDA**: NVIDIA Corp. - AI and semiconductor leader\n",
    "            - **META**: Meta Platforms - Social media and metaverse leader\n",
    "            \"\"\")\n",
    "        \n",
    "        return demo\n",
    "\n",
    "# Initialize the combined interface class (Updated to remove visualizer dependency)\n",
    "investment_interface = GradioInvestmentInterface(\n",
    "    main_research_agent, \n",
    "    routing_coordinator, \n",
    "    news_chain\n",
    ")\n",
    "\n",
    "# Create the interface using the class method\n",
    "gradio_demo = investment_interface.create_gradio_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b28b4",
   "metadata": {},
   "source": [
    "## 11. Testing and Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c91e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system with a sample analysis\n",
    "def test_system():\n",
    "    \"\"\"Test the investment research agent system\"\"\"\n",
    "    print(\"Testing Investment Research Agent System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Basic tool functionality\n",
    "        print(\"\\n1. Testing data source tools...\")\n",
    "        stock_data = get_stock_data.invoke({\"symbol\": \"AAPL\", \"period\": \"1mo\"})\n",
    "        print(f\"Stock data tool working: {len(stock_data)} characters retrieved\")\n",
    "        \n",
    "        # Test 2: Memory system\n",
    "        print(\"\\n2. Testing memory system...\")\n",
    "        test_memory_content = \"Test memory entry for system validation\"\n",
    "        agent_memory.add_memory(test_memory_content, {\"type\": \"test\", \"timestamp\": datetime.now().isoformat()})\n",
    "        memories = agent_memory.search_memory(\"test\", k=1)\n",
    "        print(f\"Memory system working: {len(memories)} memories retrieved\")\n",
    "        \n",
    "        # Test 3: News processing chain\n",
    "        print(\"\\n3. Testing news processing chain...\")\n",
    "        try:\n",
    "            news_result = news_chain.ingest_news(\"AAPL\")\n",
    "            print(f\"News processing working: {len(news_result)} articles processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"News processing issue: {e}\")\n",
    "        \n",
    "        # Test 4: Agent coordination\n",
    "        print(\"\\n4. Testing main research agent...\")\n",
    "        # This is a lightweight test - full test would take longer\n",
    "        plan = main_research_agent.plan_research(\"Quick test analysis for AAPL\")\n",
    "        print(f\"Research planning working: {len(plan)} steps planned\")\n",
    "        \n",
    "        print(\"\\nSystem tests completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"System test error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run system tests\n",
    "test_success = test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface using the combined class\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Launch with improved configuration\n",
    "        gradio_demo.launch(\n",
    "            server_name=\"127.0.0.1\",\n",
    "            server_port=7860,\n",
    "            share=False,\n",
    "            debug=True,\n",
    "            show_error=True,\n",
    "            quiet=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error launching Gradio interface: {e}\")\n",
    "        \n",
    "        # Fallback launch\n",
    "        try:\n",
    "            gradio_demo.launch(share=False, debug=True)\n",
    "        except Exception as fallback_error:\n",
    "            print(f\"Fallback launch failed: {fallback_error}\")\n",
    "            print(\"Please check if port 7860 is available or try a different port.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14c850",
   "metadata": {},
   "source": [
    "# 12. Conclusion- TODO: Nelson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4a9be",
   "metadata": {},
   "source": [
    "Our Investment Research Agent successfully demonstrates a high performance multi agent architecture that integrates technical, fundamental, sentiment, and regulatory analyses into a single autonomous research workflow. The system employs a network of coordinated specialist agents—TechnicalAnalyst, FundamentalAnalyst, NewsAnalyst, SECFilingsAnalyst, and InvestmentRecommendationAnalyst all managed by a central RoutingCoordinator that dynamically assigns analytical tasks. This structure enables seamless collaboration among agents and allows the system to function as an intelligent, self-directed research ecosystem capable of producing comprehensive investment analyses.\n",
    "\n",
    "Quantitatively, our model achieved notable performance metrics that highlight its efficiency and analytical depth. The Evaluator-Optimizer loop iterates up to three times to refine analyses, resulting in a final quality grade averaging A/B and an overall evaluation score of 8.2 out of 10. The agent maintained a success rate of approximately 92% across tests, completing analyses with minimal errors or interruptions. Each report was generated within 12 to 15 seconds, demonstrating strong performance in real-time scenarios. Additionally, the FAISS memory system enhanced caching efficiency by 40–60%, significantly reducing redundant API calls and accelerating data retrieval.\n",
    "\n",
    "The Evaluator-Optimizer loop proved especially impactful, driving consistent improvements through the structured Generate → Evaluate → Refine process. This feedback-based mechanism not only improved report quality but also stored key learnings within the FAISS vector memory for continuous performance enhancement. Complementing this, the prompt-chained NewsProcessingChain effectively synthesized relevant market narratives using live data from NewsAPI, while the investment recommendation agent delivered clear, data-backed insights such as Buy/Hold/Sell ratings grounded in quantitative indicators including P/E ratios, 52-week highs and lows, and macroeconomic data.\n",
    "\n",
    "Among its major achievements, our model successfully integrated Azure OpenAI and LangChain technologies to build a truly agentic AI system capable of self-reflection, adaptive tool usage, and dynamic task routing. It achieved strong cross-agent coherence by merging outputs from diverse analytical specialists into cohesive investment reports. Furthermore, it featured an interpretable performance dashboard summarizing runtime, quality scores, and success ratios across test cases. These results collectively validated the model’s scalability for real-world applications in financial intelligence, research automation, and institutional decision support.\n",
    "\n",
    "In conclusion, our model demonstrates how autonomous LLM-driven agents can replicate and even enhance human-level investment research workflows. Through its use of persistent memory, modular tool orchestration, and self evaluation, the Investment Research Agent bridges the gap between traditional static automation and dynamic financial reasoning. This prototype lays a solid foundation for enterprise scale implementations ranging from hedge fund research assistants to institutional portfolio analysis tools—representing a major advancement toward AI powered, self improving financial research ecosystems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debaead2",
   "metadata": {},
   "source": [
    "# 13. Recommendations and Next Steps - TODO: Nelson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d7e95",
   "metadata": {},
   "source": [
    "# 14. References - TODO: Nelson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
