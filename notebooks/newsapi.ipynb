{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a443d5",
   "metadata": {},
   "source": [
    "# NewsAPI Financial News Analysis\n",
    "\n",
    "This notebook demonstrates how to integrate with NewsAPI to retrieve and analyze financial news articles for investment research using LangChain.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Service**: NewsAPI (https://newsapi.org/)\n",
    "- **Purpose**: Retrieve and analyze financial news for investment research\n",
    "- **Rate Limits**: 1,000 requests/month (free tier)\n",
    "- **Documentation**: https://newsapi.org/docs\n",
    "- **Integration**: LangChain for data processing and analysis\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Register for a free NewsAPI account\n",
    "2. Obtain your API key\n",
    "3. Install required dependencies\n",
    "4. Set up environment variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34163dc6",
   "metadata": {},
   "source": [
    "# Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b310d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: langchain in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-text-splitters in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (0.3.11)\n",
      "Requirement already satisfied: python-dotenv in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (0.4.28)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: anyio in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\personal\\ai-admissions\\semester4\\aai-520-natural language processing and genai\\final team project\\aai520_8proj\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests pandas langchain langchain-text-splitters python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60542b4a",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all necessary libraries for NewsAPI integration and LangChain processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c9f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Analysis timestamp: 2025-09-20 22:57:16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a2ff1",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n",
    "\n",
    "Configure NewsAPI credentials and connection parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9531ba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsAPI Configuration:\n",
      "Base URL: https://newsapi.org/v2\n",
      "API Key configured: Yes\n",
      "Test ticker: AAPL\n",
      "Date range: 2025-09-13 to 2025-09-20\n"
     ]
    }
   ],
   "source": [
    "# NewsAPI Configuration\n",
    "NEWSAPI_KEY = os.getenv('NEWSAPI_KEY', 'your_newsapi_key_here')\n",
    "NEWSAPI_BASE_URL = \"https://newsapi.org/v2\"\n",
    "\n",
    "# Test ticker for demonstration\n",
    "TEST_TICKER = \"AAPL\"  # Apple Inc.\n",
    "\n",
    "# Date range for news search (last 7 days)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=7)\n",
    "\n",
    "print(f\"NewsAPI Configuration:\")\n",
    "print(f\"Base URL: {NEWSAPI_BASE_URL}\")\n",
    "print(f\"API Key configured: {'Yes' if NEWSAPI_KEY != 'your_newsapi_key_here' else 'No - Please set NEWSAPI_KEY environment variable'}\")\n",
    "print(f\"Test ticker: {TEST_TICKER}\")\n",
    "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b12c4",
   "metadata": {},
   "source": [
    "## NewsAPI Client Class\n",
    "\n",
    "Create a professional NewsAPI client class with error handling and data processing capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9a76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsAPI client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "class NewsAPIClient:\n",
    "    \"\"\"Professional NewsAPI client for financial news retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = NEWSAPI_BASE_URL\n",
    "        \n",
    "    def get_news_for_ticker(self, ticker: str, from_date: str, to_date: str, \n",
    "                           page_size: int = 20) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve news articles for a specific stock ticker.\n",
    "        \n",
    "        Args:\n",
    "            ticker: Stock ticker symbol (e.g., 'AAPL')\n",
    "            from_date: Start date in YYYY-MM-DD format\n",
    "            to_date: End date in YYYY-MM-DD format\n",
    "            page_size: Number of articles to retrieve (max 100)\n",
    "            \n",
    "        Returns:\n",
    "            List of news articles with metadata\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/everything\"\n",
    "        \n",
    "        params = {\n",
    "            'q': f'{ticker} OR \"{ticker}\" stock shares earnings revenue profit',\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'sortBy': 'relevancy',\n",
    "            'language': 'en',\n",
    "            'pageSize': min(page_size, 100),\n",
    "            'apiKey': self.api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if data['status'] == 'ok':\n",
    "                articles = data.get('articles', [])\n",
    "                print(f\"Retrieved {len(articles)} articles for {ticker}\")\n",
    "                return articles\n",
    "            else:\n",
    "                print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to decode response: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def format_articles_for_analysis(self, articles: List[Dict[str, Any]]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Format news articles for LangChain processing.\n",
    "        \n",
    "        Args:\n",
    "            articles: List of news articles from NewsAPI\n",
    "            \n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for article in articles:\n",
    "            # Combine title, description, and content for comprehensive analysis\n",
    "            content_parts = []\n",
    "            \n",
    "            if article.get('title'):\n",
    "                content_parts.append(f\"Title: {article['title']}\")\n",
    "            \n",
    "            if article.get('description'):\n",
    "                content_parts.append(f\"Description: {article['description']}\")\n",
    "            \n",
    "            if article.get('content'):\n",
    "                # Remove source attribution that often appears at the end\n",
    "                content = article['content']\n",
    "                if '[+' in content:\n",
    "                    content = content.split('[+')[0].strip()\n",
    "                content_parts.append(f\"Content: {content}\")\n",
    "            \n",
    "            full_content = \"\\n\\n\".join(content_parts)\n",
    "            \n",
    "            # Create metadata for the document\n",
    "            metadata = {\n",
    "                'source': article.get('source', {}).get('name', 'Unknown'),\n",
    "                'author': article.get('author', 'Unknown'),\n",
    "                'published_at': article.get('publishedAt', ''),\n",
    "                'url': article.get('url', ''),\n",
    "                'ticker': article.get('ticker', 'Unknown')\n",
    "            }\n",
    "            \n",
    "            documents.append(Document(page_content=full_content, metadata=metadata))\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# Initialize the NewsAPI client\n",
    "news_client = NewsAPIClient(NEWSAPI_KEY)\n",
    "print(\"NewsAPI client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796e3da",
   "metadata": {},
   "source": [
    "## Retrieve Financial News Data\n",
    "\n",
    "Now let's retrieve news articles for our test ticker and process them using LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9cd32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving news articles for AAPL...\n",
      "Retrieved 10 articles for AAPL\n",
      "Successfully retrieved 10 articles\n",
      "Article overview:\n",
      "1. This Little-Known AI Stock Is Up 70% in 2025 and Analysts Think It Can Rally Further From Here\n",
      "   Source: Barchart.com\n",
      "   Published: 2025-09-15T15:46:32Z\n",
      "\n",
      "2. Top Stock Movers Now: Apple, FedEx, Lennar, and More\n",
      "   Source: Investopedia\n",
      "   Published: 2025-09-19T17:02:52Z\n",
      "\n",
      "3. How Micron Stock Surges 2x To $300\n",
      "   Source: Forbes\n",
      "   Published: 2025-09-18T09:00:27Z\n",
      "\n",
      "... and 7 more articles\n",
      "Retrieved 10 articles for AAPL\n",
      "Successfully retrieved 10 articles\n",
      "Article overview:\n",
      "1. This Little-Known AI Stock Is Up 70% in 2025 and Analysts Think It Can Rally Further From Here\n",
      "   Source: Barchart.com\n",
      "   Published: 2025-09-15T15:46:32Z\n",
      "\n",
      "2. Top Stock Movers Now: Apple, FedEx, Lennar, and More\n",
      "   Source: Investopedia\n",
      "   Published: 2025-09-19T17:02:52Z\n",
      "\n",
      "3. How Micron Stock Surges 2x To $300\n",
      "   Source: Forbes\n",
      "   Published: 2025-09-18T09:00:27Z\n",
      "\n",
      "... and 7 more articles\n"
     ]
    }
   ],
   "source": [
    "# Retrieve news articles for the test ticker\n",
    "print(f\"Retrieving news articles for {TEST_TICKER}...\")\n",
    "\n",
    "articles = news_client.get_news_for_ticker(\n",
    "    ticker=TEST_TICKER,\n",
    "    from_date=start_date.strftime('%Y-%m-%d'),\n",
    "    to_date=end_date.strftime('%Y-%m-%d'),\n",
    "    page_size=10  # Limit for demonstration\n",
    ")\n",
    "\n",
    "if articles:\n",
    "    print(f\"Successfully retrieved {len(articles)} articles\")\n",
    "    \n",
    "    # Display basic information about retrieved articles\n",
    "    print(\"Article overview:\")\n",
    "    \n",
    "    for i, article in enumerate(articles[:3], 1):  # Show first 3\n",
    "        print(f\"{i}. {article.get('title', 'No Title')}\")\n",
    "        print(f\"   Source: {article.get('source', {}).get('name', 'Unknown')}\")\n",
    "        print(f\"   Published: {article.get('publishedAt', 'Unknown')}\")\n",
    "        print()\n",
    "    \n",
    "    if len(articles) > 3:\n",
    "        print(f\"... and {len(articles) - 3} more articles\")\n",
    "        \n",
    "else:\n",
    "    print(\"No articles retrieved. Please check your API key and network connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ed63b",
   "metadata": {},
   "source": [
    "## LangChain Processing and Analysis\n",
    "\n",
    "Process the retrieved news articles using LangChain for sentiment analysis and summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff1f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing articles with LangChain...\n",
      "Converted 10 articles to LangChain documents\n",
      "Sentiment Analysis Results for AAPL:\n",
      "Article 1: This Little-Known AI Stock Is Up 70% in 2025 and Analysts Think It Can Rally Fur...\n",
      "  Source: Barchart.com\n",
      "  Sentiment: POSITIVE (1.00)\n",
      "  Signals: +3 -0 =0\n",
      "Article 2: Top Stock Movers Now: Apple, FedEx, Lennar, and More...\n",
      "  Source: Investopedia\n",
      "  Sentiment: NEUTRAL (0.00)\n",
      "  Signals: +0 -0 =0\n",
      "Article 3: How Micron Stock Surges 2x To $300...\n",
      "  Source: Forbes\n",
      "  Sentiment: NEUTRAL (0.00)\n",
      "  Signals: +0 -0 =0\n",
      "\n",
      "Overall Sentiment Summary for AAPL:\n",
      "Articles analyzed: 3\n",
      "Sentiment distribution: 1P 0N 2Neu\n",
      "Average confidence: 0.33\n",
      "Total signals: +3 -0 =0\n",
      "Overall market sentiment for AAPL: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "if articles:\n",
    "    # Convert articles to LangChain documents\n",
    "    print(\"Processing articles with LangChain...\")\n",
    "    \n",
    "    documents = news_client.format_articles_for_analysis(articles)\n",
    "    print(f\"Converted {len(documents)} articles to LangChain documents\")\n",
    "    \n",
    "    # Create a simple sentiment analysis function using keyword matching\n",
    "    def analyze_sentiment(text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Simple sentiment analysis using keyword matching.\n",
    "        In a production environment, you would use a proper LLM here.\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Define sentiment keywords\n",
    "        positive_keywords = [\n",
    "            'positive', 'growth', 'increase', 'profit', 'earnings beat', 'strong',\n",
    "            'bullish', 'gain', 'rise', 'up', 'success', 'better', 'good', 'excellent',\n",
    "            'outperform', 'upgrade', 'buy', 'revenue growth', 'expansion'\n",
    "        ]\n",
    "        \n",
    "        negative_keywords = [\n",
    "            'negative', 'decline', 'decrease', 'loss', 'earnings miss', 'weak',\n",
    "            'bearish', 'fall', 'drop', 'down', 'failure', 'worse', 'bad', 'poor',\n",
    "            'underperform', 'downgrade', 'sell', 'revenue decline', 'contraction'\n",
    "        ]\n",
    "        \n",
    "        neutral_keywords = [\n",
    "            'stable', 'steady', 'unchanged', 'flat', 'hold', 'maintain',\n",
    "            'neutral', 'mixed', 'uncertain', 'wait', 'monitor'\n",
    "        ]\n",
    "        \n",
    "        # Count sentiment indicators\n",
    "        positive_score = sum(1 for keyword in positive_keywords if keyword in text_lower)\n",
    "        negative_score = sum(1 for keyword in negative_keywords if keyword in text_lower)\n",
    "        neutral_score = sum(1 for keyword in neutral_keywords if keyword in text_lower)\n",
    "        \n",
    "        # Determine overall sentiment\n",
    "        total_score = positive_score + negative_score + neutral_score\n",
    "        \n",
    "        if total_score == 0:\n",
    "            sentiment = 'neutral'\n",
    "            confidence = 0.0\n",
    "        else:\n",
    "            if positive_score > negative_score and positive_score > neutral_score:\n",
    "                sentiment = 'positive'\n",
    "                confidence = positive_score / total_score\n",
    "            elif negative_score > positive_score and negative_score > neutral_score:\n",
    "                sentiment = 'negative'\n",
    "                confidence = negative_score / total_score\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "                confidence = max(neutral_score, positive_score, negative_score) / total_score\n",
    "        \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': round(confidence, 2),\n",
    "            'positive_signals': positive_score,\n",
    "            'negative_signals': negative_score,\n",
    "            'neutral_signals': neutral_score,\n",
    "            'total_signals': total_score\n",
    "        }\n",
    "    \n",
    "    # Analyze each document\n",
    "    print(f\"Sentiment Analysis Results for {TEST_TICKER}:\")\n",
    "    \n",
    "    sentiment_results = []\n",
    "    \n",
    "    for i, doc in enumerate(documents[:3], 1):  # Analyze first 3 documents\n",
    "        # Extract title from content\n",
    "        content_lines = doc.page_content.split('\\n\\n')\n",
    "        title = content_lines[0].replace('Title: ', '') if content_lines else 'No Title'\n",
    "        \n",
    "        print(f\"Article {i}: {title[:80]}...\")\n",
    "        print(f\"  Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "        # Perform sentiment analysis\n",
    "        sentiment_result = analyze_sentiment(doc.page_content)\n",
    "        sentiment_results.append(sentiment_result)\n",
    "        \n",
    "        print(f\"  Sentiment: {sentiment_result['sentiment'].upper()} ({sentiment_result['confidence']:.2f})\")\n",
    "        print(f\"  Signals: +{sentiment_result['positive_signals']} -{sentiment_result['negative_signals']} ={sentiment_result['neutral_signals']}\")\n",
    "    \n",
    "    print(f\"\\nOverall Sentiment Summary for {TEST_TICKER}:\")\n",
    "    \n",
    "    if sentiment_results:\n",
    "        # Calculate overall sentiment\n",
    "        total_positive = sum(r['positive_signals'] for r in sentiment_results)\n",
    "        total_negative = sum(r['negative_signals'] for r in sentiment_results)\n",
    "        total_neutral = sum(r['neutral_signals'] for r in sentiment_results)\n",
    "        \n",
    "        sentiment_counts = {\n",
    "            'positive': sum(1 for r in sentiment_results if r['sentiment'] == 'positive'),\n",
    "            'negative': sum(1 for r in sentiment_results if r['sentiment'] == 'negative'),\n",
    "            'neutral': sum(1 for r in sentiment_results if r['sentiment'] == 'neutral')\n",
    "        }\n",
    "        \n",
    "        avg_confidence = sum(r['confidence'] for r in sentiment_results) / len(sentiment_results)\n",
    "        \n",
    "        print(f\"Articles analyzed: {len(sentiment_results)}\")\n",
    "        print(f\"Sentiment distribution: {sentiment_counts['positive']}P {sentiment_counts['negative']}N {sentiment_counts['neutral']}Neu\")\n",
    "        print(f\"Average confidence: {avg_confidence:.2f}\")\n",
    "        print(f\"Total signals: +{total_positive} -{total_negative} ={total_neutral}\")\n",
    "        \n",
    "        # Overall market sentiment\n",
    "        if total_positive > total_negative:\n",
    "            overall_sentiment = \"POSITIVE\"\n",
    "        elif total_negative > total_positive:\n",
    "            overall_sentiment = \"NEGATIVE\"\n",
    "        else:\n",
    "            overall_sentiment = \"NEUTRAL\"\n",
    "            \n",
    "        print(f\"Overall market sentiment for {TEST_TICKER}: {overall_sentiment}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No articles available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96413b0",
   "metadata": {},
   "source": [
    "## Data Export and Visualization\n",
    "\n",
    "Export the processed data and create visualizations for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac3986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Analysis Summary for AAPL\n",
      "                                                                                         title       source sentiment  confidence\n",
      "This Little-Known AI Stock Is Up 70% in 2025 and Analysts Think It Can Rally Further From Here Barchart.com  positive         1.0\n",
      "                                          Top Stock Movers Now: Apple, FedEx, Lennar, and More Investopedia   neutral         0.0\n",
      "                                                            How Micron Stock Surges 2x To $300       Forbes   neutral         0.0\n",
      "\n",
      "Sentiment Distribution:\n",
      "Neutral: 2 articles (66.7%)\n",
      "Positive: 1 articles (33.3%)\n",
      "\n",
      "Signal Analysis:\n",
      "Total signals: +3 -0 =0\n",
      "Positive/Negative ratio: Infinite (no negative signals)\n",
      "\n",
      "Data exported to: data\\newsapi_analysis_AAPL_20250920_225716.csv\n",
      "\n",
      "Investment Research Summary for AAPL:\n",
      "Analysis period: 2025-09-13 to 2025-09-20\n",
      "Articles analyzed: 3\n",
      "Average sentiment confidence: 0.33\n",
      "Market sentiment indicator: BULLISH - Strong positive sentiment detected\n",
      "Key news sources:\n",
      "  Barchart.com: 1P 0N 0Neu\n",
      "  Forbes: 0P 0N 1Neu\n",
      "  Investopedia: 0P 0N 1Neu\n"
     ]
    }
   ],
   "source": [
    "if articles and 'sentiment_results' in locals():\n",
    "    # Create data directory if it doesn't exist\n",
    "    import os\n",
    "    data_dir = \"data\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a DataFrame for easier analysis and export\n",
    "    news_data = []\n",
    "    \n",
    "    for i, (article, sentiment) in enumerate(zip(articles[:len(sentiment_results)], sentiment_results)):\n",
    "        news_data.append({\n",
    "            'ticker': TEST_TICKER,\n",
    "            'title': article.get('title', 'No Title'),\n",
    "            'source': article.get('source', {}).get('name', 'Unknown'),\n",
    "            'published_at': article.get('publishedAt', 'Unknown'),\n",
    "            'url': article.get('url', 'No URL'),\n",
    "            'sentiment': sentiment['sentiment'],\n",
    "            'confidence': sentiment['confidence'],\n",
    "            'positive_signals': sentiment['positive_signals'],\n",
    "            'negative_signals': sentiment['negative_signals'],\n",
    "            'neutral_signals': sentiment['neutral_signals'],\n",
    "            'total_signals': sentiment['total_signals']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(news_data)\n",
    "    \n",
    "    print(f\"News Analysis Summary for {TEST_TICKER}\")\n",
    "    print(df[['title', 'source', 'sentiment', 'confidence']].to_string(index=False))\n",
    "    \n",
    "    # Create summary statistics\n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    sentiment_dist = df['sentiment'].value_counts()\n",
    "    for sentiment, count in sentiment_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{sentiment.capitalize()}: {count} articles ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSignal Analysis:\")\n",
    "    total_positive_signals = df['positive_signals'].sum()\n",
    "    total_negative_signals = df['negative_signals'].sum()\n",
    "    total_neutral_signals = df['neutral_signals'].sum()\n",
    "    \n",
    "    print(f\"Total signals: +{total_positive_signals} -{total_negative_signals} ={total_neutral_signals}\")\n",
    "    \n",
    "    # Calculate signal ratio\n",
    "    if total_negative_signals > 0:\n",
    "        signal_ratio = total_positive_signals / total_negative_signals\n",
    "        print(f\"Positive/Negative ratio: {signal_ratio:.2f}\")\n",
    "    else:\n",
    "        print(\"Positive/Negative ratio: Infinite (no negative signals)\")\n",
    "    \n",
    "    # Save to CSV for further analysis\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_filename = os.path.join(data_dir, f\"newsapi_analysis_{TEST_TICKER}_{timestamp}.csv\")\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nData exported to: {output_filename}\")\n",
    "    \n",
    "    print(f\"\\nInvestment Research Summary for {TEST_TICKER}:\")\n",
    "    print(f\"Analysis period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Articles analyzed: {len(df)}\")\n",
    "    print(f\"Average sentiment confidence: {df['confidence'].mean():.2f}\")\n",
    "    \n",
    "    # Generate investment insight\n",
    "    if total_positive_signals > total_negative_signals * 1.5:\n",
    "        insight = \"BULLISH - Strong positive sentiment detected\"\n",
    "    elif total_negative_signals > total_positive_signals * 1.5:\n",
    "        insight = \"BEARISH - Strong negative sentiment detected\"\n",
    "    else:\n",
    "        insight = \"NEUTRAL - Mixed or balanced sentiment\"\n",
    "    \n",
    "    print(f\"Market sentiment indicator: {insight}\")\n",
    "    \n",
    "    # Key sources contributing to sentiment\n",
    "    source_sentiment = df.groupby('source')['sentiment'].apply(list).to_dict()\n",
    "    print(f\"Key news sources:\")\n",
    "    for source, sentiments in source_sentiment.items():\n",
    "        positive_count = sentiments.count('positive')\n",
    "        negative_count = sentiments.count('negative')\n",
    "        neutral_count = sentiments.count('neutral')\n",
    "        print(f\"  {source}: {positive_count}P {negative_count}N {neutral_count}Neu\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for export and visualization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
