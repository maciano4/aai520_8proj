{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6498d17d",
      "metadata": {
        "id": "6498d17d"
      },
      "source": [
        "# Investment Research Agent - Multi-Agent System (Skeleton)\n",
        "\n",
        "## Project Overview\n",
        "This notebook implements an autonomous Investment Research Agent that demonstrates:\n",
        "\n",
        "## Architecture\n",
        "- **Multi-Agent System**: Coordinator, Specialist Agents (News, Technical, Fundamental)\n",
        "- **Memory System**: FAISS vector database for persistent learning\n",
        "- **Data Sources**: Yahoo Finance, NewsAPI, FRED, Alpha Vantage\n",
        "- **Interface**: Gradio web interface for user interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c50fc2f",
      "metadata": {
        "id": "4c50fc2f"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0584bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a0584bd",
        "outputId": "cd746b14-c2b3-47ff-a25d-78590bf3a26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.34)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.30)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: fredapi in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.12/dist-packages (0.2.7)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.4.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai langchain-community yfinance pandas numpy matplotlib seaborn plotly gradio faiss-cpu python-dotenv requests fredapi newsapi-python chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61665033",
      "metadata": {
        "id": "61665033"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Any, Optional\n",
        "import gradio as gr\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.tools import BaseTool, tool\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Data source imports\n",
        "import yfinance as yf\n",
        "from newsapi import NewsApiClient\n",
        "from fredapi import Fred\n",
        "import requests\n",
        "\n",
        "# Environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e8574a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "84e8574a",
        "outputId": "c1ad040c-601d-42fa-9c5a-67a0b646ed3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3447722202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Initialize Azure OpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m llm = AzureChatOpenAI(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mazure_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAZURE_OPENAI_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mazure_deployment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAZURE_OPENAI_GPT_DEPLOYMENT_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419  # Intentional blank docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/azure.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAzureOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/lib/azure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_version, azure_endpoint, azure_deployment, api_key, azure_ad_token, azure_ad_token_provider, organization, project, webhook_secret, websocket_base_url, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mazure_ad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mazure_ad_token_provider\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;34m\"Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables."
          ]
        }
      ],
      "source": [
        "# Configuration from environment variables\n",
        "AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "AZURE_OPENAI_GPT_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_GPT_DEPLOYMENT_NAME')\n",
        "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')\n",
        "AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
        "\n",
        "ALPHA_VANTAGE_API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
        "NEWS_API_KEY = os.getenv('NEWSAPI_KEY')\n",
        "FRED_API_KEY = os.getenv('FRED_API_KEY')\n",
        "\n",
        "# Initialize Azure OpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "    azure_deployment=AZURE_OPENAI_GPT_DEPLOYMENT_NAME,\n",
        "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
        "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Initialize embeddings for vector database\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "    azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
        "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
        "    openai_api_key=AZURE_OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "print(\"Environment setup completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b6e6b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81b6e6b6",
        "outputId": "ed305a0b-6755-4b28-b984-4819067fc403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt configuration class created!\n"
          ]
        }
      ],
      "source": [
        "# Swapnil\n",
        "class PromptConfiguration:\n",
        "    \"\"\"Central configuration class for all prompts used in the investment research system\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_planning_prompt(role: str, task: str) -> str:\n",
        "        \"\"\"Get planning prompt for research tasks\n",
        "\n",
        "        TODO: Create a formatted string prompt that:\n",
        "        - Takes the agent's role (e.g., \"Investment Analyst\") and specific task\n",
        "        - Guides the agent to create a detailed research plan\n",
        "        - Should cover: data gathering, analysis techniques, risk assessment, market context\n",
        "        - Returns a numbered list of specific, actionable research steps\n",
        "        - Use f-string formatting: f\"As an {role}, create a detailed research plan for: {task}\"\n",
        "        - Include sections for: data gathering, analysis techniques, risk assessment, market context\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_reflection_prompt(analysis: str, context: str = \"\") -> str:\n",
        "        \"\"\"Get self-reflection prompt for quality assessment\n",
        "\n",
        "        TODO: Create a prompt that evaluates analysis quality on:\n",
        "        - Completeness (1-10): Are all key aspects covered?\n",
        "        - Data Quality (1-10): Is the data comprehensive and current?\n",
        "        - Logic (1-10): Is the reasoning sound and well-structured?\n",
        "        - Actionability (1-10): Are the conclusions practical and specific?\n",
        "        - Risk Assessment (1-10): Are risks properly identified and evaluated?\n",
        "        - Should return JSON format with scores, strengths, improvements, recommendations\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_news_classification_prompt(news_text: str) -> str:\n",
        "        \"\"\"Get news classification prompt for sentiment analysis\n",
        "\n",
        "        TODO: Create a prompt that classifies news articles with:\n",
        "        - Takes title and description as parameters\n",
        "        - Returns JSON format with:\n",
        "          - category: \"earnings|product|market|regulation|management|merger|other\"\n",
        "          - sentiment: \"positive|negative|neutral\"\n",
        "          - importance: \"high|medium|low\"\n",
        "          - reasoning: brief explanation\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_news_extraction_prompt(news_data: List[Dict]) -> str:\n",
        "        \"\"\"Get news extraction prompt for key information\n",
        "\n",
        "        TODO: Create a prompt that extracts insights from classified articles:\n",
        "        - Takes classified articles as input\n",
        "        - Returns JSON with: key_themes, sentiment_distribution, high_importance_items,\n",
        "          potential_catalysts, risk_factors\n",
        "        - Should analyze multiple articles and identify patterns\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_news_summarization_prompt(extracted_data: str) -> str:\n",
        "        \"\"\"Get news summarization prompt\n",
        "\n",
        "        TODO: Create comprehensive news analysis summary covering:\n",
        "        - Executive Summary (2-3 sentences)\n",
        "        - Key Developments and Themes\n",
        "        - Sentiment Analysis\n",
        "        - Potential Stock Price Catalysts\n",
        "        - Risk Factors to Monitor\n",
        "        - Investment Implications\n",
        "        - Should be suitable for investment decision-making\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_routing_prompt(request: str, symbol: str) -> str:\n",
        "        \"\"\"Get routing prompt for specialist assignment\n",
        "\n",
        "        TODO: Create a prompt that determines which specialists to use:\n",
        "        - Available specialists: technical, fundamental, news\n",
        "        - Returns JSON with: specialists_needed, priority_order, reasoning\n",
        "        - Should route based on request type and symbol needs\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_technical_analysis_prompt(symbol: str, stock_data: str) -> str:\n",
        "        \"\"\"Get technical analysis prompt\n",
        "\n",
        "        TODO: Create comprehensive technical analysis prompt covering:\n",
        "        - Price Trend Analysis (short-term and medium-term)\n",
        "        - Support and Resistance Levels\n",
        "        - Volume Analysis, Key Technical Indicators, Chart Patterns\n",
        "        - Technical Price Targets, Risk Levels and Stop-Loss Recommendations\n",
        "        - Should conclude with: Technical Rating, Confidence Level, Key Risks, Price Levels to Watch\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_fundamental_analysis_prompt(symbol: str, stock_data: str, alpha_overview: str) -> str:\n",
        "        \"\"\"Get fundamental analysis prompt\n",
        "\n",
        "        TODO: Create comprehensive fundamental analysis covering:\n",
        "        - Company Business Model and Competitive Position\n",
        "        - Financial Health Assessment, Valuation Analysis (P/E, PEG ratios)\n",
        "        - Growth Prospects, Management Quality, Industry Analysis\n",
        "        - Competitive Advantages and Moats\n",
        "        - Should conclude with: Fundamental Rating, Fair Value Estimate, Key Risks, Catalysts\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_news_analysis_prompt(symbol: str, news_summary: str) -> str:\n",
        "        \"\"\"Get news analysis prompt\n",
        "\n",
        "        TODO: Create sentiment analysis prompt focusing on:\n",
        "        - Market Sentiment Implications\n",
        "        - News Flow Impact on Stock Price\n",
        "        - Institutional vs Retail Sentiment\n",
        "        - Social Media and Public Perception Trends\n",
        "        - News-Based Trading Opportunities, Event-Driven Catalysts\n",
        "        - Should conclude with: Sentiment Rating, News Impact Assessment, Recommended Action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_evaluation_prompt(analysis: str, criteria: List[str]) -> str:\n",
        "        \"\"\"Get evaluation prompt for analysis quality assessment\n",
        "\n",
        "        TODO: Create quality evaluator prompt that scores analysis on:\n",
        "        - Completeness, Data Integration, Risk Assessment, Actionability\n",
        "        - Logic and Reasoning, Market Context, Clarity (all 1-10 scale)\n",
        "        - Returns JSON with: scores dict, overall_score, grade (A-F)\n",
        "        - Include: strengths, weaknesses, specific_improvements, missing_elements\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_optimization_prompt(analysis: str, evaluation: str, iteration: int) -> str:\n",
        "        \"\"\"Get optimization prompt for analysis refinement\n",
        "\n",
        "        TODO: Create refinement prompt that:\n",
        "        - Takes original analysis and evaluation feedback\n",
        "        - Addresses identified weaknesses and adds missing elements\n",
        "        - Implements specific improvements from evaluation\n",
        "        - Focuses on areas that scored below 7/10\n",
        "        - Maintains professional investment analysis standards\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def get_synthesis_prompt(specialist_analyses: Dict[str, Any]) -> str:\n",
        "        \"\"\"Get synthesis prompt for combining specialist analyses\n",
        "\n",
        "        TODO: Create comprehensive investment analysis prompt that:\n",
        "        - Combines technical, fundamental, and news specialist reports\n",
        "        - Creates structured analysis with: Executive Summary, Investment Thesis\n",
        "        - Includes: Key Strengths/Opportunities, Risks/Concerns, Analysis Summaries\n",
        "        - Concludes with: Price Target, Recommendation, Risk-Adjusted Returns, Action Items\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "print(\"Prompt configuration class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281b126c",
      "metadata": {
        "id": "281b126c"
      },
      "source": [
        "## 2. Data Source Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5bda7fd",
      "metadata": {
        "id": "d5bda7fd"
      },
      "outputs": [],
      "source": [
        "# Nelson\n",
        "@tool\n",
        "def get_stock_data(symbol: str, period: str = \"1y\") -> str:\n",
        "    \"\"\"Get comprehensive stock data including price, volume, and basic metrics.\n",
        "\n",
        "    TODO: Implement using yfinance library:\n",
        "    - Create yf.Ticker(symbol) object\n",
        "    - Get historical data with stock.history(period=period)\n",
        "    - Extract current_price, price_change, price_change_pct, volume\n",
        "    - Get company info with stock.info for market_cap, pe_ratio, company_name, sector, industry\n",
        "    - Return JSON string with all data formatted nicely\n",
        "    - Handle exceptions and return error message if data retrieval fails\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def get_stock_news(symbol: str, days: int = 7) -> str:\n",
        "    \"\"\"Get recent news articles for a stock symbol.\n",
        "\n",
        "    TODO: Implement using NewsApiClient:\n",
        "    - Initialize NewsApiClient with NEWS_API_KEY\n",
        "    - Calculate from_date using datetime.now() - timedelta(days=days)\n",
        "    - Use newsapi.get_everything() with symbol as query, language='en', sort_by='relevancy'\n",
        "    - Extract top 5 articles with: title, description, source, published_at, url\n",
        "    - Return JSON string of news_items list\n",
        "    - Handle exceptions and return error message if news retrieval fails\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def get_economic_data(indicator: str = \"GDP\", start_date: str = \"2020-01-01\") -> str:\n",
        "    \"\"\"Get economic indicators from FRED (Federal Reserve Economic Data).\n",
        "\n",
        "    TODO: Implement using fredapi.Fred:\n",
        "    - Initialize Fred(api_key=FRED_API_KEY)\n",
        "    - Use fred.get_series(series_id, limit=12) to get last 12 observations\n",
        "    - Calculate latest_value, previous_value, change, change_pct\n",
        "    - Return JSON with series_id, latest_value, latest_date, previous_value, change, change_pct\n",
        "    - Common series IDs: 'GDP', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL'\n",
        "    - Handle exceptions and return error message if economic data retrieval fails\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "@tool\n",
        "def get_alpha_vantage_data(symbol: str, function: str = \"OVERVIEW\") -> str:\n",
        "    \"\"\"Get detailed financial data from Alpha Vantage API.\n",
        "\n",
        "    TODO: Implement using requests library:\n",
        "    - Create base_url = \"https://www.alphavantage.co/query\"\n",
        "    - Set params with function, symbol, and ALPHA_VANTAGE_API_KEY\n",
        "    - Make GET request and parse JSON response\n",
        "    - For OVERVIEW function: extract symbol, market_cap, pe_ratio, peg_ratio, dividend_yield, eps, 52_week_high, 52_week_low\n",
        "    - Return JSON string (truncate to avoid token limits)\n",
        "    - Handle exceptions and return error message if Alpha Vantage data retrieval fails\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "print(\"Data source tools created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb3b568",
      "metadata": {
        "id": "9bb3b568"
      },
      "source": [
        "## 3. Agent Memory System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef29b8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef29b8e",
        "outputId": "fe357f44-a019-44b6-e3f0-c23f72bfde3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TechnicalAnalyst class created!\n"
          ]
        }
      ],
      "source": [
        "# Chris\n",
        "class TechnicalAnalyst:\n",
        "    \"\"\"Agent specialized in technical analysis of stock price and volume data.\n",
        "\n",
        "    TODO: Implement technical analysis capabilities:\n",
        "    - Accept price data (OHLCV) as input\n",
        "    - Calculate key technical indicators: moving averages (SMA, EMA), RSI, MACD, Bollinger Bands\n",
        "    - Identify chart patterns and trend analysis\n",
        "    - Provide buy/sell/hold recommendations based on technical signals\n",
        "    - Return structured analysis with indicator values, signals, and confidence levels\n",
        "    \"\"\"\n",
        "    def __init__(self, llm, prompt_config):\n",
        "        # Store LLM client and prompt configuration\n",
        "        self.llm = llm\n",
        "        self.prompt_config = prompt_config\n",
        "\n",
        "\n",
        "    def analyze(self, stock_data, symbol):\n",
        "        # Implement technical analysis logic:\n",
        "        # - Parse stock_data JSON to extract price/volume information\n",
        "        # - Use prompt_config.get_technical_analysis_prompt() for analysis template\n",
        "        # - Send structured prompt to LLM with price data and technical requirements\n",
        "        # - Return analysis with technical indicators, trend assessment, and trading signals\n",
        "        try:\n",
        "            # Parse stock_data JSON\n",
        "            stock_data_str = json.dumps(stock_data)\n",
        "\n",
        "            # Get technical analysis prompt\n",
        "            prompt = self.prompt_config.get_technical_analysis_prompt(symbol, stock_data_str)\n",
        "\n",
        "            # Send prompt to LLM\n",
        "            response = self.llm.invoke(prompt)\n",
        "            analysis = response.content\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during technical analysis for {symbol}: {e}\")\n",
        "            return f\"Error performing technical analysis for {symbol}: {e}\"\n",
        "\n",
        "print(\"TechnicalAnalyst class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a640e517",
      "metadata": {
        "id": "a640e517"
      },
      "source": [
        "## 4. Base Agent Class with Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf8085c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bf8085c",
        "outputId": "3ecf7830-2dcd-4fcd-a907-faa93ef4608a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FundamentalAnalyst class created!\n"
          ]
        }
      ],
      "source": [
        "# Chris\n",
        "class FundamentalAnalyst:\n",
        "    \"\"\"Agent specialized in fundamental analysis of company financials and valuation.\n",
        "\n",
        "    TODO: Implement fundamental analysis capabilities:\n",
        "    - Analyze financial metrics: P/E ratio, PEG ratio, dividend yield, market cap\n",
        "    - Evaluate company fundamentals: revenue growth, profitability, debt levels\n",
        "    - Compare metrics to industry averages and historical performance\n",
        "    - Assess intrinsic value and investment quality\n",
        "    - Return structured analysis with valuation metrics and investment thesis\n",
        "    \"\"\"\n",
        "    def __init__(self, llm, prompt_config):\n",
        "        # TODO: Store LLM client and prompt configuration\n",
        "        self.llm = llm\n",
        "        self.prompt_config = prompt_config\n",
        "\n",
        "    def analyze(self, stock_data, alpha_vantage_data, symbol):\n",
        "        # TODO: Implement fundamental analysis logic:\n",
        "        # - Parse both stock_data and alpha_vantage_data for financial metrics\n",
        "        # - Use prompt_config.get_fundamental_analysis_prompt() for analysis template\n",
        "        # - Calculate and evaluate key ratios and financial health indicators\n",
        "        # - Return analysis with valuation assessment, strengths/weaknesses, and investment rating\n",
        "        try:\n",
        "            # Parse stock_data and alpha_vantage_data\n",
        "            stock_data_str = json.dumps(stock_data)\n",
        "            alpha_vantage_data_str = json.dumps(alpha_vantage_data)\n",
        "\n",
        "            # Get fundamental analysis prompt\n",
        "            prompt = self.prompt_config.get_fundamental_analysis_prompt(\n",
        "                symbol, stock_data_str, alpha_vantage_data_str\n",
        "            )\n",
        "\n",
        "            # Send prompt to LLM\n",
        "            response = self.llm.invoke(prompt)\n",
        "            analysis = response.content\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(\n",
        "                f\"Error during fundamental analysis for {symbol}: {e}\"\n",
        "            )\n",
        "            return f\"Error performing fundamental analysis for {symbol}: {e}\"\n",
        "\n",
        "\n",
        "print(\"FundamentalAnalyst class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba78e95",
      "metadata": {
        "id": "5ba78e95"
      },
      "source": [
        "## 5. Workflow Pattern 1: Prompt Chaining (News Processing Pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a60461",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4a60461",
        "outputId": "f958fb87-5216-444b-f790-30b125cb0444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsAnalyst class created!\n"
          ]
        }
      ],
      "source": [
        "# Chris\n",
        "class NewsAnalyst:\n",
        "    \"\"\"Agent specialized in sentiment analysis and news impact assessment.\n",
        "\n",
        "    TODO: Implement news sentiment analysis capabilities:\n",
        "    - Process news articles for sentiment (positive/negative/neutral)\n",
        "    - Identify key themes and market-moving events\n",
        "    - Assess potential impact on stock price and investor sentiment\n",
        "    - Correlate news sentiment with recent price movements\n",
        "    - Return structured analysis with sentiment scores and impact assessment\n",
        "    \"\"\"\n",
        "    def __init__(self, llm, prompt_config):\n",
        "        # TODO: Store LLM client and prompt configuration\n",
        "        self.llm = llm\n",
        "        self.prompt_config = prompt_config\n",
        "\n",
        "    def analyze(self, news_data, symbol):\n",
        "        # TODO: Implement news sentiment analysis logic:\n",
        "        # - Parse news_data JSON to extract article titles, descriptions, and sources\n",
        "        # - Use prompt_config.get_news_analysis_prompt() for sentiment analysis template\n",
        "        # - Analyze each article for sentiment and relevance to stock performance\n",
        "        # - Aggregate sentiment scores and identify key themes/events\n",
        "        # - Return analysis with overall sentiment, key insights, and potential price impact\n",
        "        try:\n",
        "            # Parse news_data JSON\n",
        "            news_data_str = json.dumps(news_data)\n",
        "\n",
        "            # Get news analysis prompt\n",
        "            prompt = self.prompt_config.get_news_analysis_prompt(symbol, news_data_str)\n",
        "\n",
        "            # Send prompt to LLM\n",
        "            response = self.llm.invoke(prompt)\n",
        "            analysis = response.content\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during news analysis for {symbol}: {e}\")\n",
        "            return f\"Error performing news analysis for {symbol}: {e}\"\n",
        "\n",
        "print(\"NewsAnalyst class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6bd1e5b",
      "metadata": {
        "id": "e6bd1e5b"
      },
      "source": [
        "## 6. Workflow Pattern 2: Routing (Specialist Agents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7ecae9",
      "metadata": {
        "id": "1a7ecae9"
      },
      "outputs": [],
      "source": [
        "# Nelson\n",
        "class AgentMemory:\n",
        "    \"\"\"Persistent memory system for storing and retrieving agent experiences and insights.\n",
        "\n",
        "    TODO: Implement FAISS-based vector memory system:\n",
        "    - Initialize FAISS index for similarity search and storage\n",
        "    - Use embeddings to store agent experiences, analysis results, and insights\n",
        "    - Implement add_memory() to store new experiences with metadata (timestamp, symbol, analysis_type)\n",
        "    - Implement search_memory() to retrieve relevant past experiences using similarity search\n",
        "    - Implement save_memory() and load_memory() for persistence across sessions\n",
        "    - Support different memory types: analysis_results, market_insights, user_preferences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model):\n",
        "        # TODO: Initialize FAISS index and embedding model\n",
        "        # - Create FAISS IndexFlatL2 for vector similarity search\n",
        "        # - Store embedding_model for converting text to vectors\n",
        "        # - Initialize memories list for storing text and metadata\n",
        "        # - Set memory_file path for persistence\n",
        "        pass\n",
        "\n",
        "    def add_memory(self, content, memory_type, symbol=None, metadata=None):\n",
        "        # TODO: Add new memory to the vector store:\n",
        "        # - Convert content to embedding using embedding_model\n",
        "        # - Add vector to FAISS index\n",
        "        # - Store memory with metadata (type, symbol, timestamp, etc.)\n",
        "        # - Automatically save to disk for persistence\n",
        "        pass\n",
        "\n",
        "    def search_memory(self, query, memory_type=None, k=5):\n",
        "        # TODO: Search for relevant memories:\n",
        "        # - Convert query to embedding\n",
        "        # - Use FAISS index.search() to find k most similar memories\n",
        "        # - Filter by memory_type if specified\n",
        "        # - Return list of relevant memories with similarity scores\n",
        "        pass\n",
        "\n",
        "    def save_memory(self):\n",
        "        # TODO: Persist memory to disk:\n",
        "        # - Save FAISS index using faiss.write_index()\n",
        "        # - Save memories list and metadata to pickle file\n",
        "        # - Handle exceptions and log save status\n",
        "        pass\n",
        "\n",
        "    def load_memory(self):\n",
        "        # TODO: Load memory from disk:\n",
        "        # - Load FAISS index using faiss.read_index()\n",
        "        # - Load memories list and metadata from pickle file\n",
        "        # - Handle file not found exceptions gracefully\n",
        "        # - Return True if loaded successfully, False otherwise\n",
        "        pass\n",
        "\n",
        "print(\"AgentMemory class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3d0027",
      "metadata": {
        "id": "bf3d0027"
      },
      "source": [
        "## 7. Workflow Pattern 3: Evaluator-Optimizer (Analysis Refinement Loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ca909f",
      "metadata": {
        "id": "a4ca909f"
      },
      "outputs": [],
      "source": [
        "# Swapnil\n",
        "class NewsProcessingChain:\n",
        "    \"\"\"LangChain-based pipeline for processing and analyzing news data.\n",
        "\n",
        "    TODO: Implement 5-step news processing pipeline:\n",
        "    1. Ingest: Load and parse news articles from various sources\n",
        "    2. Preprocess: Clean text, remove duplicates, standardize format\n",
        "    3. Classify: Categorize news by relevance and impact level\n",
        "    4. Extract: Identify key entities, events, and sentiment indicators\n",
        "    5. Summarize: Generate concise summaries with investment implications\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        # TODO: Initialize LangChain components:\n",
        "        # - Store LLM for processing steps\n",
        "        # - Create chain components for each processing step\n",
        "        # - Set up prompt templates for classification and extraction\n",
        "        pass\n",
        "\n",
        "    def process_news(self, news_data, symbol):\n",
        "        # TODO: Execute complete news processing pipeline:\n",
        "        # - Step 1: Parse news_data JSON and validate articles\n",
        "        # - Step 2: Clean and deduplicate news articles\n",
        "        # - Step 3: Classify articles by relevance to symbol and market impact\n",
        "        # - Step 4: Extract entities, sentiment, and key events using LLM\n",
        "        # - Step 5: Generate investment-focused summary of all relevant news\n",
        "        # - Return structured result with processed articles and summary\n",
        "        pass\n",
        "\n",
        "print(\"NewsProcessingChain class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7830923",
      "metadata": {
        "id": "b7830923"
      },
      "source": [
        "## 8. Main Investment Research Agent Coordinator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed4af69",
      "metadata": {
        "id": "aed4af69"
      },
      "outputs": [],
      "source": [
        "# Swapnil\n",
        "class InvestmentResearchAgent:\n",
        "    \"\"\"Main coordinating agent that orchestrates the complete investment research process.\n",
        "\n",
        "    TODO: Implement comprehensive investment research orchestration:\n",
        "    - Coordinate data collection from all sources (stock, news, economic, Alpha Vantage)\n",
        "    - Route analysis to appropriate specialist agents (technical, fundamental, news)\n",
        "    - Implement planning phase to determine research approach\n",
        "    - Implement reflection phase to validate and synthesize results\n",
        "    - Implement learning phase to update memory with insights\n",
        "    - Generate final investment recommendation with confidence levels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm, memory, prompt_config):\n",
        "        # TODO: Initialize main coordinating agent:\n",
        "        # - Store LLM client, memory system, and prompt configuration\n",
        "        # - Create instances of specialist agents (TechnicalAnalyst, FundamentalAnalyst, NewsAnalyst)\n",
        "        # - Initialize NewsProcessingChain for news analysis\n",
        "        # - Set up tools for data collection (get_stock_data, get_stock_news, etc.)\n",
        "        pass\n",
        "\n",
        "    def research_stock(self, symbol, analysis_type=\"comprehensive\"):\n",
        "        # TODO: Execute complete stock research workflow:\n",
        "        # - Planning Phase: Use prompt_config to determine research approach\n",
        "        # - Data Collection: Gather data from all relevant sources\n",
        "        # - Specialist Analysis: Route to appropriate analysts based on analysis_type\n",
        "        # - Synthesis: Combine all analysis results into coherent assessment\n",
        "        # - Reflection Phase: Validate results and identify gaps\n",
        "        # - Learning Phase: Store insights in memory for future use\n",
        "        # - Return comprehensive investment report with recommendations\n",
        "        pass\n",
        "\n",
        "print(\"InvestmentResearchAgent class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2ad7d7",
      "metadata": {
        "id": "ba2ad7d7"
      },
      "source": [
        "## 9. Visualization and Reporting Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6acb3e",
      "metadata": {
        "id": "3e6acb3e"
      },
      "outputs": [],
      "source": [
        "# Nelson\n",
        "# TODO: Main execution workflow - Create and use the investment research system\n",
        "\n",
        "# TODO: Initialize the system components:\n",
        "# - Set up Azure OpenAI client with proper credentials\n",
        "# - Create embedding model for memory system (e.g., text-embedding-ada-002)\n",
        "# - Initialize AgentMemory with embedding model\n",
        "# - Create PromptConfiguration instance for all prompt templates\n",
        "# - Initialize InvestmentResearchAgent with LLM, memory, and prompts\n",
        "\n",
        "# TODO: Example usage pattern:\n",
        "# 1. Create investment_agent = InvestmentResearchAgent(llm, memory, prompt_config)\n",
        "# 2. Load any existing memory: memory.load_memory()\n",
        "# 3. Research a stock: result = investment_agent.research_stock(\"AAPL\", \"comprehensive\")\n",
        "# 4. Display results and save memory: memory.save_memory()\n",
        "\n",
        "# TODO: Implement error handling and logging:\n",
        "# - Wrap API calls in try-catch blocks\n",
        "# - Log all research activities and results\n",
        "# - Handle rate limiting and API failures gracefully\n",
        "# - Provide fallback options when data sources are unavailable\n",
        "\n",
        "print(\"Investment Research Agent System Ready!\")\n",
        "print(\"Usage: agent.research_stock('SYMBOL', analysis_type='comprehensive|technical|fundamental|news')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7cd317",
      "metadata": {
        "id": "8b7cd317"
      },
      "source": [
        "## 10. Gradio Web Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9fcc3c",
      "metadata": {
        "id": "9d9fcc3c"
      },
      "outputs": [],
      "source": [
        "# Swapnil\n",
        "# TODO: Testing and validation framework\n",
        "\n",
        "# TODO: Implement comprehensive testing:\n",
        "# - Test each data source tool individually (get_stock_data, get_stock_news, etc.)\n",
        "# - Test specialist agents with sample data to verify analysis quality\n",
        "# - Test memory system save/load functionality and search capabilities\n",
        "# - Test complete research workflow with known stocks for validation\n",
        "# - Implement unit tests for core functionality and error handling\n",
        "\n",
        "# TODO: Example test cases:\n",
        "# 1. Test data collection: verify all APIs return properly formatted data\n",
        "# 2. Test analysis quality: compare agent outputs with expected results\n",
        "# 3. Test memory persistence: save data, restart system, verify retrieval\n",
        "# 4. Test error handling: simulate API failures and validate graceful handling\n",
        "# 5. Test performance: measure response times and optimize bottlenecks\n",
        "\n",
        "print(\"Testing framework ready - implement comprehensive validation before production use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d5afc3",
      "metadata": {
        "id": "e7d5afc3"
      },
      "source": [
        "## 11. Interface Launch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5c8a14",
      "metadata": {
        "id": "cd5c8a14"
      },
      "outputs": [],
      "source": [
        "# TODO: Configuration and deployment notes\n",
        "\n",
        "# TODO: Required API keys and setup:\n",
        "# - AZURE_OPENAI_API_KEY: Azure OpenAI service key\n",
        "# - AZURE_OPENAI_ENDPOINT: Azure OpenAI service endpoint\n",
        "# - NEWS_API_KEY: NewsAPI.org key for news data\n",
        "# - FRED_API_KEY: Federal Reserve Economic Data API key\n",
        "# - ALPHA_VANTAGE_API_KEY: Alpha Vantage financial data API key\n",
        "\n",
        "# TODO: Required Python packages:\n",
        "# - langchain: For LLM orchestration and chaining\n",
        "# - openai: Azure OpenAI client\n",
        "# - yfinance: Yahoo Finance stock data\n",
        "# - newsapi-python: News API client\n",
        "# - fredapi: Federal Reserve Economic Data API\n",
        "# - faiss-cpu: Vector similarity search for memory\n",
        "# - pandas: Data manipulation and analysis\n",
        "# - numpy: Numerical computations\n",
        "# - requests: HTTP requests for Alpha Vantage\n",
        "\n",
        "print(\"Configuration complete - ensure all API keys are properly set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70832a54",
      "metadata": {
        "id": "70832a54"
      },
      "source": [
        "## 12. Testing and Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800f53dc",
      "metadata": {
        "id": "800f53dc"
      },
      "outputs": [],
      "source": [
        "# TODO: Advanced features and extensions\n",
        "\n",
        "# TODO: Potential enhancements for production deployment:\n",
        "# - Real-time data streaming: WebSocket connections for live price updates\n",
        "# - Portfolio management: Track multiple stocks and generate portfolio-level insights\n",
        "# - Risk management: Implement position sizing and risk assessment algorithms\n",
        "# - Backtesting framework: Test strategies against historical data\n",
        "# - Web interface: Create FastAPI/Streamlit dashboard for user interaction\n",
        "# - Database integration: Store research results and user preferences in database\n",
        "# - Advanced analytics: Machine learning models for price prediction and pattern recognition\n",
        "# - Alert system: Automated notifications for significant market events or analysis updates\n",
        "# - Multi-language support: Analyze international markets and foreign language news\n",
        "# - Integration APIs: Connect with brokerage APIs for automated trading capabilities\n",
        "\n",
        "# TODO: Monitoring and maintenance:\n",
        "# - Implement comprehensive logging and monitoring\n",
        "# - Set up automated testing and deployment pipelines\n",
        "# - Monitor API usage and costs across all data sources\n",
        "# - Regular model evaluation and prompt optimization\n",
        "# - User feedback collection and analysis quality improvement\n",
        "\n",
        "print(\"Advanced features planning complete - ready for production enhancements!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}